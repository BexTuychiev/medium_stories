{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Burning XGBoost FAQs Answered to Use the Library Like a Pro\n",
    "## Master the nitty-gritty about XGBoost\n",
    "![](images/unsplash.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@haithemfrd_off?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Haithem Ferdi</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/boost?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash.</a> All images are by the author unless specified otherwise.\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from matplotlib import rcParams\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\").dropna()\n",
    "penguins = sns.load_dataset(\"penguins\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_input, i_target = iris.drop(\"species\", axis=1), iris[[\"species\"]]\n",
    "p_input, p_target = penguins.drop(\"body_mass_g\", axis=1), penguins[[\"body_mass_g\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_input = pd.get_dummies(p_input)\n",
    "\n",
    "le = LabelEncoder()\n",
    "i_target = le.fit_transform(i_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "    i_input, i_target, test_size=0.2, random_state=1121218\n",
    ")\n",
    "\n",
    "\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
    "    p_input, p_target, test_size=0.2, random_state=1121218\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Which API should I choose - Scikit-learn or the core learning API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this question has been answered by many, I will just state my answer because most of the other questions depend on this one.\n",
    "\n",
    "XGBoost in Python have two APIs - Scikit-learn compatible (estimators have the familiar `fit/predict` pattern) and the core XGBoost-native API (there is a global `train` function, whose objectives can be tweaked to switch between regression and classification).\n",
    "\n",
    "The majority of Python community, including Kagglers and myself use the Scikit-learn API. \n",
    "\n",
    "Using the Sklearn API enables you to freely integrate XGBoost estimators into your familiar workflow. The benefits are (and not limited to) the ability to pass core XGB algorithms into [Sklearn pipelines](https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=your_stories_page-------------------------------------), using a more efficient cross-validation workflow, avoiding the hassles that come with learning a new API, etc.\n",
    "\n",
    "We will also see some nuances in XGBoost functionality that will tip the favor towards Sklearn API even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Do I Completely Control the Randomness in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The rest of the references to XGBoost algorithms mainly imply the Sklearn-compatible XGBRegressor and XGBClassifier (or similar) estimators.\n",
    "\n",
    "The estimators have the `random_state` parameter similar to Sklearn estimators (the alternative `seed` parameter has been deprecated but still works). However, running XGBoost with default parameters will yield identical results even with different seeds. \n",
    "\n",
    "The reason for this behavior is that XGBoost induces randomness only when the parameters `subsample` and all other parameters that start with `colsample_*` prefix are used. As the names suggest, these parameters have a lot to do with [random sampling](https://towardsdatascience.com/why-bootstrap-sampling-is-the-badass-tool-of-probabilistic-thinking-5d8c7343fb67?source=your_stories_page-------------------------------------) to combat overfitting.\n",
    "\n",
    "Therefore, you should only use `random_state` when tuning these hyperparameters to get the same results across runs for the same seed.\n",
    "\n",
    "When using with other Sklearn transformers or estimators that have their own `random_state`, you should pass a seed number both to XGBoost and other classes for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What are objectives in XGBoost and how to specify them for different tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both regression and classification tasks have different types and implementations. They change depending on the objective function, the distributions they can work with and their loss function.\n",
    "\n",
    "For example, regression can be performed using RMSE (Root Mean Squared Error), RMSLE (Root Mean Squared Log Error), Huber Error, etc. loss functions. Sklearn implements different regressors for each of these but in XGBoost, these are all packed into XGBRegressor estimator. \n",
    "\n",
    "You can switch between the implementations of different loss functions, supported distributions with the `objective` parameter. It accepts special code strings provided by XGBoost. Most commons ones are:\n",
    "\n",
    "- `reg:squarederror`\n",
    "- `reg:squaredlogerror`\n",
    "- `reg:gamma`\n",
    "- `reg:tweedie`\n",
    "\n",
    "Similarly, classification objectives change based on their underlying loss function. These objectives start either with `binary:*` or `multi:*` prefixes depending on the target cardinality. \n",
    "\n",
    "There are many other objective types and I will leave it to you to explore the rest and find out the details using this documentation [link](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters).\n",
    "\n",
    "> Note that specifying the correct objective gets rid of that unbelievably annoying warning you get when fitting XGB classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Which XGBoost booster should I use - gblinear, gbtree, dart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> XGBoost has 3 types of gradient boosted learners - these are gradient boosted (GB) linear functions, GB trees and DART trees. You can switch the learners using the `booster` parameter.\n",
    "\n",
    "If you ask the question from Kagglers, they will choose boosted trees over linear functions on any day (as do I). The reason is that trees have the ability to capture non-linear, much more complex relationships that linear functions cannot.\n",
    "\n",
    "So, the only question is which tree booster should you pass to the `booster` parameter - `gbtree` or `dart`? \n",
    "\n",
    "I won't bother you with the full differences here. The thing you should know is that XGBoost uses an ensemble of decision tree-based models when used with `gbtree` booster. DART trees are an improvement (to be yet validated) where they introduce random dropping the subset of the decision-trees to prevent overfitting.\n",
    "\n",
    "In the few, small experiments I did with default parameters for `gbtree` and `dart`, I got slightly better scores with dart when I set the `rate_drop` between 0.1 and 0.3. \n",
    "\n",
    "For more details, I refer to [this page](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html) of the XGB documentation to learn about the nuances and additional hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Which tree method should I use in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 types of algorithms that control the tree construction. You should pass `hist` to `tree_method` if you are doing distributed training. \n",
    "\n",
    "For other scenarios, the default is `auto` which changes from `exact` for small-to-medium datasets to `approx.` for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What is a boosting round in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, XGBoost is an ensemble of gradient boosted decision trees. Each tree in the XGBoost ensemble is called a base or weak learner. A weak learner is any algorithm that performs slightly better than random guessing.\n",
    "\n",
    "By combining the predictions of multiples of weak learners, XGBoost yields a final prediction (skipping a lot of details now). Each time we fit a tree to the data, it is called a single boosting round.\n",
    "\n",
    "So, to specify the number of trees to be built, pass an integer to `num_boost_round` of the Learning API or to `n_estimators` of the Sklearn API. \n",
    "\n",
    "Typically, too few trees lead to underfitting and too large a number of trees leads to overfitting. You will normally tune this parameter with hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What is `early_stopping_rounds` in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From one boosting round to the next, XGBoost builds upon the predictions of the last tree.\n",
    "\n",
    "If the predictions does not improve after a sequence of rounds, it is sensible to stop training even if we are not at a hard stop for `num_boost_round` or `n_estimators`.\n",
    "\n",
    "To achieve this, XGBoost provides `early_stopping_rounds` parameter. For example, setting it to 50 means we stop the training if the predictions have not been improving for the last 50 rounds.\n",
    "\n",
    "It is a good practice to set a higher number for `n_estimators` and change early stopping accordingly to achieve better results.\n",
    "\n",
    "Before I show an example of how it is done in code, there are two other XGBoost parameters to discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What are `eval_set`s in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is only enabled when you pass a set of evaluation data to the `fit` method. These evaluation sets are used to keep track of the performance of the ensemble from one round to the next.\n",
    "\n",
    "At each round, a tree is trained on the passed training sets and to see if the score has been improving, it makes predictions on the passed evaluation sets. Here is what it looks like in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:3072.26099\n",
      "[1]\tvalidation_0-rmse:2166.62012\n",
      "[2]\tvalidation_0-rmse:1536.37512\n",
      "[3]\tvalidation_0-rmse:1099.32336\n",
      "[4]\tvalidation_0-rmse:801.08807\n",
      "[5]\tvalidation_0-rmse:603.52173\n",
      "[6]\tvalidation_0-rmse:479.94824\n",
      "[7]\tvalidation_0-rmse:404.80151\n",
      "[8]\tvalidation_0-rmse:355.74329\n",
      "[9]\tvalidation_0-rmse:329.27765\n",
      "[10]\tvalidation_0-rmse:309.89594\n",
      "[11]\tvalidation_0-rmse:305.84323\n",
      "[12]\tvalidation_0-rmse:295.22681\n",
      "[13]\tvalidation_0-rmse:294.97986\n",
      "[14]\tvalidation_0-rmse:292.48813\n",
      "[15]\tvalidation_0-rmse:292.91577\n",
      "[16]\tvalidation_0-rmse:292.78409\n",
      "[17]\tvalidation_0-rmse:294.15451\n",
      "[18]\tvalidation_0-rmse:295.25330\n",
      "[19]\tvalidation_0-rmse:297.93341\n"
     ]
    }
   ],
   "source": [
    "reg = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=1000)\n",
    "\n",
    "reg = reg.fit(\n",
    "    X_train_p,\n",
    "    y_train_p,\n",
    "    eval_set=[(X_test_p, y_test_p)],\n",
    "    early_stopping_rounds=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Set `verbose` to False to get rid of the log messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the 13th iteration, the score starts decreasing.  So the training stops at the 19th iteration because 5 rounds of early stopping is applied. \n",
    "\n",
    "It is also possible to pass multiple evaluation sets to `eval_set` as a tuple but only the last passed list will be used when used alongside early stopping. \n",
    "\n",
    "> Check out [this post](https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/) to learn more about early stopping and evaluation sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When do evaluation metrics have effect in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify various evaluation metrics using the `eval_metric` of the `fit` method. Passed metrics only have effect internally - for example, they are used to assess the quality of the predictions during early stopping. \n",
    "\n",
    "You should change the metric according to the object you choose. The full list of objectives and their supported metrics can be found from [this page](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters) of the documentation. \n",
    "\n",
    "Below is an example of XGBoost classifier with multi-class logloss and ROC AUC as metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.97875\tvalidation_0-mlogloss:0.77632\n",
      "[1]\tvalidation_0-auc:0.97875\tvalidation_0-mlogloss:0.58204\n",
      "[2]\tvalidation_0-auc:0.97875\tvalidation_0-mlogloss:0.45879\n",
      "[3]\tvalidation_0-auc:0.97875\tvalidation_0-mlogloss:0.37651\n",
      "[4]\tvalidation_0-auc:0.97384\tvalidation_0-mlogloss:0.31858\n",
      "[5]\tvalidation_0-auc:0.97384\tvalidation_0-mlogloss:0.27883\n",
      "[6]\tvalidation_0-auc:0.95718\tvalidation_0-mlogloss:0.25332\n",
      "[7]\tvalidation_0-auc:0.95718\tvalidation_0-mlogloss:0.23344\n",
      "[8]\tvalidation_0-auc:0.95718\tvalidation_0-mlogloss:0.22527\n",
      "[9]\tvalidation_0-auc:0.95718\tvalidation_0-mlogloss:0.21470\n",
      "[10]\tvalidation_0-auc:0.98062\tvalidation_0-mlogloss:0.21210\n",
      "[11]\tvalidation_0-auc:0.96699\tvalidation_0-mlogloss:0.20545\n",
      "[12]\tvalidation_0-auc:0.98062\tvalidation_0-mlogloss:0.20394\n",
      "[13]\tvalidation_0-auc:0.98062\tvalidation_0-mlogloss:0.20141\n",
      "[14]\tvalidation_0-auc:0.98062\tvalidation_0-mlogloss:0.19812\n",
      "[15]\tvalidation_0-auc:0.98062\tvalidation_0-mlogloss:0.20025\n",
      "[16]\tvalidation_0-auc:0.98062\tvalidation_0-mlogloss:0.20425\n",
      "[17]\tvalidation_0-auc:0.98062\tvalidation_0-mlogloss:0.20991\n",
      "[18]\tvalidation_0-auc:0.98062\tvalidation_0-mlogloss:0.21326\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\", n_estimators=200, use_label_encoder=False\n",
    ")\n",
    "\n",
    "eval_set = [(X_test_i, y_test_i)]\n",
    "\n",
    "_ = clf.fit(\n",
    "    X_train_i,\n",
    "    y_train_i,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric=[\"auc\", \"mlogloss\"],\n",
    "    early_stopping_rounds=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what metric you pass to `eval_metric`, it only has effect on the `fit` function. So, when you call `score()` on the classifier, it will still use accuracy, which is the default in Sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What is learning rate (eta) in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time XGBoost adds a new tree to the ensemble, it is used to correct the residual errors of the last group of trees. \n",
    "\n",
    "The problem is that this approach is super fast and powerful, making the algorithm to quickly learn and overfit the training data. So, XGBoost or any other gradient boosting algorithms have a parameter called `learning_rate` that controls the speed of fitting and helps combat overfitting. \n",
    "\n",
    "Typical values for `learning_rate` ranges from 0.1 to 0.3 but it is quite possible to go beyond these, especially towards 0. \n",
    "\n",
    "Whatever passed to `learning_rate`, it plays as a weighting factor for the corrections made by new trees. So, lower learning rate means we place less importance on the corrections of the new trees, hence avoiding overfitting. \n",
    "\n",
    "A good practice is to set a low number for `learning_rate` and use early stopping with larger number of estimators (`n_estimators`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squaredlogerror\", n_estimators=1000, learning_rate=0.01\n",
    ")\n",
    "\n",
    "eval_set = [(X_test_p, y_test_p)]\n",
    "_ = reg.fit(\n",
    "    X_train_p,\n",
    "    y_train_p,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=10,\n",
    "    eval_metric=\"rmsle\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will immediately see the effect of slow `learning_rate` because early stopping will be applied much later during training (in the above case, after 430th iteration).\n",
    "\n",
    "However, each dataset is different, so you need to tune this parameter with hyperparameter optimization as well.\n",
    "\n",
    "> Check out [this post](https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/) on how to tune learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Should you let XGBoost deal with missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, I will give you the advice I have got (or rather read and heard) from two different Kaggle Competition Grandmasters.\n",
    "\n",
    "1. If you give `np.nan` to tree-based models, then, at each node split the missing values are either send to the left child or the right child of the node, depending on what's best. So, at each split, missing values get special treatment, which may lead to overfitting. A simple solution that works pretty well with trees is to fill in nulls with a value different than the rest of the samples like -999.\n",
    "\n",
    "2. Even though packages like XGBoost and LightGBM can treat nulls without preprocessing, it is always a good idea to come up with your own imputation strategy.\n",
    "\n",
    "For real-world datasets, you should always investigate the type of the missingness (MCAR, MAR, MNAR) and choose an imputation strategy (value-based [mean, median, mode] or model-based [KNN imputers or tree-based imputers]). \n",
    "\n",
    "If you are not familiar with these terms, I got you covered [here](https://towardsdatascience.com/going-beyond-the-simpleimputer-for-missing-data-imputation-dd8ba168d505?source=your_stories_page-------------------------------------)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
