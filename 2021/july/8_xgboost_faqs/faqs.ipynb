{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Burning XGBoost FAQs Answered to Use the Library Like a Pro\n",
    "## Master the nitty-gritty about XGBoost\n",
    "![](images/unsplash.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@haithemfrd_off?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Haithem Ferdi</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/boost?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash.</a> All images are by the author unless specified otherwise.\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from matplotlib import rcParams\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\").dropna()\n",
    "penguins = sns.load_dataset(\"penguins\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_input, i_target = iris.drop(\"species\", axis=1), iris[[\"species\"]]\n",
    "p_input, p_target = penguins.drop(\"body_mass_g\", axis=1), penguins[[\"body_mass_g\"]]\n",
    "p_input = pd.get_dummies(p_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "    i_input, i_target, test_size=0.2, random_state=1121218\n",
    ")\n",
    "\n",
    "\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
    "    p_input, p_target, test_size=0.2, random_state=1121218\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Which API should I choose - Scikit-learn or the core learning API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this question has been answered by many, I will just state my answer because most of the other questions depend on this one.\n",
    "\n",
    "XGBoost in Python have two APIs - Scikit-learn compatible (estimators have the familiar `fit/predict` pattern) and the core XGBoost-native API (there is a global `train` function, whose objectives can be tweaked to switch between regression and classification).\n",
    "\n",
    "The majority of Python community, including Kagglers and myself use the Scikit-learn API. \n",
    "\n",
    "Using the Sklearn API enables you to freely integrate XGBoost estimators into your familiar workflow. The benefits are (and not limited to) the ability to pass core XGB algorithms into [Sklearn pipelines](https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=your_stories_page-------------------------------------), using a more efficient cross-validation workflow, avoiding the hassles that come with learning a new API, etc.\n",
    "\n",
    "We will also see some nuances in XGBoost functionality that will tip the favor towards Sklearn API even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Do I Completely Control the Randomness in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The rest of the references to XGBoost algorithms mainly imply the Sklearn-compatible XGBRegressor and XGBClassifier (or similar) estimators.\n",
    "\n",
    "The estimators have the `random_state` parameter similar to Sklearn estimators (the alternative `seed` parameter has been deprecated but still works). However, running XGBoost with default parameters will yield identical results even with different seeds. \n",
    "\n",
    "The reason for this behavior is that XGBoost induces randomness only when the parameters `subsample` and all other parameters that start with `colsample_*` prefix are used. As the names suggest, these parameters have a lot to do with [random sampling](https://towardsdatascience.com/why-bootstrap-sampling-is-the-badass-tool-of-probabilistic-thinking-5d8c7343fb67?source=your_stories_page-------------------------------------) to combat overfitting.\n",
    "\n",
    "Therefore, you should only use `random_state` when tuning these hyperparameters to get the same results across runs for the same seed.\n",
    "\n",
    "When using with other Sklearn transformers or estimators that have their own `random_state`, you should pass a seed number both to XGBoost and other classes for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What are objectives in XGBoost and how to specify them for different tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both regression and classification tasks have different types and implementations. They change depending on the objective function, the distributions they can work with and their loss function.\n",
    "\n",
    "For example, regression can be performed using RMSE (Root Mean Squared Error), RMSLE (Root Mean Squared Log Error), Huber Error, etc. loss functions. Sklearn implements different regressors for each of these but in XGBoost, these are all packed into XGBRegressor estimator. \n",
    "\n",
    "You can switch between the implementations of different loss functions, supported distributions with the `objective` parameter. It accepts special code strings provided by XGBoost. Most commons ones are:\n",
    "\n",
    "- `reg:squarederror`\n",
    "- `reg:squaredlogerror`\n",
    "- `reg:gamma`\n",
    "- `reg:tweedie`\n",
    "\n",
    "Similarly, classification objectives change based on their underlying loss function. These objectives start either with `binary:*` or `multi:*` prefixes depending on the target cardinality. \n",
    "\n",
    "There are many other objective types and I will leave it to you to explore the rest and find out the details using this documentation [link](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters).\n",
    "\n",
    "> Note that specifying the correct objective gets rid of that unbelievably annoying warning you get when fitting XGB classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
