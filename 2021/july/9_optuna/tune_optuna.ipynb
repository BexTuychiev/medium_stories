{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Is Everyone at Kaggle Obsessed with Optuna For Hyperparameter Tuning?\n",
    "## Let's find out by trying it out...\n",
    "![](images/pixabay.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://pixabay.com/users/bomei615-2623913/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1751855'>Bo Mei</a>\n",
    "        on \n",
    "        <a href='https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1751855'>Pixabay.</a> All images are by author unless specified otherwise.\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, I have been living under a rock...\n",
    "\n",
    "Every single MOOC I have taken taught me to use GridSearch for hyperparameter tuning. Naively, I loved it and went the extra mile to learn its cousins - Randomized GridSearch and Halving GridSearch.\n",
    "\n",
    "Even though they were great, I somehow tried to avoid hyperparameter tuning as much as possible. Why?\n",
    "\n",
    "First, they take such a damn long time to train. We are talking in multiples of 24-hour sessions if you are doing an exhaustive GridSearch. If you are not, then there is a pretty high chance randomized search comes up with hyperparameters that are actually worse than the defaults.\n",
    "\n",
    "While I was complaining, Kagglers have been using Optuna almost exclusively for the past 2 years to do hyperparameter tuning. \n",
    "\n",
    "After giving it a try, I am truly amazed at how it takes the whole tuning experience to the next level. So, without further ado, let me show you how to use it in your own workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Optuna?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Optuna logo\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna is a next-generation automatic hyperparameter tuning framework, written completely in Python.\n",
    "\n",
    "Its most prominent features are:\n",
    "- the ability to define more Pythonic search spaces using loops and conditionals. \n",
    "- completely platform agnostic API - using Optuna, you can tune estimators of almost any ML, DL package/framework including Sklearn, PyTorch, TensorFlow, Keras, XGBoost, LightGBM, CatBoost, etc.\n",
    "- a large suite of optimization algorithms with early stopping and pruning features baked in.\n",
    "- easy parallelization with little or no changes to the code.\n",
    "- built-in support for visual exploration of search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's familiarize ourselves with Optuna by tuning a simple function like $(x-1)^2 + (y+3)^2$. We know the function converges to its minimum at x=1 and y=-3. Let's see if Optuna can find these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  # pip install optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -7, 7)\n",
    "    y = trial.suggest_float(\"y\", -7, 7)\n",
    "    return (x - 1) ** 2 + (y + 3) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing `optuna`, we define an objective that returns the function we want to minimize. \n",
    "\n",
    "In the body of the objective, we define the parameters to be optimized, in this case simple `x` and `y`. The argument `trial` is a special Trial object of optuna which does the optimization for each hyperparameter. \n",
    "\n",
    "Along many others, it has a `suggest_float` function which takes the name of the hyperparameter and the range to look for its optimal value. In other words\n",
    "\n",
    "```\n",
    "x = trial.suggest_float(\"x\", -7, 7)\n",
    "```\n",
    "is almost the same as `{\"x\": np.arange(-7, 7)}` when doing GridSearch.\n",
    "\n",
    "To start the optimization, we create a `study` object from Optuna and pass the `objective` function to its `optimize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)  # number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 0.8806708977164549, 'y': -3.0941841160297767}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close but not as close as you would want. Here, we only did 100 trials, as can be seen with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(study.trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will introduce the first magic that comes with Optuna. We can resume the optimization even after it is finished if we are not satisfied with the results! \n",
    "\n",
    "This is a huge advantage over other tools because after the search is done, they completely forget the history of previous trials. Optuna does not!\n",
    "\n",
    "To continue searching, just call `optimize` again with the desired params. Here, we will add 100 more trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 1.045706853335669, 'y': -2.9501109059847512}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are much closer to the optimal parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
