{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Is Everyone at Kaggle Obsessed with Optuna For Hyperparameter Tuning?\n",
    "## Let's find out by trying it out...\n",
    "![](images/pixabay.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://pixabay.com/users/bomei615-2623913/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1751855'>Bo Mei</a>\n",
    "        on \n",
    "        <a href='https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1751855'>Pixabay.</a> All images are by author unless specified otherwise.\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, I have been living under a rock...\n",
    "\n",
    "Every single MOOC I have taken taught me to use GridSearch for hyperparameter tuning. Naively, I loved it and went the extra mile to learn its cousins - Randomized GridSearch and Halving GridSearch.\n",
    "\n",
    "Even though they were great, I somehow tried to avoid hyperparameter tuning as much as possible. Why?\n",
    "\n",
    "First, they take such a damn long time to train. We are talking in multiples of 24-hour sessions if you are doing an exhaustive GridSearch. If you are not, then there is a pretty high chance randomized search comes up with hyperparameters that are actually worse than the defaults.\n",
    "\n",
    "While I was complaining, Kagglers have been using Optuna almost exclusively for the past 2 years to do hyperparameter tuning. \n",
    "\n",
    "After giving it a try, I am truly amazed at how it takes the whole tuning experience to the next level. So, without further ado, let me show you how to use it in your own workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Optuna?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Optuna logo\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna is a next-generation automatic hyperparameter tuning framework, written completely in Python.\n",
    "\n",
    "Its most prominent features are:\n",
    "- the ability to define Pythonic search spaces using loops and conditionals. \n",
    "- completely platform agnostic API - using Optuna, you can tune estimators of almost any ML, DL package/framework including Sklearn, PyTorch, TensorFlow, Keras, XGBoost, LightGBM, CatBoost, etc.\n",
    "- a large suite of optimization algorithms with early stopping and pruning features baked in.\n",
    "- easy parallelization with little or no changes to the code.\n",
    "- built-in support for visual exploration of search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's familiarize ourselves with Optuna API by tuning a simple function like $(x-1)^2 + (y+3)^2$. We know the function converges to its minimum at x=1 and y=-3. Let's see if Optuna can find these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  # pip install optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -7, 7)\n",
    "    y = trial.suggest_float(\"y\", -7, 7)\n",
    "    return (x - 1) ** 2 + (y + 3) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing `optuna`, we define an objective that returns the function we want to minimize. \n",
    "\n",
    "In the body of the objective, we define the parameters to be optimized, in this case simple `x` and `y`. The argument `trial` is a special Trial object of optuna which does the optimization for each hyperparameter. \n",
    "\n",
    "Along many others, it has a `suggest_float` method which takes the name of the hyperparameter and the range to look for its optimal value. In other words\n",
    "\n",
    "```\n",
    "x = trial.suggest_float(\"x\", -7, 7)\n",
    "```\n",
    "is almost the same as `{\"x\": np.arange(-7, 7)}` when doing GridSearch.\n",
    "\n",
    "To start the optimization, we create a `study` object from Optuna and pass the `objective` function to its `optimize` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)  # number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 0.8806708977164549, 'y': -3.0941841160297767}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close but not as close as you would want. Here, we only did 100 trials, as can be seen with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(study.trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will introduce the first magic that comes with Optuna. We can resume the optimization even after it is finished if we are not satisfied with the results! \n",
    "\n",
    "This is a huge advantage over other tools because after the search is done, they completely forget the history of previous trials. Optuna does not!\n",
    "\n",
    "To continue searching, just call `optimize` again with the desired params. Here, we will add 100 more trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 1.045706853335669, 'y': -2.9501109059847512}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are much closer to the optimal parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on Optuna terminology and conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Optuna, the whole optimization process is called a *study*. For example, tuning XGBoost parameters with a log loss as a metric is one study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optuna.study.Study"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "type(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study needs a function it can optimize. Typically, this function is defined by the user and by convention, it should be named `objective`. \n",
    "\n",
    "The objective function is expected to have this signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    \"\"\"Conventional optimization function\n",
    "    signature for optuna.\n",
    "    \"\"\"\n",
    "    custom_metric = ...\n",
    "    return custom_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should accept `optuna.Trial` object as a parameter and return the metric we want to optimize for. \n",
    "\n",
    "As we saw in the first example, a study is a collection of *trials* where in each trial, we evaluate the objective function using a single set of hyperparameters from the given search space. \n",
    "\n",
    "Each trial in the study is represented as `optuna.Trial` class. This class is key to how Optuna finds optimal values for parameters. \n",
    "\n",
    "To start a study, we create a study object with `direction`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the metric we want to optimize is a point-performance score like ROC AUC or accuracy, we set the direction to `maximize`. Otherwise, we minimize a loss function like RMSE, RMSLE, log loss, etc. by setting direction to `minimize`. \n",
    "\n",
    "Then, we will call the `optimize` method of the study passing the objective function name and the number of \n",
    "trials we want:\n",
    "\n",
    "```python\n",
    "# Optimization with 100 trials\n",
    "study.optimize(objective, n_trials=100)\n",
    "```\n",
    "\n",
    "Next, we will take a closer look into creating the objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the first thing you do in an objective function is to create the search space using built-in Optuna methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    rf_params = {\n",
    "        \"n_estimators\": trial.suggest_integer(name=\"n_estimators\", low=100, high=2000),\n",
    "        \"max_depth\": trial.suggest_float(\"max_depth\", 3, 8),\n",
    "        \"max_features\": trial.suggest_categorical(\n",
    "            \"max_features\", choices=[\"auto\", \"sqrt\", \"log2\"]\n",
    "        ),\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 1121218,\n",
    "    }\n",
    "\n",
    "    rf = RandomForestRegressor(**rf_params)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above objective function, we are creating a small search space of Random Forest hyperparameters. \n",
    "\n",
    "The search space is a plain-old dictionary. To create possible values to search over, you must use trial object's `suggest_*` functions. \n",
    "\n",
    "These functions require at least the hyperparameter name, min and max of the range to search over or possible categories for categorical hyperparameters.\n",
    "\n",
    "To make the space smaller, `suggest_float` and `suggest_int` have additional `step` or `log` arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 10000, step=200),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e7, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12, step=2),\n",
    "        \"random_state\": 1121218,\n",
    "    }\n",
    "    boost_reg = GradientBoostingRegressor(**params)\n",
    "    rmsle = ...\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we are binning the distribution of `n_estimators` by 200-intervals to make it sparser. Also, `learning_rate` is defined at a logarithmic scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are possible parameters sampled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, Optuna has a number of classes responsible for parameter sampling. These are:\n",
    "- `GridSampler`: the same as GridSearch of Sklearn. Never use for large search spaces!\n",
    "- `RandomSampler`: the same as RandomizedGridSearch of Sklearn.\n",
    "- `TPESampler`: Tree-structured Parzen Estimator sampler - bayesian optimization using kernel fitting\n",
    "- `CmaEsSampler`: a sampler based on CMA ES algorithm (does not allow categorical hyperparameters).\n",
    "\n",
    "> I have no idea of how the last two samplers work and I don't expect this to affect any interaction I have with Optuna. \n",
    "\n",
    "Just know that TPE Sampler is used by default - it tries to sample hyperparameter candidates by trying to improve on the last trial's scores. In other words, you can expect an incremental (maybe marginal) iprovements from trial to trial with this sampler.\n",
    "\n",
    "If you ever want to switch samplers, this is how you do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import CmaEsSampler, RandomSampler\n",
    "\n",
    "# Study with a random sampler\n",
    "study = optuna.create_study(sampler=RandomSampler(seed=1121218))\n",
    "\n",
    "# Study with a CMA ES sampler\n",
    "study = optuna.create_study(sampler=CmaEsSampler(seed=1121218))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end example with GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything we have learned into something tangible. We will be predicting penguin body weights using a number of numeric and categorical features. \n",
    "\n",
    "We will establish a base score with Sklearn GradientBoostingRegressor and improve it by tuning with Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_validate, train_test_split\n",
    "\n",
    "# Load data\n",
    "penguins = sns.load_dataset(\"penguins\").dropna()\n",
    "X, y = penguins.drop(\"body_mass_g\", axis=1), penguins[[\"body_mass_g\"]]\n",
    "\n",
    "# OH encode categoricals\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Init model with defaults\n",
    "gr_reg = GradientBoostingRegressor(random_state=1121218)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1121218)\n",
    "scores = cross_validate(\n",
    "    gr_reg, X, y, cv=kf, scoring=\"neg_mean_squared_log_error\", n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base RMSLE: 0.07573\n"
     ]
    }
   ],
   "source": [
    "rmsle = np.sqrt(-scores[\"test_score\"].mean())\n",
    "print(f\"Base RMSLE: {rmsle:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create the `objective` function and define the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y, cv, scoring):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 5000, step=100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.9, step=0.1),\n",
    "        \"max_features\": trial.suggest_categorical(\n",
    "            \"max_features\", [\"auto\", \"sqrt\", \"log2\"]\n",
    "        ),\n",
    "        \"random_state\": 1121218,\n",
    "        \"n_iter_no_change\": 50,  # early stopping\n",
    "        \"validation_fraction\": 0.05,\n",
    "    }\n",
    "    # Perform CV\n",
    "    gr_reg = GradientBoostingRegressor(**params)\n",
    "    scores = cross_validate(gr_reg, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    # Compute RMSLE\n",
    "    rmsle = np.sqrt(-scores[\"test_score\"].mean())\n",
    "\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This objective function is slightly different - it accepts additional arguments for the data sets, scoring and `cv`. That's why we have to wrap it inside another function. Generally, you do this with a `lambda` function like below (only if the `objective` accepts more than just `trial` object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create study that minimizes\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "# Wrap the objective inside a lambda with the relevant arguments\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1121218)\n",
    "# Pass additional arguments inside another function\n",
    "func = lambda trial: objective(trial, X, y, cv=kf, scoring=\"neg_mean_squared_log_error\")\n",
    "\n",
    "# Start optimizing with 100 trials\n",
    "study.optimize(func, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base RMSLE    : 0.07573\n",
      "Optimized RMSE: 0.07177\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base RMSLE    : {rmsle:.5f}\")\n",
    "print(f\"Optimized RMSE: {study.best_value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In just over a minute, we achieved a significant score boost (in terms of log errors, 0.004 is pretty huge). We did this with just 100 trials. Let's boldly give another 200 trials and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "study.optimize(func, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base RMSLE    : 0.07573\n",
      "Optimized RMSE: 0.07155\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base RMSLE    : {rmsle:.5f}\")\n",
    "print(f\"Optimized RMSE: {study.best_value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score did improve but marginally. Looks like we hit it close to the max in the first run!\n",
    "\n",
    "Most importantly, we achieved this score in just 2 minutes using a search space which would probably take hours with regular GridSearch. \n",
    "\n",
    "I don't know about you, but I am sold!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
