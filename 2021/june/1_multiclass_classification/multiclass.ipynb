{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate Guide to Multiclass Classification With Sklearn\n",
    "## Model selection, developing strategy and evaluation metrics\n",
    "![](./images/pexels.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://www.pexels.com/@sergiu-iacob-10475786?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Sergiu Iacob</a>\n",
    "        on \n",
    "        <a href='https://www.pexels.com/photo/wave-dark-abstract-motion-7868341/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though multi-class classification is not as common, it certainly poses a much bigger challenge than binary classification problems. Many of the well-known strategies for solving multi-class problems breaks down the task into several or multiple (yes, in this case, there is a difference and I will explain) binary classification problems. \n",
    "\n",
    "After that, there is the issue of choosing an evaluation metric which accurately shows the model's performance across all classes. Since we are dealing with multiple binary classifiers, these metrics tend to get pretty complex. Finally, you must do hyperparameter tuning to optimize for a particular metric. Well, how can you do that if you don't know what you are optimizing for in the first place?\n",
    "\n",
    "For these reasons, this article will be about an end-to-end tutorial on how to solve any multi-class supervised classification problem using Sklearn. You will learn:\n",
    "- the methods Sklearn offers to binarize a multi-class problem\n",
    "- quick overview of the preprocessing steps required\n",
    "- how to evaluate a default model of your choice\n",
    "- details of multi-class classification metrics and finally,\n",
    "- how to maximize model performance for a particular metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which strategy to choose: One-vs-One or One-vs-Rest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many algorithms such as Logistic Regression, Support Vector Machines do not support multi-class classification natively. Even if you were blindly using Sklearn classifiers for a multi-class problem, Sklearn fits a *number* of binary classifiers to different versions of the training data under the hood. \n",
    "\n",
    "This *number* depends on what type of strategy your model uses to binarize the problem. There are two strategies which produce different number of binary classifiers:\n",
    "1. One-vs-One: this strategy splits multi-class problem into a single binary classifier for each pair of classes (more on this in a bit). Sklearn implements this strategy in `OneVsOneClassifier` (OVO) which takes a binary classifier as input. Here is an example on a synthetic dataset involving a LogisticRegression estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "# Create a dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=8, n_redundant=2, n_classes=4\n",
    ")\n",
    "\n",
    "# Init the strategy\n",
    "clf = OneVsOneClassifier(estimator=LogisticRegression())\n",
    "# Fit\n",
    "_ = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the 3 classes we created above refer to lung, breast, kidney and brain cancers. In this case, OVO creates 6 individual LogisticRegression models:\n",
    "- Classifier 1: lung vs breast\n",
    "- Classifier 2: lung vs kidney\n",
    "- Classifier 3: lung vs brain\n",
    "- Classifier 4: breast vs kidney\n",
    "- Classifier 5: breast vs brain\n",
    "- Classifier 6: kidney vs brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Print the number of estimators created\n",
    "print(len(clf.estimators_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the number of binary classifiers created for N-class classification problem can be found using this formula: \n",
    "![](./images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might guess, this strategy can be computationally expensive because the number of binary classifiers grow exponentially when the target has high cardinality. Therefore, the second approach is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. One-vs-All or One-vs-Rest (OVR). For N-class classification problem, this strategy creates N number of binary classifiers, one for each class. For the cancer example with 4 target classes:\n",
    "- Classifier 1: lung vs \\[breast, kidney, brain\\]\n",
    "- Classifier 2: breast vs \\[lung, kidney, brain\\]\n",
    "- Classifier 3: kidney vs \\[lung, breast, brain\\]\n",
    "- Classifier 4: brain vs \\[lung, breast kidney\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first problem, Sklearn treats lung class as the positive and encodes it as 1 and the rest of class gets converted to 0s. The same pattern continues for all classes in N-class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its implementation in Sklearn can be found under `sklearn.multiclass`. Here is an example of OVR on our synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Init/fit\n",
    "clf = OneVsRestClassifier(estimator=LogisticRegression())\n",
    "_ = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(),\n",
       " LogisticRegression(),\n",
       " LogisticRegression(),\n",
       " LogisticRegression()]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this strategy significantly lowers the computational cost, the fact that only one class is considered positive and the rest negative makes each binary problem an *imbalanced classification*. This problem is even more pronounced for classes with low proportions in the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both approaches, depending on the passed estimator, the results of all binary classifiers can be summarized in two ways:\n",
    "- majority of the vote: each binary classifier predicts one class and the class that got the most votes from all classifiers is chosen\n",
    "- depending on the argmax of class membership probability scores: classifiers such as LogisticRegression computes probability scores for each class (`.predict_proba()`). Then, the argmax of the sum of the scores is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tree-based and ensemble models support multi-class classification natively. So, there is no need to wrap them either in OVO or OVR. However, regardless of model type, these strategies are still essential when we are talking about the evaluation metrics in the coming sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
