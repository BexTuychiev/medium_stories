{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Sklearn Mistakes That Silently Tell You Are a Rookie\n",
    "## No error messages - that's what makes them subtle...\n",
    "![](images/unsplash.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@santabarbara77?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Varvara Grabova</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/mistake?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"figure.figsize\"] = [12, 9]\n",
    "rcParams[\"figure.autolayout\"] = True\n",
    "rcParams[\"xtick.labelsize\"] = 15\n",
    "rcParams[\"ytick.labelsize\"] = 15\n",
    "rcParams[\"legend.fontsize\"] = \"small\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=5,\n",
    "    n_informative=4,\n",
    "    n_redundant=1,\n",
    "    n_classes=3,\n",
    "    weights=[0.4, 0.4, 0.3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `fit` or `fit_transform` everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the most serious mistake - a mistake that is related to *data leakage*. Data leakage is subtle and can be destructive to model performance. It occurs when information that would not be available at prediction time is used during the model training. Data leakage causes models to give very optimistic results, even in cross-validation but perform terribly when testing on actual novel data. \n",
    "\n",
    "Data leakage is common during data preprocessing, particularly if the training and test sets are not separated. Many Sklearn preprocessing transformers such as imputers, normalizers, standardization functions and log transformers tap into the underlying distribution of the data during the fit time. \n",
    "\n",
    "For example, `StandardScaler` normalizes the data by subtracting the mean from each sample and dividing by the standard deviation. Calling the `fit()` function on the full data (X) allows the transformer to learn the mean and standard deviation of the whole distribution of each feature. After transformation, if this data is then split into train and test sets, the train set would be contaminated because `StandardScaler` leaked important information from the actual distribution. \n",
    "\n",
    "Even though this might not be apparent to us, Sklearn algorithms are powerful enough to notice this and take advantage during testing. In other words, the train data would be too perfect for the model because it has useful information of the test set and the test would not be novel enough to test the model's performance on actual unseen data.\n",
    "\n",
    "The easiest solution is to never call `fit` on the full data. Before doing any preprocessing, always split the data into train and test sets. Even after the split, you should never call `fit` or `fit_transform` on the test set because you will end up at the same problem. \n",
    "\n",
    "Since both train and test sets should receive the same preprocessing steps, a golden rule is to use `fit_transform` on the train data - this ensures that the transformer learns only from the train set and transforms it simultaneously. Then, call the `transform` method on the test set to transform it based on the information learned only from the training data.\n",
    "\n",
    "A more robust solution would be using Sklearn's built-in pipelines. Pipeline classes are specifically built to guard algorithms from data leakage. Using pipelines ensures that only the training data is used during `fit` and the test data is used only for calculations. You can learn about them in detail in my separate article:\n",
    "\n",
    "https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=your_stories_page-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judging Model Performance Only By Test Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got a test score over 0.85 - should you be celebrating? Big, fat NO!\n",
    "\n",
    "Even though high test scores generally mean robust performance, there are important caveats to interpreting test results. First and most importantly, regardless of the value, test scores should only be judged based on the score you get from training.\n",
    "\n",
    "The only time you should be happy with your model is when the training score is higher than the test score and both are high enough to satisfy the expectations of your unique case. However, this does not imply that the higher the difference between train and test scores, the better. \n",
    "\n",
    "For example, 0.85 training score and 0.8 test score suggests a robust model that is neither overfit nor underfit. But, if the training score is over 0.9 and the test score is 0.8, your model is overfit - instead of generalizing during training, the model memorized some of the training data resulting in a much lower test score than training. You will often see such cases with tree-based and ensemble models. Algorithms such as Random Forests tend to achieve very high training scores if their tree depth is not controlled which leads to overfitting. You can read [this discussion](https://stats.stackexchange.com/questions/156694/how-can-training-and-testing-error-comparisons-be-indicative-of-overfitting?noredirect=1&lq=1) on StackExchange to learn more about this difference between train and test scores.\n",
    "\n",
    "There is also the case where the test score is higher than train. If the test score is higher than the test score even in the slightest, feel alarmed because you made a blunder! The major cause of such scenarios is data leakage and we discussed an example of that in the last section. \n",
    "\n",
    "Sometimes, it is also possible to get a good training score and extremely low testing score. When the difference between train and test score is unusually large, the problem will often be associated with the test set rather than overfitting. One reason this might happen is using different preprocessing steps for the train and test sets, or simply forgetting to apply preprocessing to the test set.\n",
    "\n",
    "In summary, always examine the gap between train and test scores closely. Doing so will tell you whether you should apply regularization to overcome overfitting, look for possible mistakes you made during preprocessing or the best case scenario, prepare the model for final evaluation and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Incorrect Train/Test Sets in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "A common mistake among beginners is forgetting to generate stratified train and test sets for classification. \n",
    "\n",
    "A model is more likely to generate correct predictions when the distribution of the new data matches that of training as much as possible. In classification, we only care about the class weights or proportions. For example, in 3-class classification problem, let's say the class weights of the full data are 0.4, 0.3, 0.3. When we divide this data into train and test sets, the distributions of both sets should reflect the distribution of the full data. \n",
    "\n",
    "We commonly use `train_test_split` function of Sklearn to divide the data and Sklearn provides handy argument - `stratify` to generate stratified splits. Here is an example train/test sets with and without stratified splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.4\n",
       "0    0.4\n",
       "2    0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the class weights before splitting\n",
    "pd.Series(y).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.42\n",
       "1    0.38\n",
       "2    0.20\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate unstratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# Look at the class weights of train set\n",
    "pd.Series(y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.46\n",
       "0    0.34\n",
       "2    0.20\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the class weights of the test set\n",
    "pd.Series(y_test).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both train and test sets have different class weights for the first and second classes. Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.4\n",
       "0    0.4\n",
       "2    0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "# Train set class weights\n",
    "pd.Series(y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.4\n",
       "0    0.4\n",
       "2    0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set class weights\n",
    "pd.Series(y_test).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `stratify` to the target (`y`) yielded identical distributions in both the train and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altered class weights are a serious problem that might make a model make more biased towards a particular class. Forgetting to generate stratified splits might result in more favorable train or test sets or cause problems such as these:\n",
    "\n",
    "![](https://miro.medium.com/proxy/1*QrFi76DR3mka7c_akr7WFw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the performance of a KNN classifier I have built when I have just started learning Sklearn. As you can see, almost all test scores are higher than training because I had forgotten to generate stratified splits. As a result, the test set yielded too favorable distribution for my model to take advantage. After fixing the problem:\n",
    "\n",
    "![](https://miro.medium.com/proxy/1*MOBa9yEvtppQwdgMT7ZvZg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is all back to normal. \n",
    "\n",
    "When using cross-validation or pipelines, you don't have to worry about this problem, because CV splitters perform stratification under the hood using `StratifiedKFold` for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `LabelEcoder` to Encode the X array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ever got annoyed when you found out that `LabelEncoder` encodes categorical columns only one at a time? Compared to other text transformers such as `OneHotEncoder` which can transform multiple features at the same time, this seems kind of a let down by Sklearn. \n",
    "\n",
    "But I am here to tell you that it isn't! It is simply the result of your inexperience and your unwillingness to read the awesome documentation of Sklearn. Here is an excerpt of `LabelEncoder`'s 2-sentence documentation:\n",
    "\n",
    "> This transformer should be used to encode target values, i.e. `y`, and not the input `X`.\n",
    "\n",
    "Then, what do you use to encode ordinal text features? If you kindly move on to the Sklearn user guide on encoding categorical features, you will see that it clearly states:\n",
    "\n",
    "> To convert categorical features to integer codes, we can use the `OrdinalEncoder`. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1)\n",
    "\n",
    "Using `OrdinalEncoder` allows us to transform multiple columns at once as expected and it has the benefit of being able to integrate into Pipeline instances, which `LabelEncoder` cannot. The encoder follows the familiar transformer API of Sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder(handle_unknown='ignore')\n",
    "\n",
    "X_train = oe.fit_transform(X_train)\n",
    "X_test = oe.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn a lot about Sklearn by just reading the documentation and the user guide!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judging Model Performance Without Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Default Scorers to Evaluate the Performance of a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Variance Thresholding Without Normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
