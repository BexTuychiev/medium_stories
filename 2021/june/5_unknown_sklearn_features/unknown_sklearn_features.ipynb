{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Sklearn Features You Didn't Know About | P(Guarantee) = 0.75\n",
    "## 1. EllipticalEnvelope, 3. ExtraTrees, 7. Perceptron, 11. IsolationForest, 13. RobustScaler\n",
    "![](images/unsplash.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@alevisionco?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>alevision.co</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/secret?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"figure.figsize\"] = [12, 9]\n",
    "rcParams[\"figure.autolayout\"] = True\n",
    "rcParams[\"xtick.labelsize\"] = 15\n",
    "rcParams[\"ytick.labelsize\"] = 15\n",
    "rcParams[\"legend.fontsize\"] = \"small\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(1121218)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [covariance.EllipticEnvelope](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common for distributions to have outliers. There are many algorithms that deal with outliers and `EllipticalEnvelope` is an example that is directly built-in to Sklearn. The advantage of this algorithm is that it performs exceptionally well at detecting outliers in normally distributed (Gaussian) features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1,  1,  1,  1, -1, -1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Create a sample normal distribution\n",
    "X = np.random.normal(loc=5, scale=2, size=50).reshape(-1, 1)\n",
    "\n",
    "# Fit the estimator\n",
    "ee = EllipticEnvelope(random_state=0)\n",
    "_ = ee.fit(X)\n",
    "\n",
    "# Test\n",
    "test = np.array([6, 8, 20, 4, 5, 6, 10, 13]).reshape(-1, 1)\n",
    "\n",
    "# predict returns 1 for an inlier and -1 for an outlier\n",
    "ee.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the estimator, we are creating a normal distribution with a mean of 5 and a standard deviation of 2. After it is trained, we pass some random numbers to its `predict` method. The method returns -1 for outliers in the `test` - 20, 10, 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. feature_selection.RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is an invaluable skill in machine learning. Selecting the features that help with predictions the most is a must step to combat overfitting and reduce model complexity. There are various techniques for feature selection that work based on the properties of each feature or the relationships between them. However, there is a more robust algorithm offered by Sklearn - Recursive Feature Elimination (RFE). It automatically finds the most important features by using cross-validation and discards the rest.\n",
    "\n",
    "An advantage of this estimator is that it is a wrapper - it can be used around any Sklearn algorithm that returns feature importance or coefficient scores. Here is an example on a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Build a synthetic dataset\n",
    "X, y = make_regression(n_samples=10000, n_features=15, n_informative=10)\n",
    "\n",
    "# Init/fit the selector\n",
    "rfecv = RFECV(estimator=BayesianRidge(), cv=5)\n",
    "_ = rfecv.fit(X, y)\n",
    "# Transform the feature array\n",
    "rfecv.transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fake dataset has 15 features, 10 of which are informative, the rest are redundant. We are fitting 5-fold RFECV with `BayesianRidge` as an estimator. After training, you can use the `transform` method to discard the redundant features. Calling `.shape` shows us that the estimator managed to drop all 5 unnecessary features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have wrote an [entire article](https://towardsdatascience.com/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e?source=your_stories_page-------------------------------------) on this algorithm that covers the nitty-gritty details of how it works with a real-world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ensemble.ExtraTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests is one of the most powerful algorithms in classic machine learning. It uses multiple decision trees and reduces overfitting by averaging the prediction scores. Each tree is trained on a random subset of the samples and during the construction of each tree, random subset of features are chosen to generate the node splits. By using these two sources of randomness, RF tries to decrease variance because its main disadvantage is that it tends to overfit.  \n",
    "\n",
    "Regardless, in RF, the risk of overfitting is still high. Therefore, Sklearn offers a drop-in alternative to RF called ExtraTrees (both classifier and regressor). The word 'extra' does not mean more trees but more randomness. The algorithm uses another type of trees that closely resemble decision trees. The only difference is that instead of calculating the split thresholds while building each tree, these thresholds are drawn randomly for each feature and the best of these thresholds is chosen as a splitting rule. This allows to reduce the variance a bit at the cost of slight increase in bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6376080094392635"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, y = make_regression(n_samples=10000, n_features=20)\n",
    "\n",
    "# Decision trees\n",
    "clf = DecisionTreeRegressor(max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446103607404536"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "clf = RandomForestRegressor(\n",
    "    n_estimators=10, max_depth=None, min_samples_split=2, random_state=0\n",
    ")\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8737373931608834"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ExtraTrees\n",
    "clf = ExtraTreesRegressor(\n",
    "    n_estimators=10, max_depth=None, min_samples_split=2, random_state=0\n",
    ")\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. impute.IterativeImputer and KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. linear_model.HuberRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. tree.plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. linear_model.Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. feature_selection.SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. model_selection.TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. metrics.ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. ensemble.IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. OVR and OVO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. preprocessing.RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. compose.make_column_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. compose.make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. preprocessing.OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. metrics.get_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. utils.class_weight.compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  20. model_selection.HalvingGrid and HalvingRandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. preprocessing.PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. sklearn.utils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
