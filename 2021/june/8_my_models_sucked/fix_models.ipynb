{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Machine Learning Models Just SUCKED. Here Is How I Fixed Them\n",
    "## Here are 7 things I learned to consistently achieve over 90 point-performance\n",
    "![](images/unsplash.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@wannabephotographer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Walid Hamadeh</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/suck?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remember taking my first ML course on Kaggle. I was introduced to Decision Trees and performed my first \"serious\" regression task on the over-used Ames Housing dataset. I was so happy! I even went as far as thinking that Machine Learning was not so hard after all... What a noob!\n",
    "\n",
    "Turns out, Decision Trees was like the \"Hello World\" in ML and I had only dipped my pinkie toe into the world of beautiful math and data. Since then, I have learned and improved a lot (or I think I have). \n",
    "\n",
    "Now, I am not just blindly training my favorite models based on the target. I have moved away from writing template code and started taking data preprocessing more seriously. Because of these changes and many others, my models started achieving robust results, often upwards of 85 point-performance, even with large datasets.\n",
    "\n",
    "So, in this article, I will lay out 7 most important things I learned to consistently push my models to squeeze every bit of performance increase. ## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find out the hyperparameters that control overfitting/underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the first thing I learned about the ML world is the issue of overfitting and how to combat it. Generating a robust model that generalizes well is a step-by-step process and the initial stage starts right at the model initialization. \n",
    "\n",
    "After choosing your baseline model(s), search for its parameters that influences its objective function the most. Often, these hyperparameters are the ones that directly affect model's learning and most importantly, how it generalizes.\n",
    "\n",
    "The best way to do this is to read the documentation of the model thoroughly. After reading enough documentation, you will find out that there are certain keywords that immediately suggest the parameter is related to controlling overfitting. \n",
    "\n",
    "For example, tree-based and ensemble models use the term \"prune\" to control the depth of each tree. RandomForests have `n_estimators`, `max_features` that control the build of each tree. Sklearn user guide also says `max_depth` and `min_samples_split` are important.\n",
    "\n",
    "For linear models, most common keywords include *regularization*, *penalty*, etc. LogisticRegression and Linear SVMs have the `C` - the inverse of regularization strength or `alpha` and `gamma` hyperparameters that exist in all SVMs. \n",
    "\n",
    "For establishing base performance, you can simply use the default values or values that are suggested on the documentation. Often, these values are not optimal and should be tuned with hyperparameter optimizers at the last stage of your workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
