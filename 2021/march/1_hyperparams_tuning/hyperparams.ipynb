{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Hyperparameter Tuning with Sklearn Using Grid and Random Search\n",
    "## Grid and Random Search vs. Halving Search in Sklearn\n",
    "<img src='images/4.jpg'></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://www.pexels.com/@wildlittlethingsphoto?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Helena Lopes</a>\n",
    "        on \n",
    "        <a href='https://www.pexels.com/photo/four-person-standing-on-cliff-in-front-of-sun-697243/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from colorama import Back, Fore, Style\n",
    "from matplotlib import rcParams\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "rcParams[\"axes.spines.top\"] = False\n",
    "rcParams[\"axes.spines.right\"] = False\n",
    "rcParams[\"figure.figsize\"] = [12, 9]\n",
    "rcParams[\"figure.dpi\"] = 300\n",
    "rcParams[\"figure.autolayout\"] = True\n",
    "rcParams[\"font.style\"] = 16\n",
    "rcParams[\"xtick.labelsize\"] = 10\n",
    "rcParams[\"ytick.labelsize\"] = 10\n",
    "custom_palette = [\"#221f1f\", \"#b20710\", \"#e50914\", \"#f5f5f1\"]\n",
    "sns.set_palette(custom_palette)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.options.display.max_columns = 12\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "np.random.seed(1121218)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, algorithms that hide a world of math under the hood can be trained with only a few lines of code. Their success depends first on the data trained and then, on what hyperparameters that were used by the user. So, what are these hyperparameters?\n",
    "\n",
    "Hyperparameters are user-defined values like [*k* in kNN](https://towardsdatascience.com/intro-to-scikit-learns-k-nearest-neighbors-classifier-and-regressor-4228d8d1cba6?source=your_stories_page-------------------------------------) and *alpha* in [Ridge and Lasso regression](https://towardsdatascience.com/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a?source=your_stories_page-------------------------------------). They strictly control the fit of the model and this means, for each dataset, there is a unique set of optimal hyperparameters to be found. The most basic way of finding this perfect set would be randomly trying out different values based on gut feeling. However, as you might guess, this method quickly becomes useless when there are many hyperparameters to tune.\n",
    "\n",
    "Instead, today you will learn about two methods for automatic hyperparameter tuning: Random search and Grid search. Given a set of possible values for all hyperparameters of a model, Grid search fits a model using every single combination of these hyperparameters. What is more, in each fit, Grid search uses cross-validation to account for overfitting. After all combinations are tried out, the search retains the parameters that resulted in best scores so that you can use them to build your final model.\n",
    "\n",
    "Random search takes a bit different approach than Grid. Instead of exhaustively trying out every single combination of hyperparameters, which can be computationally-expensive and time-consuming, it randomly samples hyperparameters and tries to get closer to the best set. \n",
    "\n",
    "Fortunately, Scikit-learn provides `GridSearchCV` and `RandomizedSearchCV` classes that make this process a breeze. Today, you will learn all about them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepping the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will be tuning a RandomForestRegressor model on [Iowa housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). I chose Random Forests because it has large enough hyperparameters that make this guide more informative but the process you will be learning can be applied to any model in the Sklearn API. So, let's start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>...</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street  ... MiscVal MoSold  \\\n",
       "0   1          60       RL         65.0     8450   Pave  ...       0      2   \n",
       "1   2          20       RL         80.0     9600   Pave  ...       0      5   \n",
       "2   3          60       RL         68.0    11250   Pave  ...       0      9   \n",
       "3   4          70       RL         60.0     9550   Pave  ...       0      2   \n",
       "4   5          60       RL         84.0    14260   Pave  ...       0     12   \n",
       "\n",
       "  YrSold SaleType SaleCondition SalePrice  \n",
       "0   2008       WD        Normal    208500  \n",
       "1   2007       WD        Normal    181500  \n",
       "2   2008       WD        Normal    223500  \n",
       "3   2006       WD       Abnorml    140000  \n",
       "4   2008       WD        Normal    250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_train = pd.read_csv(\"data/train.csv\")\n",
    "houses_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "houses_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is `SalePrice`. For simplicity, I will choose only numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = houses_train.select_dtypes(include=\"number\").drop(\"SalePrice\", axis=1)\n",
    "y = houses_train.SalePrice\n",
    "\n",
    "X_test = houses_test.select_dtypes(include=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, both training and test sets contain missing values. We will use `SimpleImputer` to deal with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute both train and test sets\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X = imputer.fit_transform(X)\n",
    "X_test = imputer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit a base `RandomForestRegressor` with default parameters. As we will use the test set only for final evaluation, I will create a separate validation set using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for training set: 0.9785951576271396\n",
      "R2 for validation set: 0.832622375495487\n",
      "\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Fit a base model\n",
    "forest = RandomForestRegressor()\n",
    "\n",
    "_ = forest.fit(X_train, y_train)\n",
    "\n",
    "print(f\"R2 for training set: {forest.score(X_train, y_train)}\")\n",
    "print(f\"R2 for validation set: {forest.score(X_valid, y_valid)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The main focus of this article is on how to perform hyperparameter tuning. We won't worry about other topics like overfitting or feature engineering but only narrow down on how to use Random and Grid search so that you can apply automatic hyperparameter tuning in real-life setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a 0.83 for R2 on the test set. We fit the regressor only with default parameters which are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of hyperparameters. We won't be tweaking all of them but focus only on the most important ones. Specifically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_esimators` - number of trees to be used\n",
    "- `max_feauters` - the number of features to use at each node plit\n",
    "- `max_depth`: the number of leaves on each tree\n",
    "- `min_samples_split`: the minimum number of samples required to split an internal node\n",
    "- `min_samples_leaf`: the minimum number of samples in each leaf\n",
    "- `bootstrap`: method of sampling - with or without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Grid Search and Random Search tries to find the optimal values for each of these hyperparameters. Let's see this in action first with Random Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search with Sklearn RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides `RandomizedSearchCV` class to implement random search. It requires two arguments to set up: an estimator and the set of possible values for hyperparameters called a *parameter grid* or *space*. Let's define this parameter grid for our random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000, 1100,\n",
       "        1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900]),\n",
       " 'max_features': ['auto', 'sqrt', 'log2'],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, None],\n",
       " 'min_samples_split': array([2, 4, 6, 8]),\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = np.arange(100, 2000, step=100)\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "max_depth = list(np.arange(10, 100, step=10)) + [None]\n",
    "min_samples_split = np.arange(2, 10, step=2)\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"max_features\": max_features,\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_samples_split\": min_samples_split,\n",
    "    \"min_samples_leaf\": min_samples_leaf,\n",
    "    \"bootstrap\": bootstrap,\n",
    "}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter grid dictionary should have hyperparameters as keys in the syntax they appear in the model's documentation. The possible values can be given as an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's finally import `RandomizedSearchCV` from `sklearn.model_selection` and instantiate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "\n",
    "random_cv = RandomizedSearchCV(\n",
    "    forest, param_grid, n_iter=100, cv=3, scoring=\"r2\", n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the accepted estimator and the parameter grid, it has `n_iter` parameter. It controls how many iterations of random picking of hyperparameter combinations we allow in the search. We set it to 100, so it will randomly sample 100 combinations and return the best score. We are also using a 3-fold cross-validation with coefficient of determination as scoring which is the default. You can pass any other scoring function from `sklearn.metrics.SCORERS.keys()`. Now, let's start the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, since Randomized Search performs cross-validation, we can fit it on the training data as a whole. Because of how CV works, it will create separate sets for training and evaluation. Also, I am setting `n_jobs` to -1 to use all cores on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "\n",
      "{'n_estimators': 800, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': False}\n",
      "Wall time: 16min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = random_cv.fit(X, y)\n",
    "\n",
    "print(\"Best params:\\n\")\n",
    "print(random_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ~17 minutes of training, the best params found can be accessed with `.best_params_` attribute. We can also see the best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8690868090696587"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got around 87% coefficient of determination which is an improvement by 4% over the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
