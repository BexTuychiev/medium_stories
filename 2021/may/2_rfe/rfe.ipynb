{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Powerful Feature Selection with Recursive Feature Elimination (RFE) of Sklearn\n",
    "## Feature selection based on single model performance\n",
    "<img src='images/unsplash.jpg'></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@victoriano?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Victoriano Izquierdo</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/selection?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic methods of feature selection are mostly about individual properties of features and how they interact with each other. [*Variance thresholding*](https://towardsdatascience.com/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f?source=your_stories_page-------------------------------------) and [*pairwise feature selection*](https://towardsdatascience.com/how-to-use-pairwise-correlation-for-robust-feature-selection-20a60ef7d10?source=your_stories_page-------------------------------------) are a few examples that remove unnecessary features based on the amount of variance and the correlation between features. However, a more pragmatic approach would select features based on how they affect a particular model's performance. One such technique offered by Sklearn is Recursive Feature Elimination (RFE). It reduces model complexity by removing features one by one until the desired number of features are left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea behind Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this subset of [Ansur Male dataset](https://www.kaggle.com/seshadrikolluri/ansur-ii):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wristcircumference</th>\n",
       "      <th>wristheight</th>\n",
       "      <th>SubjectNumericRace</th>\n",
       "      <th>DODRace</th>\n",
       "      <th>Age</th>\n",
       "      <th>Heightin</th>\n",
       "      <th>Weightlbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>853</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>71</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>815</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>68</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180</td>\n",
       "      <td>831</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>68</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176</td>\n",
       "      <td>793</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>66</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>954</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wristcircumference  wristheight  SubjectNumericRace  DODRace  Age  \\\n",
       "0                 175          853                   1        1   41   \n",
       "1                 167          815                   1        1   35   \n",
       "2                 180          831                   2        2   42   \n",
       "3                 176          793                   1        1   31   \n",
       "4                 188          954                   2        2   21   \n",
       "\n",
       "   Heightin  Weightlbs  \n",
       "0        71        180  \n",
       "1        68        160  \n",
       "2        68        205  \n",
       "3        66        175  \n",
       "4        77        213  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansur = pd.read_csv(\"data/ansur_male.csv\", encoding=\"latin\").select_dtypes(\n",
    "    include=\"number\"\n",
    ")\n",
    "ansur.iloc[:, -7:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It records more than 100 different types of body measurements of more than 6000 US Army Personnel. Our goal is to predict the weight in pounds using only the numeric features (there are 93) for simplicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish a base performance with Random Forest Regressor. We will first build the feature and target arrays and divide them into train and test sets. Then, we will fit the estimator and score its performance using R-squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9485677708139042"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature, target arrays\n",
    "X, y = ansur.iloc[:, :-1], ansur.iloc[:, -1]\n",
    "\n",
    "# Train/test set generation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1121218\n",
    ")\n",
    "\n",
    "# Scale train and test sets with StandardScaler\n",
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_test_std = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# Fix the dimensions of the target array\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Init, fit, test Lasso Regressor\n",
    "forest = RandomForestRegressor()\n",
    "_ = forest.fit(X_train_std, y_train)\n",
    "forest.score(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved a really good R-squared of 0.948. We were able to do this using all 98 features which is much more than we might need. All Sklearn estimators have special attributes that show feature weights (or coefficients), either given as `coef_` or `.feature_importances_`. Let's see the computed coefficients for our Random Forest Regressor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heightin</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suprasternaleheight</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>crotchheight</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DODRace</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cervicaleheight</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>forearmforearmbreadth</td>\n",
       "      <td>0.001464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>elbowrestheight</td>\n",
       "      <td>0.001519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>forearmcircumferenceflexed</td>\n",
       "      <td>0.003284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>bideltoidbreadth</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>weightkg</td>\n",
       "      <td>0.940290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature    weight\n",
       "0                     Heightin  0.000097\n",
       "1          suprasternaleheight  0.000170\n",
       "2                 crotchheight  0.000180\n",
       "3                      DODRace  0.000182\n",
       "4              cervicaleheight  0.000212\n",
       "..                         ...       ...\n",
       "93       forearmforearmbreadth  0.001464\n",
       "94             elbowrestheight  0.001519\n",
       "95  forearmcircumferenceflexed  0.003284\n",
       "96            bideltoidbreadth  0.005946\n",
       "97                    weightkg  0.940290\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    zip(X_train.columns, abs(forest.feature_importances_)),\n",
    "    columns=[\"feature\", \"weight\"],\n",
    ").sort_values(\"weight\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce model complexity, always start by removing features with close to 0 weights. Since all weights are multiplied by the values of features, such small weights contribute very little to the overall predictions. Looking at the above weights, we can see that many weights are close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could set some low threshold and filter out features based on it. But we have to remember that even removing a single feature forces other coefficients to change. So, we have to eliminate them step-by-step, leaving out lowest weighted feature by sorting the fitted models coefficients. Doing this manually for 98 features would be cumbersome, but thankfully Sklearn provides us with Recursive Feature Elimination - [RFE class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) to do the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Recursive Feature Elimination Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE is a transformer estimator which means it follows the familiar fit/transform pattern of Sklearn. It is a popular algorithm due to its easy configurable nature and robust performance. As the name suggests, it removes features one at a time based on the weights given by a model of our choice in each iteration. \n",
    "\n",
    "Below, you will see an example of RFE using the above Random Forest Regressor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Init the transformer\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=10)\n",
    "\n",
    "# Fit to the training data\n",
    "_ = rfe.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the estimator, it has a `.support_` attribute that gives a boolean mask with False values for discarded features. We can use it to subset our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bideltoidbreadth</th>\n",
       "      <th>bizygomaticbreadth</th>\n",
       "      <th>chestcircumference</th>\n",
       "      <th>forearmcircumferenceflexed</th>\n",
       "      <th>forearmforearmbreadth</th>\n",
       "      <th>handcircumference</th>\n",
       "      <th>neckcircumferencebase</th>\n",
       "      <th>span</th>\n",
       "      <th>weightkg</th>\n",
       "      <th>wristheight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>477</td>\n",
       "      <td>146</td>\n",
       "      <td>974</td>\n",
       "      <td>307</td>\n",
       "      <td>527</td>\n",
       "      <td>196</td>\n",
       "      <td>421</td>\n",
       "      <td>1756</td>\n",
       "      <td>712</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>518</td>\n",
       "      <td>132</td>\n",
       "      <td>1172</td>\n",
       "      <td>295</td>\n",
       "      <td>616</td>\n",
       "      <td>201</td>\n",
       "      <td>450</td>\n",
       "      <td>1790</td>\n",
       "      <td>917</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>506</td>\n",
       "      <td>138</td>\n",
       "      <td>1146</td>\n",
       "      <td>288</td>\n",
       "      <td>611</td>\n",
       "      <td>208</td>\n",
       "      <td>410</td>\n",
       "      <td>1760</td>\n",
       "      <td>858</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>496</td>\n",
       "      <td>136</td>\n",
       "      <td>1053</td>\n",
       "      <td>341</td>\n",
       "      <td>552</td>\n",
       "      <td>212</td>\n",
       "      <td>448</td>\n",
       "      <td>1816</td>\n",
       "      <td>952</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>445</td>\n",
       "      <td>131</td>\n",
       "      <td>946</td>\n",
       "      <td>269</td>\n",
       "      <td>512</td>\n",
       "      <td>208</td>\n",
       "      <td>387</td>\n",
       "      <td>1737</td>\n",
       "      <td>673</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>512</td>\n",
       "      <td>150</td>\n",
       "      <td>1067</td>\n",
       "      <td>286</td>\n",
       "      <td>602</td>\n",
       "      <td>196</td>\n",
       "      <td>403</td>\n",
       "      <td>1706</td>\n",
       "      <td>750</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>523</td>\n",
       "      <td>143</td>\n",
       "      <td>1049</td>\n",
       "      <td>311</td>\n",
       "      <td>608</td>\n",
       "      <td>213</td>\n",
       "      <td>453</td>\n",
       "      <td>1819</td>\n",
       "      <td>878</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>538</td>\n",
       "      <td>137</td>\n",
       "      <td>1076</td>\n",
       "      <td>312</td>\n",
       "      <td>584</td>\n",
       "      <td>205</td>\n",
       "      <td>447</td>\n",
       "      <td>1880</td>\n",
       "      <td>892</td>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>521</td>\n",
       "      <td>144</td>\n",
       "      <td>1066</td>\n",
       "      <td>316</td>\n",
       "      <td>572</td>\n",
       "      <td>207</td>\n",
       "      <td>423</td>\n",
       "      <td>1807</td>\n",
       "      <td>916</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>572</td>\n",
       "      <td>136</td>\n",
       "      <td>1152</td>\n",
       "      <td>324</td>\n",
       "      <td>643</td>\n",
       "      <td>222</td>\n",
       "      <td>460</td>\n",
       "      <td>1909</td>\n",
       "      <td>1071</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2857 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bideltoidbreadth  bizygomaticbreadth  chestcircumference  \\\n",
       "2132               477                 146                 974   \n",
       "3107               518                 132                1172   \n",
       "3111               506                 138                1146   \n",
       "3975               496                 136                1053   \n",
       "2921               445                 131                 946   \n",
       "...                ...                 ...                 ...   \n",
       "619                512                 150                1067   \n",
       "588                523                 143                1049   \n",
       "3467               538                 137                1076   \n",
       "1442               521                 144                1066   \n",
       "2787               572                 136                1152   \n",
       "\n",
       "      forearmcircumferenceflexed  forearmforearmbreadth  handcircumference  \\\n",
       "2132                         307                    527                196   \n",
       "3107                         295                    616                201   \n",
       "3111                         288                    611                208   \n",
       "3975                         341                    552                212   \n",
       "2921                         269                    512                208   \n",
       "...                          ...                    ...                ...   \n",
       "619                          286                    602                196   \n",
       "588                          311                    608                213   \n",
       "3467                         312                    584                205   \n",
       "1442                         316                    572                207   \n",
       "2787                         324                    643                222   \n",
       "\n",
       "      neckcircumferencebase  span  weightkg  wristheight  \n",
       "2132                    421  1756       712          768  \n",
       "3107                    450  1790       917          852  \n",
       "3111                    410  1760       858          845  \n",
       "3975                    448  1816       952          885  \n",
       "2921                    387  1737       673          894  \n",
       "...                     ...   ...       ...          ...  \n",
       "619                     403  1706       750          759  \n",
       "588                     453  1819       878          858  \n",
       "3467                    447  1880       892          930  \n",
       "1442                    423  1807       916          874  \n",
       "2787                    460  1909      1071          896  \n",
       "\n",
       "[2857 rows x 10 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.loc[:, rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can directly call `.transform()` to get a new `numpy` array with the relevant features. Let's use this smaller subset to test Random Forest Regressor once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94878355877858"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init, fit, score\n",
    "forest = RandomForestRegressor()\n",
    "_ = forest.fit(rfe.transform(X_train_std), y_train)\n",
    "forest.score(rfe.transform(X_test_std), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after dropping almost 90 features, we got the same score which is very impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE Performance Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since RFE trains the given model on the full dataset every time it drops a feature, the computation time will be heavy for large datasets with many features like ours. To control this behavior, RFE provides `step` parameter which lets use drop arbitrary number of features in each iteration instead of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the transformer\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=10, step=10)\n",
    "_ = rfe.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bideltoidbreadth', 'elbowrestheight', 'forearmcircumferenceflexed',\n",
       "       'forearmforearmbreadth', 'handcircumference', 'handlength',\n",
       "       'neckcircumference', 'span', 'weightkg', 'wristheight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of features to keep automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important hyperparameters of RFE are *estimator* and *n_features_to_select*. In the last example, we arbitrarily chose 10 features and hoped for the best. However, as RFE can be wrapped around any model, we have to choose the number of relevant features based on their performance.\n",
    "\n",
    "To achieve this, Sklearn provides a similar `RFECV` class which implements Recursive Feature Elimination with cross-validation and automatically finds the optimal number of features to keep. Below is an example that uses RFECV around a simple Linear Regression. We will be choosing Linear regression because we can guess that body measurements will be linear correlated. Besides, combined with cross-validation, Random Forest Regressor will become more computationally expensive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Init, fit\n",
    "rfecv = RFECV(\n",
    "    estimator=LinearRegression(),\n",
    "    min_features_to_select=5,\n",
    "    step=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"r2\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "_ = rfecv.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I provided the default values to `cv` and `scoring` parameters. A new hyperparameter is `min_features_to_select` - you can probably guess what it does from the name. Let's see how many features the estimator computed to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bideltoidbreadth', 'shouldercircumference', 'tibialheight',\n",
       "       'waistheightomphalion', 'weightkg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[rfecv.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RFECV` tells us to keep only 5 out of 98. Let's train the model only on those 5 and look at its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainign R-sqaured: 0.9380873069456624\n",
      "Testing R-squared: 0.9565258599872342\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train_std, y_train)\n",
    "print(\"Trainign R-sqaured:\", lr.score(X_train_std, y_train))\n",
    "print(\"Testing R-squared:\",lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after dropping 93 features, we still got an impressive score of 0.956. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reading this tutorial, you learned:\n",
    "- the idea behind Recursive Feature Elimination is\n",
    "- how to use the implementation of the algorithm using Sklearn RFE class\n",
    "- how to decide the number of features to keep automatically using RFECV class\n",
    "\n",
    "If you want a deeper look at the algorithm, you can read this [post](https://machinelearningmastery.com/rfe-feature-selection-in-python/).\n",
    "\n",
    "### Further reading on feature selection:\n",
    "- [How to Use Variance Thresholding For Robust Feature Selection]()\n",
    "- [How to Use Pairwise Correlation For Robust Feature Selection]()\n",
    "- [Recursive Feature Elimination (RFE) Sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    "- [RFECV Sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)\n",
    "\n",
    "### You might also be interested:\n",
    "- Article 1\n",
    "- Article 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
