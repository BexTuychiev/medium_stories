{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Powerful Feature Selection with Recursive Feature Elimination (RFE) of Sklearn\n",
    "## Feature selection based on single model performance\n",
    "<img src='images/unsplash.jpg'></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@victoriano?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Victoriano Izquierdo</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/selection?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic methods of feature selection are mostly about individual properties of features and how they interact with each other. [*Variance thresholding*](https://towardsdatascience.com/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f?source=your_stories_page-------------------------------------) and [*pairwise feature selection*](https://towardsdatascience.com/how-to-use-pairwise-correlation-for-robust-feature-selection-20a60ef7d10?source=your_stories_page-------------------------------------) are a few examples that remove unnecessary features based on the amount of variance and the correlation between features. However, a more pragmatic approach would select features based on how they affect a particular model's performance. One such technique offered by Sklearn is Recursive Feature Elimination (RFE). It reduces model complexity by removing features one by one until the desired number of features are left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea behind Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this subset of [Ansur Male dataset](https://www.kaggle.com/seshadrikolluri/ansur-ii):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wristcircumference</th>\n",
       "      <th>wristheight</th>\n",
       "      <th>SubjectNumericRace</th>\n",
       "      <th>DODRace</th>\n",
       "      <th>Age</th>\n",
       "      <th>Heightin</th>\n",
       "      <th>Weightlbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>853</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>71</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>815</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>68</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180</td>\n",
       "      <td>831</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>68</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176</td>\n",
       "      <td>793</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>66</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>954</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wristcircumference  wristheight  SubjectNumericRace  DODRace  Age  \\\n",
       "0                 175          853                   1        1   41   \n",
       "1                 167          815                   1        1   35   \n",
       "2                 180          831                   2        2   42   \n",
       "3                 176          793                   1        1   31   \n",
       "4                 188          954                   2        2   21   \n",
       "\n",
       "   Heightin  Weightlbs  \n",
       "0        71        180  \n",
       "1        68        160  \n",
       "2        68        205  \n",
       "3        66        175  \n",
       "4        77        213  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansur = pd.read_csv(\"data/ansur_male.csv\", encoding=\"latin\").select_dtypes(\n",
    "    include=\"number\"\n",
    ")\n",
    "ansur.iloc[:, -7:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It records more than 100 different types of body measurements of more than 6000 US Army Personnel. Our goal is to predict the weight in pounds using only the numeric features (there are 93) for simplicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish a base performance with Random Forest Regressor. We will first build the feature and target arrays and divide them into train and test sets. Then, we will fit the estimator and score its performance using R-squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9485677708139042"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature, target arrays\n",
    "X, y = ansur.iloc[:, :-1], ansur.iloc[:, -1]\n",
    "\n",
    "# Train/test set generation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1121218\n",
    ")\n",
    "\n",
    "# Scale train and test sets with StandardScaler\n",
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_test_std = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# Fix the dimensions of the target array\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Init, fit, test Lasso Regressor\n",
    "forest = RandomForestRegressor()\n",
    "_ = forest.fit(X_train_std, y_train)\n",
    "forest.score(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved a really good R-squared of 0.948. We were able to do this using all 98 features which is much more than we might need. All Sklearn estimators have special attributes that show feature weights (or coefficients), either given as `coef_` or `.feature_importances_`. Let's see the computed coefficients for our Random Forest Regressor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heightin</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suprasternaleheight</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>crotchheight</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DODRace</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cervicaleheight</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>forearmforearmbreadth</td>\n",
       "      <td>0.001464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>elbowrestheight</td>\n",
       "      <td>0.001519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>forearmcircumferenceflexed</td>\n",
       "      <td>0.003284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>bideltoidbreadth</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>weightkg</td>\n",
       "      <td>0.940290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature    weight\n",
       "0                     Heightin  0.000097\n",
       "1          suprasternaleheight  0.000170\n",
       "2                 crotchheight  0.000180\n",
       "3                      DODRace  0.000182\n",
       "4              cervicaleheight  0.000212\n",
       "..                         ...       ...\n",
       "93       forearmforearmbreadth  0.001464\n",
       "94             elbowrestheight  0.001519\n",
       "95  forearmcircumferenceflexed  0.003284\n",
       "96            bideltoidbreadth  0.005946\n",
       "97                    weightkg  0.940290\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    zip(X_train.columns, abs(forest.feature_importances_)),\n",
    "    columns=[\"feature\", \"weight\"],\n",
    ").sort_values(\"weight\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce model complexity, always start by removing features with close to 0 weights. Since all weights are multiplied by the values of features, such small weights contribute very little to the overall predictions. Looking at the above weights, we can see that many weights are close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could set some low threshold and filter out features based on it. But we have to remember that even removing a single feature forces other coefficients to change. So, we have to eliminate them step-by-step, leaving out lowest weighted feature by sorting the fitted models coefficients. Doing this manually for 98 features would be cumbersome, but thankfully Sklearn provides us with RFE class to do the task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
