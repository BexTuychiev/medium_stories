{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad-ass Feature Selection by Combining Multiple Models\n",
    "## Your favorite models choosing features themselves\n",
    "<img src='images/pexels.jpg'></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://www.pexels.com/@olly?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Andrea Piacquadio</a>\n",
    "        on \n",
    "        <a href='https://www.pexels.com/photo/crop-multiracial-people-joining-hands-together-during-break-in-modern-workplace-3931562/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X, y = tips.drop(\"total_bill\", axis=1), tips.total_bill\n",
    "for col in X.columns:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1121218, test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many feature selection methods in Machine Learning. Each one may give different results depending on how you use them, so it is hard to entirely trust a single method. Wouldn't be cool to have multiple methods cast their own vote on whether we should keep a feature or not? It would be just like Random Forests algorithm, where it combines the predictions of multiple weak learners to form a strong one. Turns out, Sklearn has already given us the tools to make such feature selector on our own. \n",
    "\n",
    "Together, using those tools, we will build a feature selector that accepts an arbitrary number of Sklearn models. All these models will give votes on which features we should keep and we make decision by gather all the votes across models (democracy?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite Knowledge to Build the Selector: Weights and Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to building the selector, let's brush up on some of the topics required. Firstly, almost all Sklearn estimators that yield predictions have either `.coef_` and `.feature_importances_` attributes after being fitted to the training data. \n",
    "\n",
    "`.coef_` attribute mostly occurs in models given under `sklearn.linear_model` submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.104,  1.193,  2.085,  0.19 , -2.142,  3.593])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Init, fit\n",
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train, y_train)\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, the above are *coefficients* calculated by fitting the line of best fit for Linear Regression. Other models follow a similar pattern and yield the coefficients of their internal equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.132,  0.   ,  0.   , -0.   , -0.   ,  2.137])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "_ = lasso.fit(X_train, y_train)\n",
    "\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models in `sklearn.tree` and `sklearn.ensemble` work differently and they compute the *importance* or *weight* of each feature under `.feature_importances_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.401 0.056 0.064 0.075 0.034 0.371]\n",
      "[0.515 0.018 0.071 0.017 0.012 0.368]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "for model in [dt, gb]:\n",
    "    _ = model.fit(X_train, y_train)\n",
    "    print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the coefficients of linear models, the weights add up to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dt.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the model, the feature contributes less and less to the overall prediction as its weight or coefficient decreases. This means that we can drop features with close to 0 coefficients or weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Overview of RFECVhttps://towardsdatascience.com/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e?source=your_stories_page-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (RFE) is a popular feature selection algorithm. It automatically finds the best number of features to keep to achieve the best performance for a given model. Below is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Init the estimator\n",
    "rfecv = RFECV(\n",
    "    estimator=Lasso(),\n",
    "    cv=3,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    min_features_to_select=2,\n",
    ")\n",
    "\n",
    "# Fit\n",
    "_ = rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting to the training data, it has `.support_` attribute which gives a boolean mask, with True values for the features that should be kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False,  True])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this mask to subset the original data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "X.loc[:, rfecv.support_]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of our custom feature selector will be this `RFECV` class. I didn't go into detail of how it works \n",
    "but my previous article solely focused on it. I recommend reading it before continuing:\n",
    "\n",
    "https://towardsdatascience.com/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e?source=your_stories_page-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Choosing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [Ansur Male](https://www.kaggle.com/seshadrikolluri/ansur-ii) dataset mainly because it contains many features (98 numerical) about body measurements of 6000 US Army Personnel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wristcircumference</th>\n",
       "      <th>wristheight</th>\n",
       "      <th>SubjectNumericRace</th>\n",
       "      <th>DODRace</th>\n",
       "      <th>Age</th>\n",
       "      <th>Heightin</th>\n",
       "      <th>Weightlbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>853</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>71</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>815</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>68</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180</td>\n",
       "      <td>831</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>68</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176</td>\n",
       "      <td>793</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>66</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>954</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wristcircumference  wristheight  SubjectNumericRace  DODRace  Age  \\\n",
       "0                 175          853                   1        1   41   \n",
       "1                 167          815                   1        1   35   \n",
       "2                 180          831                   2        2   42   \n",
       "3                 176          793                   1        1   31   \n",
       "4                 188          954                   2        2   21   \n",
       "\n",
       "   Heightin  Weightlbs  \n",
       "0        71        180  \n",
       "1        68        160  \n",
       "2        68        205  \n",
       "3        66        175  \n",
       "4        77        213  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ansur = pd.read_csv(\"data/ansur_male.csv\", encoding=\"latin\").select_dtypes(\n",
    "    include=\"number\"\n",
    ")\n",
    "ansur.iloc[:5, -7:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to predict weight in pounds and to do that we need to reduce model complexity - i. e. create a model with as much predictive power as possible using as few features as possible. Currently, there are 98 and we will be trying to decrease that number. Also, we will be dropping the column which records weight in kilograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansur.drop(\"weightkg\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model will be Lasso Regressor and we will plug it into `RFECV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True, False,  True, False, False,\n",
       "        True, False,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True,  True, False,\n",
       "       False,  True,  True, False, False, False, False, False, False,\n",
       "        True, False,  True,  True, False,  True, False,  True, False,\n",
       "        True,  True, False,  True,  True, False, False,  True, False,\n",
       "        True, False,  True, False, False, False,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True, False, False,  True,\n",
       "        True, False, False,  True,  True, False,  True,  True,  True,\n",
       "       False, False, False,  True, False, False,  True,  True, False,\n",
       "       False,  True, False, False, False,  True, False])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Feature/target arrays\n",
    "X, y = ansur.iloc[:, :-1], ansur.iloc[:, -1]\n",
    "\n",
    "# Generate train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1121218, test_size=0.25\n",
    ")\n",
    "\n",
    "# Init/fit\n",
    "rfecv = RFECV(\n",
    "    estimator=Lasso(), cv=5, scoring=\"r2\", min_features_to_select=1, n_jobs=-1\n",
    ")\n",
    "\n",
    "_ = rfecv.fit(X_train, y_train)\n",
    "\n",
    "lasso_mask = rfecv.support_\n",
    "lasso_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are storing the boolean mask generated from the Lasso in `lasso_mask`, and you are going to see why in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will do the same for two more models: Linear Regression and GradientBoostingRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "rfecv = RFECV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    cv=3,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    min_features_to_select=1,\n",
    ")\n",
    "_ = rfecv.fit(X_train, y_train)\n",
    "\n",
    "gb_mask = rfecv.support_\n",
    "\n",
    "# Simple Linear Regression\n",
    "rfecv = RFECV(\n",
    "    estimator=LinearRegression(),\n",
    "    cv=3,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    min_features_to_select=5,\n",
    ")\n",
    "_ = rfecv.fit(X_train, y_train)\n",
    "\n",
    "lr_mask = rfecv.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Combining the votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the votes as boolean masks in three arrays: `lasso_mask`, `gb_mask` and `lr_mask`. Since True/False values represent 1 and 0s, we can add the three arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 3, 3, 3, 2, 2, 2, 1, 3, 1, 3, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3,\n",
       "       1, 1, 3, 3, 1, 2, 3, 2, 2, 2, 2, 1, 0, 1, 3, 1, 3, 2, 2, 2, 2, 2,\n",
       "       2, 3, 3, 1, 2, 3, 2, 1, 2, 0, 3, 1, 2, 2, 1, 2, 3, 3, 3, 3, 1, 2,\n",
       "       1, 3, 3, 1, 1, 3, 2, 2, 1, 3, 3, 2, 3, 3, 2, 0, 2, 2, 3, 1, 1, 3,\n",
       "       3, 0, 1, 3, 1, 0, 1, 3, 2])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes = np.sum([lasso_mask, gb_mask, lr_mask], axis=0)\n",
    "votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be an array with counts of how many times each feature were chosen by all models. Now, we can set a threshold of votes to finally decide whether we will keep the feature or not. This threshold depends on conservative we want to be. We can set a strict threshold where we want the feature to have been chosen by all 3 or we can choose 1 as a threshold to be safe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True, False, False, False, False,\n",
       "        True, False,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True,  True, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "        True, False,  True, False, False, False, False, False, False,\n",
       "        True,  True, False, False,  True, False, False, False, False,\n",
       "        True, False, False, False, False, False,  True,  True,  True,\n",
       "        True, False, False, False,  True,  True, False, False,  True,\n",
       "       False, False, False,  True,  True, False,  True,  True, False,\n",
       "       False, False, False,  True, False, False,  True,  True, False,\n",
       "       False,  True, False, False, False,  True, False])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mask = votes == 3\n",
    "final_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `final_mask` is a boolean array with True values if a feature was chosen at least 1 by the 3 estimators. We can use it to subset the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4082, 39)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[:, final_mask].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the final votes chose 39 columns to keep out of 98. You can use this subset of the dataset to create a less complex model. For example, we will choose a Linear Regression model because we can expect body measurements to be linearly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9449234990234734"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new train/test sets from the feature selected data\n",
    "X_reduced = X.loc[:, final_mask]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y, random_state=1121218, test_size=0.25\n",
    ")\n",
    "\n",
    "# Fit/score\n",
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it takes some work to get to the final results it will be worth it. In the examples, we only chose 3 models but you can include as many models as you wish to make the results more robust and trustworthy.\n",
    "\n",
    "A logical step at this point is to wrap all this code in a function or even a custom Sklearn transformer but custom transformers are a topic for another article. To reinforce the ideas above and give you an outline, let's review the steps we have taken:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose arbitrary number of Sklearn estimators that have either `.coef_` or `.feature_importnances_` attributes. The more estimators the more robust the results will be. However, multiple models come at a cost - as `RFECV` uses [cross-validation](https://towardsdatascience.com/how-to-master-the-subtle-art-of-train-test-set-generation-7a8408bcd578) under the hood, the training times will be computationally expensive for ensemble models and large datasets. Also, make sure to choose estimators depending on the type of the problem - remember to pass either classification or regression-only estimators for `RFECV` to work.\n",
    "2. Plug all chosen models into `RFECV` class and make sure to save each round's boolean mask (accessed via `.support_`). To speed things up, you can tweak the `step` parameter so that an arbitrary number of features are dropped in each elimination round.\n",
    "3. Sum up the masks from all estimators. \n",
    "4. Set a threshold for vote count. This threshold depends on how conservative you want to be. Convert the votes array into a boolean mask using this threshold.\n",
    "5. Subset the original data using the final mask for final model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading Related to Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [How to Use Variance Thresholding For Robust Feature Selection](https://towardsdatascience.com/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f?source=your_stories_page-------------------------------------)\n",
    "- [How to Use Pairwise Correlation For Robust Feature Selection](https://towardsdatascience.com/how-to-use-pairwise-correlation-for-robust-feature-selection-20a60ef7d10?source=your_stories_page-------------------------------------)\n",
    "- [Powerful Feature Selection With Recursive Feature Elimination](https://towardsdatascience.com/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e)\n",
    "- [RFECV Sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)\n",
    "- [Sklearn Official Feature Selection User Guide](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You might also be interested...\n",
    "- [Intro to Object-Oriented-Programming For Data Scientists](https://towardsdev.com/intro-to-object-oriented-programming-for-data-scientists-9308e6b726a2?source=your_stories_page-------------------------------------)\n",
    "- [My 6-Part Powerful EDA Template That Speaks of Ultimate Skill](https://towardsdatascience.com/my-6-part-powerful-eda-template-that-speaks-of-ultimate-skill-6bdde3c91431?source=your_stories_page-------------------------------------)\n",
    "- [How to Use Sklearn Pipelines For Ridiculously Neat Code](https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=your_stories_page-------------------------------------)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
