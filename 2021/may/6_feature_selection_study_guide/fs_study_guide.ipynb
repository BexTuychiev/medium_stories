{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-part Practical Study Guide To Sklearn Feature Selection\n",
    "## Give me less than an hour to teach you a robust Feature Selection workflow\n",
    "![](./images/unsplash.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@aaronburden?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Aaron Burden</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/study?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, it is common for datasets to have hundreds if not thousands of features. On the surface, this might seem like a good thing ‚Äî more features give more information about each sample. But more often than not, these additional features don‚Äôt provide much value and introduce complexity.\n",
    "\n",
    "The biggest challenge of Machine Learning is to create models that have robust predictive power by using as few features as possible. But given the massive sizes of today‚Äôs datasets, it is easy to lose the oversight of which features are important and which ones aren‚Äôt.\n",
    "\n",
    "That‚Äôs why there is an entire skill to be learned in the ML field ‚Äî feature selection. Feature selection is the process of choosing a subset of the most important features while trying to retain as much information as possible (An excerpt from the [first article](https://towardsdatascience.com/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f) in this series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As feature selection is such a pressing issue there is a myriad of solutions you can *select* fromü§¶‚Äç‚ôÇÔ∏èü§¶‚Äç‚ôÇÔ∏è. To spare you some pain, I will teach you 4 feature selection techniques that when used together, can supercharge any model's performance. \n",
    "\n",
    "In this article, I will give you an overview of these techniques and how to readily use them without wondering too much about the internals. For a deeper understanding, I have written separate posts for each with the nitty-gritty explained. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to the dataset and the problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the [Ansur Male](https://www.kaggle.com/seshadrikolluri/ansur-ii) dataset wich contains more than 100 different body measurements of US Army Personnel. I have been using this dataset excessively throughout this feature selection series mainly because it contains 98 numeric features - a perfect dataset to teach feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightkg</th>\n",
       "      <th>wristcircumference</th>\n",
       "      <th>wristheight</th>\n",
       "      <th>SubjectNumericRace</th>\n",
       "      <th>DODRace</th>\n",
       "      <th>Age</th>\n",
       "      <th>Heightin</th>\n",
       "      <th>Weightlbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815</td>\n",
       "      <td>175</td>\n",
       "      <td>853</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>71</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>726</td>\n",
       "      <td>167</td>\n",
       "      <td>815</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>68</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>929</td>\n",
       "      <td>180</td>\n",
       "      <td>831</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>68</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>794</td>\n",
       "      <td>176</td>\n",
       "      <td>793</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>66</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>946</td>\n",
       "      <td>188</td>\n",
       "      <td>954</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weightkg  wristcircumference  wristheight  SubjectNumericRace  DODRace  Age  Heightin  Weightlbs\n",
       "0       815                 175          853                   1        1   41        71        180\n",
       "1       726                 167          815                   1        1   35        68        160\n",
       "2       929                 180          831                   2        2   42        68        205\n",
       "3       794                 176          793                   1        1   31        66        175\n",
       "4       946                 188          954                   2        2   21        77        213"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ansur_numeric = pd.read_csv(\"data/ansur_male.csv\", encoding=\"latin\").select_dtypes(\n",
    "    include=\"number\"\n",
    ")\n",
    "\n",
    "ansur_numeric.iloc[:5, -8:]  ## A few of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to predict the weight in pounds, so it is a regression problem. Let's establish a base performance with simple Linear Regression. LR is a good candidate for this problem, because we can expect body measurements to be linearly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9566669936078463"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Build feature/target arrays\n",
    "X, y = ansur_numeric.iloc[:, :-1], ansur_numeric.iloc[:, -1]\n",
    "\n",
    "# Train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1121218\n",
    ")\n",
    "\n",
    "# Init LR, fit/score\n",
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base performance, we got an impressive R-squared of 0.956. However, this might be due to the fact that there is also a weight in kilograms column among features, giving the algorithm all it needs (we are trying to predict weight in pounds). So, let's try without that feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9468586903272265"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop weightkg\n",
    "X.drop(\"weightkg\", axis=1, inplace=True)\n",
    "\n",
    "# Train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1121218\n",
    ")\n",
    "\n",
    "# Init LR, fit/score\n",
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have 0.945 but we managed to reduce model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step I: Variance Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first technique will be targeted at the individual properties of each feature. The idea behind Variance Thresholding is that features with low variance do not contribute much to overall predictions. These type of features have distributions with too few unique values or low-enough variances to make no matter. VT helps us to remove them from a dataset using Sklearn.\n",
    "\n",
    "One concern before applying VT is the scale of features. As the values in a feature get bigger the variance grows exponentially. This means that features with different distributions have different scales so we cannot safely compare their variances. So, we are required to apply some form of normalization to bring all features to the same scale and then apply VT. Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4082, 47)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Normalize data\n",
    "normalized_df = X / X.mean()\n",
    "\n",
    "# Init, fit VT\n",
    "vt = VarianceThreshold(threshold=0.003)\n",
    "_ = vt.fit(normalized_df)\n",
    "\n",
    "# Get a boolean mask\n",
    "mask = vt.get_support()\n",
    "\n",
    "# Subset the data\n",
    "X_reduced = X.loc[:, mask]\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalization (here, we are dividing each sample by the feature's mean), you should choose a threshold between 0 and 1. Instead of using the `.transform()` method of the VT estimator, we are using `get_support()` which gives a boolean mask (True values for features that should be kept). Then, it can be used to subset the data while preserving the column names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be a simple technique but it can go a long in eliminating useless features. For a deeper insight and more explanation of the code, you can head over to this article:\n",
    "\n",
    "https://towardsdatascience.com/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step II: Pairwise Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will further trim our dataset by focusing on the relationships between features. One of the best metrics that show a *linear* connection is Pearson's correlation coefficient (denoted *r*). The logic behind using *r* for feature selection is simple. If the correlation between features A and B is 0.9 it means you can predict the values of B using the values of A 90% of the time. In other words, in a dataset where A is present you can discard B or vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/1.png)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by author\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't an Sklearn estimator that implements feature selection based on correlation. So, we will do it on our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def identify_correlated(df, threshold):\n",
    "    \"\"\"\n",
    "    A function to identify highly correlated features.\n",
    "    \"\"\"\n",
    "    # Compute correlation matrix with absolute values\n",
    "    matrix = df.corr().abs()\n",
    "\n",
    "    # Create a boolean mask\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "\n",
    "    # Subset the matrix\n",
    "    reduced_matrix = matrix.mask(mask)\n",
    "\n",
    "    # Find cols that meet the threshold\n",
    "    to_drop = [c for c in reduced_matrix.columns if any(reduced_matrix[c] > threshold)]\n",
    "\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a shorthand that returns the names of columns that should be dropped based on a custom correlation threshold. Usually, the threshold will be over 0.80 to be safe. \n",
    "\n",
    "In the function, we first create a correlation matrix using `.corr()`. Next, we create a boolean mask to only include correlations that are below the diagonal of the correlation matrix. We use this mask to subset the matrix. Finally, in a list comprehension, we find the names of features that should be dropped and return them.\n",
    "\n",
    "There is a lot I didn't explain about the code. Even though this function works perfectly well, I suggest reading my separate article on feature selection based on correlation coefficient. I fully explained the concept of correlation and how it is different from causation. There is a separate section on plotting the perfect correlation matrix as a heatmap and of course, the explanation of the above function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset, we will choose a threshold of 0.9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = identify_correlated(X_reduced, threshold=0.9)\n",
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function tells us to drop 13 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4082, 35)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, only 35 features are remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step III: Recursive Feature Elimination with Cross Validation (RFECV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will choose the final set of features based on how they affect model performance. Most of the Sklearn models have either `.coef_` (linear models) or `.feature_importances_` (tree-based and ensemble models) attributes that show the importance of each feature. For example, let's fit the Linear Regression model to the current set of features and see the computed coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subjectid</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SubjectNumericRace</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forearmforearmbreadth</td>\n",
       "      <td>0.007399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buttockdepth</td>\n",
       "      <td>0.008216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interscyeii</td>\n",
       "      <td>0.009153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>crotchlengthposterioromphalion</td>\n",
       "      <td>0.011324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lateralmalleolusheight</td>\n",
       "      <td>0.014132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>elbowrestheight</td>\n",
       "      <td>0.015555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DODRace</td>\n",
       "      <td>0.020661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lowerthighcircumference</td>\n",
       "      <td>0.029212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          feature  coefficient\n",
       "0                       subjectid     0.000107\n",
       "1              SubjectNumericRace     0.000336\n",
       "2           forearmforearmbreadth     0.007399\n",
       "3                    buttockdepth     0.008216\n",
       "4                     interscyeii     0.009153\n",
       "5  crotchlengthposterioromphalion     0.011324\n",
       "6          lateralmalleolusheight     0.014132\n",
       "7                 elbowrestheight     0.015555\n",
       "8                         DODRace     0.020661\n",
       "9         lowerthighcircumference     0.029212"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3)\n",
    "\n",
    "# Init/fit LR\n",
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train, y_train)\n",
    "\n",
    "# See the computed coefficients\n",
    "pd.DataFrame(\n",
    "    zip(X_train.columns, abs(lr.coef_)),\n",
    "    columns=[\"feature\", \"coefficient\"],\n",
    ").sort_values(\"coefficient\").reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above DataFrame shows the features with the smallest coefficients. The smaller the weight or the coefficient of a feature is, the less it contributes to the model's predictive power. With this idea in mind, Recursive Feature Elimination removes features one-by-one using cross-validation until the best smallest set of features is remaining.\n",
    "\n",
    "Sklearn implements this technique under the RFECV class which takes an arbitrary estimator and a number of other arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Init rfecv\n",
    "rfecv = RFECV(\n",
    "    estimator=LinearRegression(),\n",
    "    scoring=\"r2\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    min_features_to_select=5,\n",
    "    step=5,\n",
    ")\n",
    "\n",
    "# Fit\n",
    "_ = rfecv.fit(X_reduced, y)\n",
    "# Get a boolean mask for the features to keep\n",
    "mask = rfecv.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the estimator to the data, we can get a boolean mask with True values encoding the features that should be kept. We can finally use it to subset the original data one last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4082, 30)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final = X_reduced.loc[:, mask].copy()\n",
    "X_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying RFECV, we managed to discard 5 more features. Let's evaluate a final GradientBoostingRegressor model on this feature selected dataset and see its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 score: 0.9565125439275437\n",
      "Testing R2 score: 0.9245605254616158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# New train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.3, random_state=1121218\n",
    ")\n",
    "\n",
    "# Init, fit, score GB\n",
    "gb = GradientBoostingRegressor()\n",
    "_ = gb.fit(X_train, y_train)\n",
    "print(\"Training R2 score: {}\".format(gb.score(X_train, y_train)))\n",
    "print(\"Testing R2 score: {}\".format(gb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we got a slight drop in performance, we managed to remove almost 70 features reducing model complexity significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In a separate article, I further discussed the `.coef_` and `.feature_importances_` attributes as well as extra details of what happens in each elimination round of RFE:\n",
    "\n",
    "https://towardsdatascience.com/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary\n",
    "Feature selection should not be taken lightly. While reducing model complexity, some algorithms can even see an increase in the performance due to the lack of distracting features in the dataset. It is also not wise to rely on a single method. Instead, approach the problem from different angles and using various techniques. \n",
    "\n",
    "Today, we saw how to apply feature selection to a dataset in three stages:\n",
    "1. Based on the properties of each individual feature using Variance Thresholding.\n",
    "2. Based on the relationships between features using Pairwise Correlation.\n",
    "3. Based on how features affect a model's performance.\n",
    "\n",
    "Using these techniques in procession should give you reliable results for any type of supervised problem you face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading on Feature Selection\n",
    "- [Official guide to feature selection by Sklearn](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- [Variance Threshold Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)\n",
    "- [RFECV Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)\n",
    "- [Deep guide to Correlation Coefficients](https://towardsdev.com/how-to-not-misunderstand-correlation-75ce9b0289e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You might be interested...\n",
    "- [11 Times Faster Hyperparameter Tuning with HalvingGridSearch](https://towardsdatascience.com/11-times-faster-hyperparameter-tuning-with-halvinggridsearch-232ed0160155?source=your_stories_page-------------------------------------)\n",
    "- [Beginner‚Äôs Guide to XGBoost for Classification Problems](https://towardsdatascience.com/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390?source=your_stories_page-------------------------------------)\n",
    "- [Weekly Awesome Tricks And Best Practices From Kaggle](https://towardsdev.com/tricks-and-best-practices-from-kaggle-794a5914480f?source=your_stories_page-------------------------------------)\n",
    "- [Intro to Object-Oriented Programming For Data Scientists](https://towardsdev.com/intro-to-object-oriented-programming-for-data-scientists-9308e6b726a2?source=your_stories_page-------------------------------------)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
