{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-part Practical Study Guide To Sklearn Feature Selection\n",
    "## Give me less than an hour to teach you a robust Feature Selection workflow\n",
    "![](./images/unsplash.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@aaronburden?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Aaron Burden</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/study?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, it is common for datasets to have hundreds if not thousands of features. On the surface, this might seem like a good thing ‚Äî more features give more information about each sample. But more often than not, these additional features don‚Äôt provide much value and introduce complexity.\n",
    "\n",
    "The biggest challenge of Machine Learning is to create models that have robust predictive power by using as few features as possible. But given the massive sizes of today‚Äôs datasets, it is easy to lose the oversight of which features are important and which ones aren‚Äôt.\n",
    "\n",
    "That‚Äôs why there is an entire skill to be learned in the ML field ‚Äî feature selection. Feature selection is the process of choosing a subset of the most important features while trying to retain as much information as possible (An excerpt from the [first article](https://towardsdatascience.com/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f) in this series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As feature selection is such a pressing issue there is also a myriad of solutions you can *select* fromü§¶‚Äç‚ôÇÔ∏èü§¶‚Äç‚ôÇÔ∏è. To spare you some pain, I will teach you 4 feature selection techniques that when used together, can supercharge any model's performance. \n",
    "\n",
    "In this article, I will give you an overview of these techniques and how to readily use them without wondering too much about the internals. For a deeper understanding, I have written separate posts for each with the nitty-gritty explained. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to the dataset and the problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the [Ansur Male](https://www.kaggle.com/seshadrikolluri/ansur-ii) dataset wich contains more than 100 different body measurements of US Army Personnel. I have been using this dataset excessively throughout this feature selection series mainly because it contains 98 numeric features - a perfect dataset to teach feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightkg</th>\n",
       "      <th>wristcircumference</th>\n",
       "      <th>wristheight</th>\n",
       "      <th>SubjectNumericRace</th>\n",
       "      <th>DODRace</th>\n",
       "      <th>Age</th>\n",
       "      <th>Heightin</th>\n",
       "      <th>Weightlbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815</td>\n",
       "      <td>175</td>\n",
       "      <td>853</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>71</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>726</td>\n",
       "      <td>167</td>\n",
       "      <td>815</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>68</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>929</td>\n",
       "      <td>180</td>\n",
       "      <td>831</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>68</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>794</td>\n",
       "      <td>176</td>\n",
       "      <td>793</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>66</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>946</td>\n",
       "      <td>188</td>\n",
       "      <td>954</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weightkg  wristcircumference  wristheight  SubjectNumericRace  DODRace  Age  Heightin  Weightlbs\n",
       "0       815                 175          853                   1        1   41        71        180\n",
       "1       726                 167          815                   1        1   35        68        160\n",
       "2       929                 180          831                   2        2   42        68        205\n",
       "3       794                 176          793                   1        1   31        66        175\n",
       "4       946                 188          954                   2        2   21        77        213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ansur_numeric = pd.read_csv(\"data/ansur_male.csv\", encoding=\"latin\").select_dtypes(\n",
    "    include=\"number\"\n",
    ")\n",
    "\n",
    "ansur_numeric.iloc[:5, -8:]  ## A few of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to predict the weight in pounds, so it is a regression problem. Let's establish a base performance with simple Linear Regression. LR is a good candidate for this problem, because we can expect body measurements to be linearly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9566669936078463"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Build feature/target arrays\n",
    "X, y = ansur_numeric.iloc[:, :-1], ansur_numeric.iloc[:, -1]\n",
    "\n",
    "# Train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Init LR, fit/score\n",
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base performance, we got an impressive R-squared of 0.956. However, this might be due to the fact that there is also a weight in kilograms column among features, giving the algorithm all it needs (we are trying to predict weight in pounds). So, let's try without that feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9457535711339792"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Init LR, fit/score\n",
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train.drop(\"weightkg\", axis=1), y_train)\n",
    "lr.score(X_test.drop(\"weightkg\", axis=1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have 0.945 but we managed to reduce model complexity. The reason we are not dropping the column right now is that we want our feature selectors to figure that out on their own. Let's see if they will!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step I: Variance Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first technique will be targeted at the individual properties of each feature. The idea behind Variance Thresholding is that features with low variance do not contribute much to overall predictions. These type of features have distributions with too few unique values or low-enough variances to make no matter. VT helps us to remove them from a dataset using Sklearn.\n",
    "\n",
    "One concern before applying VT is the scale of features. As the values in a feature get bigger the variance grows exponentially. This means that features with different distributions have different scales so we cannot safely compare their variances. So, we are required to apply some form of normalization to bring all features to the same scale and then apply VT. Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4082, 48)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Normalize data\n",
    "normalized_df = X / X.mean()\n",
    "\n",
    "# Init, fit VT\n",
    "vt = VarianceThreshold(threshold=0.003)\n",
    "_ = vt.fit(normalized_df)\n",
    "\n",
    "# Get a boolean mask\n",
    "mask = vt.get_support()\n",
    "\n",
    "# Subset the data\n",
    "X_reduced = X.loc[:, mask]\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalization (here, we are dividing each sample by the feature's mean), you should choose a threshold between 0 and 1. Instead of using the `.transform()` method of the VT estimator, we are using `get_support()` which gives a boolean mask (True values for features that should be kept). Then, it can be used to subset the data while preserving the column names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be a simple technique but it can go a long in eliminating useless features. For a deeper insight and more explanation of the code, you can head over to this article:\n",
    "\n",
    "https://towardsdatascience.com/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
