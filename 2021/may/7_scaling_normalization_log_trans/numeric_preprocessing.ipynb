{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Real Difference Between Scaling, Normalization and Log Transformations with Sklearn\n",
    "## Become a pro in preprocessing your numeric features\n",
    "![](./images/pexels.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://www.pexels.com/@eye4dtail?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>George Becker</a>\n",
    "        on \n",
    "        <a href='https://www.pexels.com/photo/grayscale-photography-of-three-wise-monkey-figurines-134402/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(1121218)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it mean to conform to statistical approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have probably came across this in courses or articles: \n",
    "\n",
    "> The features in the dataset should conform to the statistical assumptions of the models.\n",
    "\n",
    "What does it mean to conform to statistical assumptions? Many models implemented in Sklearn might perform poorly if the numeric features do not more or less follow a standard Gaussian (normal) distribution. With the exception of tree-based models, the objective function of Sklearn algorithms *assume* the features follow a normal distribution. \n",
    "\n",
    "Actually, using the word *assume* would be putting things lightly. For models like K-Nearest-Neighbors, feature transformation is a requirement for the algorithm to perform expectedly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIDE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ansur_num = (\n",
    "    pd.read_csv(\"data/ansur_male.csv\", encoding=\"latin\")\n",
    "    .select_dtypes(include=\"number\")\n",
    "    .drop(\"weightkg\", axis=1)\n",
    ")\n",
    "\n",
    "X, y = ansur_num.iloc[:, :-1], ansur_num.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score of KNN before feature transformation: 0.8663552265106172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Before feature transforming\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Test score of KNN before feature transformation: {}\".format(\n",
    "        knn.score(X_test, y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score of KNN after feature transformation: 0.9166671893134017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Transform\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_scaled = ss.transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)\n",
    "\n",
    "# After feature transforming\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print(\n",
    "    \"Test score of KNN after feature transformation: {}\".format(\n",
    "        knn.score(X_test_scaled, y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you may even face scenarios where feature transformations have even larger effect than 5% increase in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many techniques you can apply to make your features more or less follow a normal distribution. They differ based on the underlying distributions of each feature. \n",
    "\n",
    "In this article, you will learn about 4 of such techniques: Scaling, normalization, logarithmic tranformers and outlier-based scaling. You will develop a practical understanding of their differences and when to apply them in your own workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
