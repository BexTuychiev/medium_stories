{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggler's Guide to LightGBM Hyperparameter Tuning with Optuna in 2021\n",
    "## Maximize LGBM's performance  TODO\n",
    "![](images/pixabay.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://www.pexels.com/@pixabay?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pixabay</a>\n",
    "        on \n",
    "        <a href='https://www.pexels.com/photo/silhouette-of-person-holding-sparkler-digital-wallpaepr-266429/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels.</a> All images are by the author unless specified otherwise.\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import catboost as cb\n",
    "import joblib\n",
    "import lightgbm as lgbm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    make_column_selector,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\", level=logging.INFO\n",
    ")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous article, we talked about the basics of LightGBM and creating LGBM models that beat XGBoost in almost every aspect. This article focuses on the last stage of any machine learning project - hyperparameter tuning (if we don't include model ensembling). \n",
    "\n",
    "First, we will take a look at the most important LGBM hyperparameters, grouped by their impact level and area. Then, we will see a hands-on example of tuning LGBM parameters using Optuna - the next-generation bayesian hyperparameter tuning framework. \n",
    "\n",
    "Most importantly, we will do this in a way that is similar to how top Kagglers tune their LGBM models that achieve impressive results (TODO THIS SENTENCE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I highly suggest reading the first part of the article if you are new to LGBM. Although I will briefly explain how Optuna works, I also recommend reading my separate post on it to get the best out this article.\n",
    "\n",
    "https://towardsdatascience.com/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c?source=your_stories_page-------------------------------------\n",
    "\n",
    "https://towardsdatascience.com/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c?source=your_stories_page-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the most important parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, hyperparameters of most tree-based model can be grouped into 4 categories:\n",
    "1. Parameters that affect the structure and learning of the decision trees\n",
    "2. Parameters that affect the training speed\n",
    "3. Parameters for better accuracy\n",
    "4. Parameters to combat overfitting\n",
    "\n",
    "Most of the time, parameters in these categories have a lot of overlap and increasing efficiency in one may come at the risk of decrease in another. That's why tuning them manually is a giant mistake and should be avoided at all costs. \n",
    "\n",
    "Frameworks like Optuna can find the \"sweet medium\" between these categories automatically if given a good enough parameter grid (and yes, we will develop this grid for LGBM today)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters that control the tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you are not familiar with decision trees, check out [this legendary video](https://www.youtube.com/watch?v=_L39rN6gz7Y) by StatQuest.\n",
    "\n",
    "In LGBM, the most important parameter to control the tree structure is `num_leaves`. As the name suggests, it controls the number of decision leaves in a single tree. Decision leaf of a tree is the node where the 'actual decision' happens.\n",
    "\n",
    "The next is `max_depth` that controls the tree depth. The higher `max_depth`, the more levels the tree has, which makes it more complex and prone to overfit. Too low and you will underfit. Even though it sounds complex, it is the easiest parameter to tune - just choose a value between 3 and 12 (this range tends to work well on Kaggle for any dataset).\n",
    "\n",
    "Tuning `num_leaves` can also be easy once you determine `max_depth`. There is a simple formula given in LGBM documentation - the maximum limit to `num_leaves` should be `2^(max_depth)`. This means the optimal value for `num_leaves` lies within the range ($2^3$, $2^{12}$) or (8, 4096). \n",
    "\n",
    "However, `num_leaves` impacts the learning in LGBM more than `max_depth`. This means you need to specify a more conservative search range like (20, 3000) - that's what I mostly do. \n",
    "\n",
    "Another important structural parameter for a tree is `min_data_in_leaf`. Its magnitude is also correlated to whether you overfit or not. In simple terms, `min_data_in_leaf` specifies the minimum number of observetaions that fit the decision criteria in a leaf.\n",
    "\n",
    "For example, if the decision leaf is checking wheter one feature is greater than, let's say, 13 - setting `min_data_in_leaf` to 100 means we want to evaluate this node only if there are at least 100 training observations that are bigger than 13. This is the gist in my lay terms.\n",
    "\n",
    "The optimal value for `min_data_in_leaf` depends on the number of training samples and `num_leaves`. For large datasets, set a value in hundreds or thousands.\n",
    "\n",
    "Check out [this section](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html#tune-parameters-for-the-leaf-wise-best-first-tree) of the LGBM documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common strategy for achieving higher accuracy is to use a large number of decision trees and decrease the learning rate. In other words, find the best combination of `n_estimators` and `learning_rate` in LGBM.\n",
    "\n",
    "`n_estimators` controls the number of decision trees while `learning_rate` is the step size parameter of the gradient descent. \n",
    "\n",
    "Ensembles like LGBM, build trees in iteration and each new tree is used to correct the \"errors\" of the previous trees. This approach is fast and powerful and prone to overfitting.\n",
    "\n",
    "That's why gradient boosted ensembles have a `learning_rate` parameter that controls the learning speed. Typical values lie within 0.01 and 0.3, but it is possible to go beyond these, epsecially towards 0. \n",
    "\n",
    "So, the perfect set up for these 2 parameters (`n_estimators` and `learning_rate`) is to use many trees with early stopping and set a low value for `learning_rate`. We will see an example later.\n",
    "\n",
    "You can also increase `max_bin` than the default (255) but again, at the risk of overfitting.\n",
    "\n",
    "Check out [this section](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html#for-better-accuracy) of the LGBM documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More hyperparameters to control overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM also has important regularization parameters.\n",
    "\n",
    "`lambda_l1` and `lambda_l2` specifies L1 or L2 regularization, like XGBoost's `reg_lambda` and `reg_alpha`. The optimal value for these parameters are a bit harder to tune since their magnitude is not directly correlated with overfitting. However, a good search range is (0, 100) for both.\n",
    "\n",
    "Next, we have `min_gain_to_split`, similar to XGBoost's `gamma`. A conservative search range is (0, 15). Can be used as extra regularization in large paramater grids.\n",
    "\n",
    "Lastly, we have `bagging_fraction` and `feature_fraction`. `bagging_fraction` takes a value within (0, 1) and specifies the percentage of training samples to be used to train each tree (exactly like `subsample` in XGBoost). To use this parameter, you also need to set `bagging_freq` to an integer value, explanation [here](https://lightgbm.readthedocs.io/en/latest/Parameters.html#:~:text=frequency%20for%20bagging).\n",
    "\n",
    "`feature_fraction` specifies the percentage of features to sample when training each tree. So, it also takes a value between (0, 1).\n",
    "\n",
    "We have already covered other parameters that affect overfitting (`max_depth`, `num_leaves`, etc.) in earlier sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the search grid in Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process in Optuna requires a function called *objective* that:\n",
    "- includes the parameter grid to search as a dictionary\n",
    "- creates a model to try hyperparameter combination sets\n",
    "- fits the model to the data with a single candidate set\n",
    "- generates predictions using this model\n",
    "- scores the predictions based on user-defined metric and returns it\n",
    "\n",
    "Here is how it looks like in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  # pip install optuna\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {}  # to be filled in later\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = lgbm.LGBMClassifier(objective=\"binary\", **param_grid)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "        preds = model.predict_proba(X_test)\n",
    "        cv_scores[idx] = preds\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above `objective` function, we haven't specified the grid yet. \n",
    "\n",
    "It is optional but we are performing training inside cross-validation. This ensures that each hyperparameter candidate set gets trained on all the data and evaluated more robustly. It also enables us to use early stopping. At the last line, we are returning the mean of the CV scores, which is what we want to optimize for.\n",
    "\n",
    "Let's focus on creating the grid now. We will include the hyperparameters introduced today with their recommended search ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 300),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.2, 0.95, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.95, step=0.1\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you don't understand the above grid or the `trial` object, check out my [article](https://towardsdatascience.com/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c) on Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Optuna study and run trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
