{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggler's Guide to LightGBM Hyperparameter Tuning with Optuna in 2021\n",
    "## Maximize LGBM's performance  TODO\n",
    "![](images/pixabay.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://www.pexels.com/@pixabay?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pixabay</a>\n",
    "        on \n",
    "        <a href='https://www.pexels.com/photo/silhouette-of-person-holding-sparkler-digital-wallpaepr-266429/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels.</a> All images are by the author unless specified otherwise.\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import catboost as cb\n",
    "import joblib\n",
    "import lightgbm as lgbm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    make_column_selector,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\", level=logging.INFO\n",
    ")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous article, we talked about the basics of LightGBM and creating LGBM models that beat XGBoost in almost every aspect. This article focuses on the last stage of any machine learning project - hyperparameter tuning (if we don't include model ensembling). \n",
    "\n",
    "First, we will take a look at the most important LGBM hyperparameters, grouped by their impact level and area. Then, we will see a hands-on example of tuning LGBM parameters using Optuna - the next-generation bayesian hyperparameter tuning framework. \n",
    "\n",
    "Most importantly, we will do this in a way that is similar to how top Kagglers tune their LGBM models that achieve impressive results (TODO THIS SENTENCE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I highly suggest reading the first part of the article if you are new to LGBM. Although I will briefly explain how Optuna works, I also recommend reading my separate post on it to get the best out this article.\n",
    "\n",
    "https://towardsdatascience.com/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c?source=your_stories_page-------------------------------------\n",
    "\n",
    "https://towardsdatascience.com/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c?source=your_stories_page-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the most important parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, hyperparameters of most tree-based model can be grouped into 4 categories:\n",
    "1. Parameters that affect the structure and learning of the decision trees\n",
    "2. Parameters that affect the training speed\n",
    "3. Parameters for better accuracy\n",
    "4. Parameters to combat overfitting\n",
    "\n",
    "Most of the time, parameters in these categories have a lot of overlap and increasing efficiency in one may come at the risk of decrease in another. That's why tuning them manually is a giant mistake and should be avoided at all costs. \n",
    "\n",
    "Frameworks like Optuna can find the \"sweet medium\" between these categories automatically if given a good enough parameter grid (and yes, we will develop this grid for LGBM today)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters that control the tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you are not familiar with decision trees, check out [this legendary video](https://www.youtube.com/watch?v=_L39rN6gz7Y) by StatQuest.\n",
    "\n",
    "In LGBM, the most important parameter to control the tree structure is `num_leaves`. As the name suggests, it controls the number of decision leaves in a single tree. Decision leaf of a tree is the node where the 'actual decision' happens.\n",
    "\n",
    "The next is `max_depth` that controls the tree depth. The higher `max_depth`, the more levels the tree has, which makes it more complex and prone to overfit. Too low and you will underfit. Even though it sounds complex, it is the easiest parameter to tune - just choose a value between 3 and 12 (this range tends to work well on Kaggle for any dataset).\n",
    "\n",
    "Tuning `num_leaves` can also be easy once you determine `max_depth`. There is a simple formula given in LGBM documentation - the maximum limit to `num_leaves` should be `2^(max_depth)`. This means the optimal value for `num_leaves` lies within the range ($2^3$, $2^{12}$) or (8, 4096). \n",
    "\n",
    "However, `num_leaves` impacts the learning in LGBM more than `max_depth`. This means you need to specify a more conservative search range like (20, 3000) - that's what I mostly do. \n",
    "\n",
    "Another important structural parameter for a tree is `min_data_in_leaf`. Its magnitude is also correlated to whether you overfit or not. In simple terms, `min_data_in_leaf` specifies the minimum number of observetaions that fit the decision criteria in a leaf.\n",
    "\n",
    "For example, if the decision leaf is checking wheter one feature is greater than, let's say, 13 - setting `min_data_in_leaf` to 100 means we want to evaluate this node only if there are at least 100 training observations that are bigger than 13. This is the gist in my lay terms.\n",
    "\n",
    "The optimal value for `min_data_in_leaf` depends on the number of training samples and `num_leaves`. For large datasets, set a value in hundreds or thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna, creating the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Optuna study and run trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
