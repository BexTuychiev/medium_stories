{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Integrate Any Data Transformation Step into Sklearn Pipelines With Custom Transformers\n",
    "## Let's do everything in Sklearn\n",
    "![](images/unsplash.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Arseny Togulev</a>\n",
    "        on \n",
    "        <a href='https://unsplash.com/s/photos/transformer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash.</a> All images are by the author unless specified otherwise.\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import catboost as cb\n",
    "import joblib\n",
    "import lightgbm as lgbm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    make_column_selector,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\", level=logging.INFO\n",
    ")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single `fit`, single `predict` - how awesome would that be?\n",
    "\n",
    "You get the data, fit your pipeline just one time and it takes care of everything - preprocessing, feature engineering, modeling, everything. All you have to do is call predict and have the output. \n",
    "\n",
    "What kind of pipeline is *that* powerful? Yes, Sklearn has many transformers but it doesn't have one for every imaginable preprocessing scenario. So, is such a pipeline a *pipe* dream?\n",
    "\n",
    "Absolutely not. Today, we will learn how to create custom Sklearn transformers that enable you to integrate virtually any function or data transformation into Sklearn's Pipeline class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Sklearn pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple pipeline that imputes the missing values in numeric data, scales them and fits an XGBRegressor to `X`, `y`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_pipe = make_pipeline(\n",
    "                SimpleImputer(strategy='mean'),\n",
    "                StandardScaler(),\n",
    "                xgb.XGBRegressor()\n",
    "            )\n",
    "\n",
    "_ = xgb_pipe.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have talked at length about the nitty-gritty of Sklearn pipelines and their benefits in an [older post](https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d). Most notable advantages are their ability to collapse all preprocessing and modeling steps into a singe estimator, preventing data leakage by never calling `fit` on validation sets and an added bonus that makes the code concise, reproducible and modular. \n",
    "\n",
    "But this whole idea of atomic, neat pipelines breaks when we need to perform operations that are not built into Sklearn as estimators. For example, what if you need to extract regex patterns to clean text data? What do you do if you want to create a new feature combining existing ones based on domain knowledge?\n",
    "\n",
    "To preserve all the benefits that come with pipelines, you need a way to integrate your custom preprocessing and feature engineering logic into Sklearn. That's where custom transformers come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating simple functions with `FunctionTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this month's (September) TPS Competition on Kaggle, one of the ideas that boosted model performance significantly was adding the number of missing values in a row as a new feature. This is a custom operation, not implemented in Sklearn, so let's create a function to achieve that after importing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f110</th>\n",
       "      <th>f111</th>\n",
       "      <th>f112</th>\n",
       "      <th>f113</th>\n",
       "      <th>f114</th>\n",
       "      <th>f115</th>\n",
       "      <th>f116</th>\n",
       "      <th>f117</th>\n",
       "      <th>f118</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.10859</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>-37.566</td>\n",
       "      <td>0.017364</td>\n",
       "      <td>0.28915</td>\n",
       "      <td>-10.25100</td>\n",
       "      <td>135.12</td>\n",
       "      <td>168900.0</td>\n",
       "      <td>3.992400e+14</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.2280</td>\n",
       "      <td>1.7482</td>\n",
       "      <td>1.90960</td>\n",
       "      <td>-7.11570</td>\n",
       "      <td>4378.80</td>\n",
       "      <td>1.2096</td>\n",
       "      <td>8.613400e+14</td>\n",
       "      <td>140.1</td>\n",
       "      <td>1.01770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>0.299610</td>\n",
       "      <td>11822.000</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.45970</td>\n",
       "      <td>-0.83733</td>\n",
       "      <td>1721.90</td>\n",
       "      <td>119810.0</td>\n",
       "      <td>3.874100e+15</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.7580</td>\n",
       "      <td>4.1684</td>\n",
       "      <td>0.34808</td>\n",
       "      <td>4.14200</td>\n",
       "      <td>913.23</td>\n",
       "      <td>1.2464</td>\n",
       "      <td>7.575100e+15</td>\n",
       "      <td>1861.0</td>\n",
       "      <td>0.28359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.17803</td>\n",
       "      <td>-0.006980</td>\n",
       "      <td>907.270</td>\n",
       "      <td>0.272140</td>\n",
       "      <td>0.45948</td>\n",
       "      <td>0.17327</td>\n",
       "      <td>2298.00</td>\n",
       "      <td>360650.0</td>\n",
       "      <td>1.224500e+13</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.7688</td>\n",
       "      <td>1.2042</td>\n",
       "      <td>0.26290</td>\n",
       "      <td>8.13120</td>\n",
       "      <td>45119.00</td>\n",
       "      <td>1.1764</td>\n",
       "      <td>3.218100e+14</td>\n",
       "      <td>3838.2</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.15236</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>780.100</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.51947</td>\n",
       "      <td>7.49140</td>\n",
       "      <td>112.51</td>\n",
       "      <td>259490.0</td>\n",
       "      <td>7.781400e+13</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.8580</td>\n",
       "      <td>2.0694</td>\n",
       "      <td>0.79631</td>\n",
       "      <td>-16.33600</td>\n",
       "      <td>4952.40</td>\n",
       "      <td>1.1784</td>\n",
       "      <td>4.533000e+12</td>\n",
       "      <td>4889.1</td>\n",
       "      <td>0.51486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>-109.150</td>\n",
       "      <td>0.297910</td>\n",
       "      <td>0.34490</td>\n",
       "      <td>-0.40932</td>\n",
       "      <td>2538.90</td>\n",
       "      <td>65332.0</td>\n",
       "      <td>1.907200e+15</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.6410</td>\n",
       "      <td>1.5298</td>\n",
       "      <td>1.14640</td>\n",
       "      <td>-0.43124</td>\n",
       "      <td>3856.50</td>\n",
       "      <td>1.4830</td>\n",
       "      <td>-8.991300e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       f1        f2         f3        f4       f5        f6       f7  \\\n",
       "0   0  0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n",
       "1   1  0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n",
       "2   2  0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n",
       "3   3  0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n",
       "4   4  0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n",
       "\n",
       "         f8            f9  ...     f110    f111     f112      f113      f114  \\\n",
       "0  168900.0  3.992400e+14  ... -12.2280  1.7482  1.90960  -7.11570   4378.80   \n",
       "1  119810.0  3.874100e+15  ... -56.7580  4.1684  0.34808   4.14200    913.23   \n",
       "2  360650.0  1.224500e+13  ...  -5.7688  1.2042  0.26290   8.13120  45119.00   \n",
       "3  259490.0  7.781400e+13  ... -34.8580  2.0694  0.79631 -16.33600   4952.40   \n",
       "4   65332.0  1.907200e+15  ... -13.6410  1.5298  1.14640  -0.43124   3856.50   \n",
       "\n",
       "     f115          f116    f117     f118  claim  \n",
       "0  1.2096  8.613400e+14   140.1  1.01770      1  \n",
       "1  1.2464  7.575100e+15  1861.0  0.28359      0  \n",
       "2  1.1764  3.218100e+14  3838.2  0.40690      1  \n",
       "3  1.1784  4.533000e+12  4889.1  0.51486      1  \n",
       "4  1.4830 -8.991300e+12     NaN  0.23049      1  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tps_df = pd.read_csv(\"data/train.csv\")\n",
    "tps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957919, 120)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tps_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         0\n",
       "2         5\n",
       "3         2\n",
       "4         8\n",
       "         ..\n",
       "957914    0\n",
       "957915    4\n",
       "957916    0\n",
       "957917    1\n",
       "957918    4\n",
       "Length: 957919, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of missing values across rows\n",
    "tps_df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that takes a DataFrame as an input and implements the above operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_missing_row(X: pd.DataFrame, y=None):\n",
    "    # Calculate some metrics across rows\n",
    "    num_missing = X.isnull().sum(axis=1)\n",
    "    num_missing_std = X.isnull().std(axis=1)\n",
    "\n",
    "    # Add the above series as a new feature to the df\n",
    "    X[\"#missing\"] = num_missing\n",
    "    X[\"num_missing_std\"] = num_missing_std\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, adding this function into a pipeline is just as easy as passing it to the `FunctionTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "num_missing_estimator = FunctionTransformer(num_missing_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a custom function to `FunctionTransformer` creates an estimator with `fit`, `transform` and `fit_transform` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before preprocessing: 120\n",
      "Number of features after preprocessing: 122\n"
     ]
    }
   ],
   "source": [
    "# Check number of columns before\n",
    "print(f\"Number of features before preprocessing: {len(tps_df.columns)}\")\n",
    "\n",
    "# Apply the custom estimator\n",
    "tps_df = num_missing_estimator.transform(tps_df)\n",
    "print(f\"Number of features after preprocessing: {len(tps_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a simple function, no need to call `fit` as it just returns the estimator untouched. The only requirement of `FunctionTransformer` is that the passed function should accept the data as its first argument.\n",
    "Optionally, you can pass the target array as well if you need it inside the function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# FunctionTransformer signature\n",
    "def custom_function(X, y=None):\n",
    "    ...\n",
    "\n",
    "estimator = FunctionTransformer(custom_function)  # no errors\n",
    "\n",
    "custom_pipeline = make_pipeline(StandardScaler(), estimator, xgb.XGBRegressor())\n",
    "custom_pipeline.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FunctionTransformer` also accepts an inverse of the passed function if you ever need to revert the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_function(X, y=None):\n",
    "    ...\n",
    "\n",
    "\n",
    "def inverse_of_custom(X, y=None):\n",
    "    ...\n",
    "\n",
    "\n",
    "estimator = FunctionTransformer(func=custom_function, inverse_func=inverse_of_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) for details on other arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Integrating more complex preprocessing steps with custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This section assumes some knowledge of Python object-oriented programming (OOP). Specifically, the basics of creating classes and inheritance. If you are not already down with those, check out my [Python OOP series](https://ibexorigin.medium.com/list/objectoriented-programming-essentials-for-data-scientists-cf2ff3dc9fc9?source=user_lists---------1-------cf2ff3dc9fc9---------------------), written for data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common scaling options for skewed data is a logarithmic transform. But here is a caveat: if a feature has even a single 0, the transformation with `np.log` or Sklearn's `PowerTransformer` return an error. \n",
    "\n",
    "So, as a workaround, Kagglers add 1 to all samples and then apply the transformation. If the transformation is performed on the target array, you will also need an inverse transform. For that, after making predictions, you need to use the exponential function and subtract 1. Here is what it looks like in code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "y_transformed = np.log(y + 1)\n",
    "\n",
    "_ = model.fit(X, y_transformed)\n",
    "preds = np.exp(model.predict(X, y_transformed) - 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but we have the same old problem - we can't include this into a pipeline out of the box. Sure, we could use our new-found friend `FunctionTransformer` but, it is not well-suited for more complex preprocessing steps such as this. \n",
    "\n",
    "Instead, we will implement a custom transformer class and write the `fit`, `transform` functions manually. In the end, we will again have an Sklearn-compatible estimator that we can pass into a pipeline. Let's start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class CustomLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a class that inherits from `BaseEstimator` and `TransformerMixin` classes of `sklearn.base`. Inheriting from these classes allows Sklearn pipelines to recognize our classes as custom estimators. \n",
    "\n",
    "Then, we will implement the `__init__` method, where we just initialize an instance of `PowerTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "class CustomLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._estimator = PowerTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the `fit` where we add 1 to all features in the data and fit the PowerTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._estimator = PowerTransformer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = np.copy(X) + 1\n",
    "        self._estimator.fit(X_copy)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method should return the transformer itself, which is done by returning `self`. Let's test what we have done so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLogTransformer()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_log = CustomLogTransformer()\n",
    "custom_log.fit(tps_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have the `transform`, in which we just use the `transform` method of PowerTransformer after adding 1 to the passed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._estimator = PowerTransformer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = np.copy(X) + 1\n",
    "        self._estimator.fit(X_copy)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = np.copy(X) + 1\n",
    "\n",
    "        return self._estimator.transform(X_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_log = CustomLogTransformer()\n",
    "custom_log.fit(tps_df)\n",
    "\n",
    "transformed_tps = custom_log.transform(tps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48908946, -2.17126787, -1.79124946, -0.52828469,         nan],\n",
       "       [ 0.38660665, -0.29384644,  1.31313666,  0.1901713 , -0.34236997],\n",
       "       [-0.04286469, -0.05047097, -1.16463754,  0.95459266,  1.71830766],\n",
       "       [-0.584329  , -1.5743182 , -1.02444525, -0.15117546,  0.46952437],\n",
       "       [-0.87027925, -0.13045462, -0.10489176, -0.36806683,  1.21317668]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_tps[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working as expected. Now, as I said earlier, we need a method for reverting the transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._estimator = PowerTransformer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = np.copy(X) + 1\n",
    "        self._estimator.fit(X_copy)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = np.copy(X) + 1\n",
    "\n",
    "        return self._estimator.transform(X_copy)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X_reversed = self._estimator.inverse_transform(np.copy(X))\n",
    "\n",
    "        return X_reversed - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also could have used `np.exp` instead of `inverse_transform`. Now, let's make a final check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_log = CustomLogTransformer()\n",
    "\n",
    "tps_transformed = custom_log.fit_transform(tps_df)\n",
    "tps_inversed = custom_log.inverse_transform(tps_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait! We didn't write `fit_transform` - where did that come from?\n",
    "\n",
    "It is simple - when you inherit from `BaseEstimator` and `TransformerMixin`, you get a `fit_transform` method for free. After the inverse transform, you can compare it with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35275, 0.17725, 0.25997, 0.4293 , 0.34079])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tps_df.values[:5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35275, 0.17725, 0.25997, 0.4293 , 0.34079])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tps_inversed[:5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a custom transformer ready to be included in a pipeline. Let's put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "xgb_pipe = make_pipeline(\n",
    "    FunctionTransformer(num_missing_row),\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=-99999),\n",
    "    CustomLogTransformer(),\n",
    "    xgb.XGBClassifier(\n",
    "        n_estimators=1000, tree_method=\"gpu_hist\", objective=\"binary:logistic\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "X, y = tps_df.drop(\"claim\", axis=1), tps_df[[\"claim\"]].values.flatten()\n",
    "split = train_test_split(X, y, test_size=0.33, random_state=1121218)\n",
    "X_train, X_test, y_train, y_test = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:42:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7986831816726399"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pipe.fit(X_train, y_train)\n",
    "preds = xgb_pipe.predict_proba(X_test)\n",
    "\n",
    "roc_auc_score(y_test, preds[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though log transform actually hurt the score, we got our custom pipeline working!\n",
    "\n",
    "In short, the signature of your custom transformer class should be like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self):\n",
    "        pass\n",
    "\n",
    "    def inverse_transform(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, you get `fit_transform` for free. If you don't need any of `__init__`, `fit`, `transform` or `inverse_transform` methods, omit them and the parent Sklearn classes take care of everything. The logic of these methods are entirely up to your coding skills and needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wrapping up..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing good code is a skill developed over time. You will realize that a big part of it comes from using the existing tools and libraries at the right time and place, without having to reinvent the wheel.\n",
    "\n",
    "One of such tools is Sklearn pipelines and custom transformers are just extensions of them. Use them well and you will produce quality code with little effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
