{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0cdb15-68f0-4266-a232-be91d96e88e9",
   "metadata": {},
   "source": [
    "# Building machine learning pipelines and tracking experiments with DVC in VSCode\n",
    "## Manage ML experiments like a pro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbfbda9-58b2-4fd0-ac87-e53313edf817",
   "metadata": {},
   "source": [
    "### Why track experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e3feae-794f-454a-ae38-aa22cb2856ff",
   "metadata": {},
   "source": [
    "### What you will learn in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6df30-24a4-48ee-9418-66df2f984a9b",
   "metadata": {},
   "source": [
    "### Setting up the project and downloading a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0e17b-5257-4a78-9366-d4391ca819dd",
   "metadata": {},
   "source": [
    "You start by forking [the following GitHub repository](https://github.com/BexTuychiev/dvc-tutorial) and cloning it:\n",
    "```\n",
    "$ git clone https://github.com/YourUsername/dvc-tutorial.git\n",
    "$ cd dvc-tutorial\n",
    "```\n",
    "\n",
    "Don't forget to replace `YourUsername`. The cloned repository has three Python scripts inside `src` (more on them later), a pre-filled `.gitignore` file for Python projects and a `requirements.txt` file. \n",
    "\n",
    "```\n",
    "$ conda create -n dvc-tutorial python==3.9 -y\n",
    "$ conda activate dvc-tutorial\n",
    "$ python -r requirements.txt\n",
    "```\n",
    "\n",
    "Next, create a `data` directory to store the `raw` images. You will be using The German Traffic Signs Recognition Benchmark (GTSRB) dataset. You can either download it from its [homepage on Kaggle](https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign) or use this [direct download link](https://storage.googleapis.com/kaggle-data-sets/82373/191501/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20221217%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20221217T125828Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=99f991f43b97aaf3c4e32ddb137d2373c8f2389af52063bc47ad2be4d4df1177f2b1438715b0f91c593cc8e905fe712b4b562314f46df619f99446dbb2673c11f5afd57ca4130d8de9e5df83ac6358698c5677f965e00914ea1b90d81fa130fecca88bbfe612bb71e67c14f8c34f0e94b2c1ffc4133918078aad039cb04dd786aac96d0a8522dffbeab9e8991ea775e1d3c189c29362c456b4ab1427b5d169d1b60b9d76f79586c5544ce7fe7adfd705401ddcd516a57af9c5bd58fe1690fc03187c82cba47ea393d160d1802f66292d9f55a20d1587c7079083cdb91f289ab8ca886a90e99b9bfa8b6c159b64b8fe4c7229a28dcd7a4ddb84e3c5e175aa63c0) below:\n",
    "\n",
    "> I recommend downloading the zipped dataset from the official webpage as the donwload link might change.\n",
    "\n",
    "```bash\n",
    "$ mkdir data data/raw\n",
    "$ curl \"the_link_inside_quotes\" -o data/traffic_signs.zip\n",
    "$ unzip data/traffic_signs.zip -d data/raw\n",
    "$ cd data/raw\n",
    "```\n",
    "\n",
    "The zipped dataset comes with a few unnecessary files and directories, which you will delete, along with the original zipfile:\n",
    "```bash\n",
    "$ rm -rf Train Test test Meta meta Meta.csv Test.csv Train.csv\n",
    "$ cd ../..\n",
    "$ rm data/traffic_signs.zip\n",
    "```\n",
    "\n",
    "Your directory structure should now look like this:\n",
    "\n",
    "```\n",
    "$ tree -L 3\n",
    "├── data\n",
    "│   └── raw\n",
    "│       └── train\n",
    "├── requirements.txt\n",
    "└── src\n",
    "    ├── preprocess.py\n",
    "    └── train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a2f56-ede9-40fc-81ad-5ad7e041eee1",
   "metadata": {},
   "source": [
    "Next, you initialize DVC and start tracking the `dvc/raw/train` directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42821d-944e-463f-9b0f-e5b0a4f69e31",
   "metadata": {},
   "source": [
    "```\n",
    "$ dvc init\n",
    "$ dvc add data/raw/train\n",
    "$ git add .\n",
    "$ git commit -m \"Download and add a dataset\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c20c245-0536-4787-8743-b81f7a84fa51",
   "metadata": {},
   "source": [
    "Don't forget to make a commit to specify DVC initialization.\n",
    "\n",
    "> I explained the fundamentals of DVC and its `init`, `add`, `remote`, `push` commands in the [first part of the article](https://medium.com/towards-data-science/how-to-version-gigabyte-sized-datasets-just-like-code-with-dvc-in-python-5197662e85bd).\n",
    "\n",
    "The only thing missing is setting up a remote storage, which you can set to any directory on your system or even a cloud storage like an S3 bucket. I recommend a local remote storage (like `~/dvc_remote`) for this tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b1c0b-0da5-4263-b0dc-56d01470015b",
   "metadata": {},
   "source": [
    "```\n",
    "# Create a directory for the remote under `home`\n",
    "$ mkdir ~/dvc_remote\n",
    "$ dvc remote add -d dvc_remote ~/dvc_remote\n",
    "```\n",
    "\n",
    "Now, you should push the Git commits to GitHub and the cached train images to the DVC remote:\n",
    "```bash\n",
    "$ git push\n",
    "$ dvc push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ffef2-c716-42dd-bb7e-a0fe771408c5",
   "metadata": {},
   "source": [
    "### What is a machine learning pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ae407-6d65-42a6-9b72-a5bc9ff97196",
   "metadata": {},
   "source": [
    "Explain what a ml pipeline is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ce065-1d7a-4e45-a35f-d00b1d2af547",
   "metadata": {},
   "source": [
    "### How to create a pipeline in DVC?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5769b5-c641-42a1-9450-c5feeb34d949",
   "metadata": {},
   "source": [
    "The first stage of our pipeline is to set aside 10% of training images for testing. Currently, the `data/raw/train` directory contains 43 classes of traffic signs, with varying number of images inside.\n",
    "\n",
    "```\n",
    "├── data\n",
    "│   └── raw\n",
    "│       ├── train\n",
    "│       │   ├── 0\n",
    "│       │   ├── 1\n",
    "│       │   ├── ...\n",
    "│       └── train.dvc\n",
    "├── notebooks\n",
    "│   └── test.ipynb\n",
    "├── requirements.txt\n",
    "└── src\n",
    "    ├── preprocess.py\n",
    "    ├── split.py\n",
    "    └── train.py\n",
    "```\n",
    "\n",
    "The `src/split.py` script, specifically lines 12-26, take 10% of images (after shuffling) in each class directory and moves them to a new `data/raw/test/class_number` mirror directory. The script uses a combination of `shutil` and `pathlib` libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979ed58-faeb-4691-90cf-f86ecd3eec39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "```python\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "base_dir = Path(__file__).parent.parent\n",
    "raw_train_dir = base_dir / \"data\" / \"raw\" / \"train\"\n",
    "raw_test_dir = base_dir / \"data\" / \"raw\" / \"test\"\n",
    "\n",
    "# Copy 10% of train images to validation directory\n",
    "for directory in raw_train_dir.iterdir():\n",
    "    test_mirror_path = str(directory).replace(\"train\", \"test\")\n",
    "    test_mirror_path = Path(test_mirror_path)\n",
    "    test_mirror_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Collect image paths in each class of train directory\n",
    "    image_paths = list(directory.glob(\"*.png\"))\n",
    "    np.random.shuffle(image_paths)\n",
    "\n",
    "    # Choose 10% of images\n",
    "    test_images = image_paths[-int(len(image_paths) * 0.1):]\n",
    "\n",
    "    # Copy images to validation directory\n",
    "    for image_path in test_images:\n",
    "        shutil.move(image_path, test_mirror_path)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdcaa2-4f4b-420a-a1a6-59a9debcd2c5",
   "metadata": {},
   "source": [
    "Instead of running the script with `python src/split.py`, we will run it as a pipeline stage with the following multi-line CLI command:\n",
    "\n",
    "```bash\n",
    "$ dvc stage add -n split \\\n",
    "                -d src/split.py -d data/raw/train \\\n",
    "                -o data/raw/test \\\n",
    "                python src/split.py\n",
    "```\n",
    "\n",
    "Let's understand the command line-by-line. The `stage add` command adds a step to a DVC pipeline, whose name you specify after the `-n` tag. We are calling this stage `split`. \n",
    "\n",
    "The next line of the command specifies two dependencies with `-d` tags. The `split` stage needs both `data/raw/train` directory and `src/split.py` to run without errors, so they are dependencies. `split.py` also moves the images to a new `data/raw/validation` directory, so it is given as an output with `-o` tag. \n",
    "\n",
    "The final line of the command is the actual CMD command to run the pipeline step, which is `python src/split.py`.\n",
    "\n",
    "You could have added the stage with the following more explicit syntax:\n",
    "\n",
    "```bash\n",
    "$ dvc stage add --name split \\\n",
    "                --deps src/split.py \\\n",
    "                --deps data/raw/train \\\n",
    "                --outs data/raw/test \\\n",
    "                --desc \"Set aside 10% of training images for testing.\" \\\n",
    "                python src/split.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35851795-af32-4756-ba16-8e95059e0f07",
   "metadata": {},
   "source": [
    "> It is important to list each dependency and output with new `-d` and `-o` tags. \n",
    "\n",
    "When you run the above `stage add` command, you will get the following output:\n",
    "\n",
    "```\n",
    "$ dvc stage add ...\n",
    "Creating 'dvc.yaml'                                                   \n",
    "Adding stage 'split' in 'dvc.yaml'\n",
    "```\n",
    "\n",
    "It is telling that a new `dvc.yaml` file is created. DVC configures your entire pipeline inside it. When you open it, you will see `split` stage added to it:\n",
    "\n",
    "```YAML\n",
    "$ cat dvc.yaml\n",
    "stages:\n",
    "  split:\n",
    "    cmd: python src/split.py\n",
    "    deps:\n",
    "    - data/raw/train\n",
    "    - src/split.py\n",
    "    outs:\n",
    "    - data/raw/test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ef360-82ee-4797-ae20-a05be0683052",
   "metadata": {},
   "source": [
    "Now, you can already run this pipeline (even though it contains only one stage right now) with `dvc repro`, which stands for 'DVC reproduce':\n",
    "\n",
    "```bash\n",
    "$ dvc repro\n",
    "Running stage 'split':                                                                                                                      \n",
    "> python src/split.py\n",
    "```\n",
    "\n",
    "After execution, if you look at `data/raw`, you will see a new `test` folder with 10% of the images of each class:\n",
    "\n",
    "```\n",
    "├── data\n",
    "│   └── raw\n",
    "│       ├── test\n",
    "│       │   ├── 0\n",
    "│       │   ├── 1\n",
    "│       │   └── ...\n",
    "│       ├── train\n",
    "│       │   ├── 0\n",
    "│       │   ├── 1\n",
    "│       │   └── ...\n",
    "│       └── train.dvc\n",
    "├── dvc.lock\n",
    "├── dvc.yaml\n",
    "├── requirements.txt\n",
    "└── src\n",
    "    ├── preprocess.py\n",
    "    ├── split.py\n",
    "    └── train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a7700-6157-4363-ac15-9cb69b8ad405",
   "metadata": {},
   "source": [
    "> We will get to the `dvc.lock` file a bit later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fd86a-0226-419a-8451-943f228a5e73",
   "metadata": {},
   "source": [
    "`data/raw/test` directory is already under DVC control and in the cache, because specifying outputs with the `-o` tag in a stage automatically adds them to DVC. For this reason, `git status` will only show changes to `.gitignore` and a couple of untracked files:\n",
    "\n",
    "```bash\n",
    "$ git status -s\n",
    " M data/raw/.gitignore\n",
    "?? dvc.lock\n",
    "?? dvc.yaml\n",
    "```\n",
    "\n",
    "Before you commit these changes to Git, be sure to run `dvc add data/raw/train` again because it changed after we ran the pipeline:\n",
    "\n",
    "```\n",
    "$ dvc status\n",
    "data/raw/train.dvc:    \n",
    "        changed outs:\n",
    "                modified:           data/raw/train\n",
    "$ dvc add data/raw/train\n",
    "```\n",
    "\n",
    "Now, turn to git:\n",
    "```\n",
    "$ git add --all\n",
    "$ git commit -m \"Add and run 'split' stage of a pipeline\"\n",
    "$ git push\n",
    "$ dvc push\n",
    "```\n",
    "\n",
    "Let's add another stage called `preprocess`:\n",
    "\n",
    "```bash\n",
    "$ dvc stage add -n preprocess \\\n",
    "                -p preprocess.denoise_weight \\\n",
    "                -d data/raw/ -d src/preprocess.py \\\n",
    "                -o data/prepared \\\n",
    "                python src/preprocess.py\n",
    "```\n",
    "\n",
    "This `stage` command has a new tag called `-p`, which specifies a stage parameter. Python scripts usually have parameters that change from run to run, and this is DVC's way of dynamically inserting them into the pipeline stages. The `-p` tag assumes you have a `params.yaml` file in the root directory when adding the stages. So, before running the above command, create the `params.yaml` and paste the following contents:\n",
    "\n",
    "```\n",
    "$ touch params.yaml\n",
    "# Paste the contents\n",
    "$ cat params.yaml\n",
    "preprocess:\n",
    "  denoise_weight: 0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57403633-f6ed-4e1a-826f-1d9313afcaf0",
   "metadata": {},
   "source": [
    "The file lists a single stage called `preprocess` and a single `denoise_weight` parameter with a value of 0.2. For our `src/preprocess.py` script to read this parameter, in line 7, we import the `params_show` function from `dvc.api`:\n",
    "\n",
    "```python\n",
    "...\n",
    "from dvc.api import params_show\n",
    "```\n",
    "\n",
    "Then, in line 48, under `__name__ == __main__`, we read the parameters for the `preprocess` stage with `params = params_show()['preprocess']`, which returns a dictionary of parameters. \n",
    "\n",
    "```python\n",
    "if __name__ == __main__:\n",
    "    ...\n",
    "    \n",
    "    params = params_show()['preprocess']\n",
    "    denoise_weight = params[\"denoise_weight\"]\n",
    "```\n",
    "\n",
    "> `params_show` function looks for `params.yaml` file by default.\n",
    "\n",
    "Then, `denoise_weight` is passed to `denoise_image` function, which runs [Total Variation filtering technique from `scikit-image`](https://scikit-image.org/docs/stable/api/skimage.restoration.html#skimage.restoration.denoise_tv_chambolle) to denoise all images in train and test folders. \n",
    "\n",
    "The `dvc.yaml` file now looks like below:\n",
    "```YAMl\n",
    "stages:\n",
    "  split:\n",
    "    ...\n",
    "  preprocess:\n",
    "    cmd: python src/preprocess.py\n",
    "    deps:\n",
    "    - data/raw/\n",
    "    - src/preprocess.py\n",
    "    params:\n",
    "    - preprocess.denoise_weight\n",
    "    outs:\n",
    "    - data/prepared\n",
    "```\n",
    "\n",
    "Let's run the entire pipeline with `dvc repro`:\n",
    "\n",
    "```\n",
    "$ dvc repro\n",
    "'data/raw/train.dvc' didn't change, skipping\n",
    "Stage 'split' didn't change, skipping\n",
    "Running stage 'preprocess' with command: ...\n",
    "```\n",
    "\n",
    "This time we see the beauty of the `repro` - it automatically detects the changes in each pipeline stage and only runs them if their dependencies or outputs are changed. How? Courtesy of the `dvc.lock` file. `dvc.lock` keeps track of the hashes of each dependency and output of a stage. Combined with `dvc.yaml`, they can detect changes in any pipeline stage and invalidate any subsequent stages to run them again, as all pipelines are connected via dependencies and outputs!\n",
    "\n",
    "Let's commit the changes to `params.yaml`, `dvc.yaml`, `dvc.lock` and the rest of the files to Git (`raw` and `prepared` are already inside DVC cache as they are outputs of pipeline stages).\n",
    "\n",
    "```\n",
    "$ git add .\n",
    "$ git commit -m \"Add and run 'preprocess' stage of a pipeline\"\n",
    "$ git push\n",
    "$ dvc push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6245462-ffcf-48c1-a52c-e0d927b30550",
   "metadata": {},
   "source": [
    "We will add one final `train` stage to the pipeline before going into evaluation:\n",
    "\n",
    "```bash\n",
    "$ dvc stage add -n train \\\n",
    "                -p train \\\n",
    "                -d data/prepared -d src/train.py \\\n",
    "                -o models -O metrics/metrics.csv \\\n",
    "                python src/train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50298d4-b2b5-4efd-9e51-1a0dd2f782ab",
   "metadata": {},
   "source": [
    "Notice how we are only passing a single keyword to `-p` in the `train` stage. The reason is that in an updated `params.yaml` file, we have multiple parameters and it would be cumbersome to list them all with commas:\n",
    "\n",
    "```\n",
    "$ cat params.yaml\n",
    "preprocess:\n",
    "  denoise_weight: 0.2\n",
    "\n",
    "train:\n",
    "  image_width: 30\n",
    "  image_height: 30\n",
    "  batch_size: 32\n",
    "  learning_rate: 0.1\n",
    "  n_epochs: 5\n",
    "```\n",
    "\n",
    "That's why we are simply specifying a parameter group via the stage name only. \n",
    "\n",
    "The `train.py` script itself fits a CNN model with five layers, with max pooling, batch normalization and drop-out layers in-between. \n",
    "\n",
    "```python\n",
    "def get_model():\n",
    "    \"\"\"Define the model to be fit\"\"\"\n",
    "    # Define a CNN model\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=16,\n",
    "                kernel_size=3,\n",
    "                activation=\"relu\",\n",
    "                input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3),\n",
    "            ),\n",
    "            ...\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(43, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "```\n",
    "\n",
    "Because of the tree structure of our project, the script uses `ImageDataGenerator`'s `flow_from_directory` method to feed the images with augmentation asynchronously to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eab27d-199a-4ecd-a7ce-c956985eac47",
   "metadata": {},
   "source": [
    "```python\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.15,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir / \"prepared\" / \"train\",\n",
    "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b3f10-9cd9-42a1-9a49-132110f23454",
   "metadata": {},
   "source": [
    "In line 11 and 12, we are loading the training stage parameters:\n",
    "\n",
    "```python\n",
    "params = params_show()[\"train\"]\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = params[\"image_width\"], params[\"image_height\"]\n",
    "```\n",
    "These are subsequently used to specify target image size of `ImageDataGenerator`, batch size, learning rate and the number of epochs.\n",
    "\n",
    "Under `main()`, we are using `ModelCheckpoint` callback to save the best model and the `history` object as a CSV to log the metrics.\n",
    "\n",
    "```python\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=params[\"n_epochs\"],\n",
    "    validation_data=test_generator,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# Save the metrics\n",
    "Path(\"metrics\").mkdir(exist_ok=True)\n",
    "pd.DataFrame(history.history).to_csv(\"metrics/metrics.csv\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c5ad4-1444-412f-bcbe-9a77f381e811",
   "metadata": {},
   "source": [
    "Notice how we are using the uppercase `-O` tag to specify the `metrics.csv` file. Since it is lightweight, we want Git to track it, so, adding `-O` makes DVC ignore the file. This behavior is also reflected in the `dvc.yaml` with the `cache: false` field:\n",
    "\n",
    "```\n",
    "$ cat dvc.yaml\n",
    "\n",
    "stages:\n",
    "  split:\n",
    "    ...\n",
    "  preprocess:\n",
    "    ...\n",
    "  train:\n",
    "    cmd: python src/train.py\n",
    "    deps:\n",
    "    - data/prepared\n",
    "    - src/train.py\n",
    "    params:\n",
    "    - train\n",
    "    outs:\n",
    "    - metrics/metrics.csv:\n",
    "        cache: false\n",
    "    - models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d54b99-963f-43d9-b1dc-0706d9c4a39d",
   "metadata": {},
   "source": [
    "Don't forget to make a snapshot of new changes after running the pipeline with `dvc repro`:\n",
    "\n",
    "```\n",
    "$ git add .\n",
    "$ git commit -m \"Add and run `train` stage of a pipeline\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eda21f-9daf-4401-b2cd-08b4fb2606fd",
   "metadata": {},
   "source": [
    "### How to track metrics and plots in DVC?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc6aca-4359-41b1-a180-54e5f7e3e380",
   "metadata": {},
   "source": [
    "We will add a final `evaluate` stage to our pipeline. A typical evaluation loop in an ML project is to test the model of the latest experiment on validation data in terms of one or metrics. This is also where you would plot model complexity curves, confusion matrices or any other plot that helps you debug your model and improve its performance.\n",
    "\n",
    "Our `src/evaluate.py` script looks like below:\n",
    "\n",
    "```python\n",
    "import ...\n",
    "\n",
    "# Extract the parameters\n",
    "params = params_show()[\"train\"]\n",
    "\n",
    "\n",
    "def plot_metric(metrics_df: pd.DataFrame, metric_name: str, plot_path: str):\n",
    "    \"\"\"\n",
    "    A function to plot both training and validation metrics from a 'metric_df'\n",
    "    \"\"\"\n",
    "    # Plot metric_name and val_metric_name\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(metrics_df) + 1)\n",
    "\n",
    "    ax.plot(epochs, metrics_df[metric_name], \"b\", label=f\"Training {metric_name}\")\n",
    "    ax.plot(\n",
    "        epochs, metrics_df[\"val_\" + metric_name], \"bo\", label=f\"Validation {metric_name}\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.title(f\"Training and validation {metric_name}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the path for plots\n",
    "    Path(\"plots\").mkdir(exist_ok=True)\n",
    "\n",
    "    # Read the metrics and plot\n",
    "    metrics = pd.read_csv(\"metrics/metrics.csv\")\n",
    "    metric_names = [\"accuracy\", \"loss\", \"precision\", \"recall\"]\n",
    "\n",
    "    for current_metric in metric_names:\n",
    "        plot_metric(metrics, current_metric, f\"plots/{current_metric}.png\")\n",
    "\n",
    "    #########################################################################################\n",
    "    # Usually, the below step would involve reporting the metric on a third, final test set #\n",
    "    #########################################################################################\n",
    "\n",
    "    # Save the best metrics as json\n",
    "    sorted_metrics = metrics.sort_values(\"val_accuracy\", ascending=False)\n",
    "    metrics_dict = {\n",
    "        \"val_\" + metric: sorted_metrics[\"val_\" + metric][0] for metric in metric_names\n",
    "    }\n",
    "\n",
    "    with open(\"metrics/metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics_dict, f)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81419fd2-6e3a-4a3a-9bb8-491acf693481",
   "metadata": {},
   "source": [
    "The most important parts of the script are line 45, which plots four different metrics - val_accuracy, val_loss, val_precision, val_recall under a new `plots` directory; In lines 57-58, we save the best metrics of the training loop as key-value pairs to a JSON file. Here is the file afterwards:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"val_accuracy\": 0.827551007270813,\n",
    "    \"val_loss\": 0.5542715191841125,\n",
    "    \"val_precision\": 0.9384615421295166,\n",
    "    \"val_recall\": 0.7469387650489807\n",
    "}\n",
    "```\n",
    "\n",
    "In your own projects, always report metrics in this JSON format, as DVC recognizes and uses them to report metric differences between different runs of a pipeline. We will see how to do it in the next section. For now, let's add the `evaluate` stage to our pipeline:\n",
    "\n",
    "```\n",
    "$ dvc stage add -n evaluate \\\n",
    "                -d metrics/metrics.csv \\\n",
    "                -d src/evaluate.py \\\n",
    "                --plots plots \\\n",
    "                -M metrics/metrics.json \\\n",
    "                python src/evaluate.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c43775-8602-49bd-b50a-17311efd7268",
   "metadata": {},
   "source": [
    "This time we are using two new tags: `-M` and `--plots`. While `-M` recognizes a specially-formatted metrics file like our `metrics.json`, `--plots` recognizes images as plots. These tags allow us to see the metrics and plots with `dvc metrics show` or `dvc plots show` commands. Again, we will see how to perform this in the next section.\n",
    "\n",
    "Here is how the stage looks in `dvc.yaml`:\n",
    "\n",
    "```YAML\n",
    "   evaluate:\n",
    "    cmd: python src/evaluate.py\n",
    "    deps:\n",
    "    - metrics/metrics.csv\n",
    "    - src/evaluate.py\n",
    "    metrics:\n",
    "    - metrics/metrics.json:\n",
    "        cache: false\n",
    "    plots:\n",
    "    - plots\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27843a7-b2d5-45b9-889a-33b17e9264f1",
   "metadata": {},
   "source": [
    "Let's run `dvc repro` one final time and make a snapshot of the changes:\n",
    "\n",
    "```\n",
    "$ dvc repro\n",
    "$ git add .\n",
    "$ git commit -m \"Add and run 'evaluate' stage of a pipeline\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a6897-cdc0-4cea-8d8c-f9f0632ec623",
   "metadata": {},
   "source": [
    "Finally, our pipeline is ready to go! Now, it is time to run some experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1686649-3577-4a2f-86ab-33df242d1027",
   "metadata": {},
   "source": [
    "### What is a machine learning experiment and how to run them in DVC?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446cf5c-5b36-4e93-86b4-149cdbe3f34e",
   "metadata": {},
   "source": [
    "### Introduction to DVC VSCode extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02749206-c471-40c5-930e-c93d739a2bbb",
   "metadata": {},
   "source": [
    "### Metrics and plots in VSCode DVC extension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
