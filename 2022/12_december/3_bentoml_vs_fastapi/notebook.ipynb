{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acabc113-bc62-454d-94ae-89a1c2606c3e",
   "metadata": {},
   "source": [
    "# BentoML vs FastAPI: Good-bye FastAPI For Machine Learning!\n",
    "## Objective comparison between BentoML and FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0449d-e711-4fbb-811e-bce279a908a6",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738689dc-85db-4c16-a926-0b68090a409b",
   "metadata": {},
   "source": [
    "FastAPI vs BentoML\n",
    "\n",
    "Similar Features\n",
    "- Pydantic validation\n",
    "- Swagger UI\n",
    "- async\n",
    "- Starlette\n",
    "\n",
    "ML Differentiators\n",
    "- Running models in separate processes\n",
    "    - https://modelserving.com/blog/breaking-up-with-flask-amp-fastapi-why-ml-model-serving-requires-a-specialized-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3d39b-f524-476a-bf82-8c9578b54a6e",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ae199-1cdb-48da-bab6-b23d51a80834",
   "metadata": {},
   "source": [
    "### But what is an API?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378bbf3b-dfc3-4fe6-bbbc-e12caea96323",
   "metadata": {},
   "source": [
    "The world is full of APIs. Yet, strangely, most do a poor job of explaining what they are. When you google \"what is an API?\", you'll get stuff like \"API stands for Application Programming Interface\" (like we would care), or as Wikipedia puts it:\n",
    "\n",
    "> An API is a way for two or more computer programs to communicate with each other.\n",
    "\n",
    "For me, at a high level, an API is just a plain-old URL. An example? ChatGPT3.\n",
    "\n",
    "The link chat.openai.com/chat is a URL that lets you interact with OpenAI's ChatGPT3 AI model. You send requests to the API via the website's UI using prompts. \n",
    "\n",
    "But APIs doesn't have to have fancy UIs. They can simply be URLs that programmers send requests to perform a variety of tasks like generating an image given a prompt. For example, I've built an API that returns a cuteness score given a pet's image. Here is its URL:\n",
    "\n",
    "PASTE THE URL HERE\n",
    "\n",
    "It has a PREDICT endpoint but APIs can have as many endpoints as possible that perform different functions.\n",
    "\n",
    "An API is just a way to hide complex programming logic like ML models with billions of parameters behind simple interfaces so that users can interact without any prior knowledge of how the thing behind was built or used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5eacb-a22d-41b7-b034-f5e352f6533b",
   "metadata": {},
   "source": [
    "So, in this article, we are comparing the king of API frameworks, FastAPI, which is generally used for web applications, to BentoML, a relatively young library specialized to deploy machine learning models as API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464ee21-fcfa-4552-b4cc-74cf02897836",
   "metadata": {},
   "source": [
    "### Similar features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f702a-9149-4c72-9a2b-8ea30c6446bd",
   "metadata": {},
   "source": [
    "Before we go into the differences in terms of machine learning use cases, let's outline discuss FastAPI and BentoML's similar features:\n",
    "\n",
    "- Starlette: built upon the same powerful ASGI web application building framework, making them fast and easy to use.\n",
    "- Automatic documentation with Swagger UI: both generate automatic documentation for APIs using the standard API docs format called OpenAPI.\n",
    "- Asynchronous requests: both allow asynchronous requests for heavy Input/Output-bound APIs. This means they can handle multiple requests simultaneously without executing them linearly.\n",
    "\n",
    "\n",
    "These are the basic features required by modern API frameworks. The real differentiators between BentoML and FastAPI are in machine learning use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b83281-28a8-4d82-9540-df754c686d6a",
   "metadata": {},
   "source": [
    "### Saving/loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a8f11-c78d-4200-a758-e5d7bb6a97d7",
   "metadata": {},
   "source": [
    "Building APIs in machine learning starts with a script or notebook that trains a model and saves it for deployment. How it saves is the problem because the format of the saved model directly affects how it is fed to the API server. \n",
    "\n",
    "In FastAPI, your choices are limited to a) pickling it or b) pickling it.\n",
    "\n",
    "```\n",
    "import joblib\n",
    "\n",
    "joblib.dump(your_awesome_model, \"models/model.pkl\")\n",
    "```\n",
    "\n",
    "With security and performance issues aside, the biggest problem in pickling is the total lack of model versioning and managing dependencies. \n",
    "\n",
    "In a typical ML project, engineers can build dozens or even hundreds of models and store them in model registry - a place where all trained models are maintained and versioned. Try building a proper model registry inside a directory with filenames like `model_v237.pkl`. \n",
    "\n",
    "In BentoML, there is a standard procedure to save and load models based on which framework they were used to train. Say you have a Keras CNN. You could save it with:\n",
    "\n",
    "```\n",
    "import bentoml\n",
    "\n",
    "bentoml.keras.save_model(\"cnn16\", model_cnn)\n",
    "```\n",
    "\n",
    "And load it back with:\n",
    "\n",
    "```\n",
    "retrieved_cnn = bentoml.keras.load_model(\"cnn16:latest\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2ca55-ecde-4a86-9d96-c4ee523d9218",
   "metadata": {},
   "source": [
    "> BentoML supports `framework.save/load_model` functions for 12 of the most popular ML frameworks and support for frameworks are planned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd9389-0505-4b75-b121-05e4eda98695",
   "metadata": {},
   "source": [
    "Models are not always stand-alone files, sometimes they have dependencies that must be saved with them. In that case, you can use the `custom_objects` and `metadata` parameters to save extra objects and info about the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aafe03f-059e-4bed-821f-1ce1791a6a14",
   "metadata": {},
   "source": [
    "```python\n",
    "import bentoml\n",
    "\n",
    "bentoml.keras.save_model(\n",
    "    \"cnn16\",\n",
    "    model_cnn,\n",
    "    metadata={\"desc\": \"CNN architecture with 16 starting filters\", \"owner\": \"Bex\"},\n",
    "    custom_objects={\"model_history\": history_object['history']}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb44c7c-5f75-4999-bcdc-bf2280392606",
   "metadata": {},
   "source": [
    "### Model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d6d50-11e8-4ae0-b625-79b4838df657",
   "metadata": {},
   "source": [
    "Once you save a model with `save_model` function, BentoML stores them into a local model registry, which is located at `~/bentoml/models`. Here is how you can list its contents:\n",
    "\n",
    "```\n",
    "$ bentoml models list\n",
    "Tag                        Module           Size        Creation Time        Path\n",
    "cnn16:2uo5fkgxj27exuqj  bentoml.keras  5.81 KiB    2022-12-19 08:36:52  ~/bentoml/models/cnn16/2uo5fkgxj27exuqj\n",
    "cnn16:nb5vrfgwfgtjruqj  bentoml.keras  5.80 KiB    2022-12-19 21:36:27  ~/bentoml/models/cnn16/nb5vrfgwfgtjruqj\n",
    "```\n",
    "\n",
    "Even if models are saved with the same name, they are assigned with unique random names. Once you choose the best model from the registry, you can share it with others by exporting them to a unified `.bentomodel` archive format, regardless of the model's framework:\n",
    "\n",
    "```\n",
    "$ bentoml models export cnn:version_tag .\n",
    "```\n",
    "\n",
    "If a teammate shares a model in a `.bentomodel` with all its custom objects and metadata, you can easily add it to your own model registry with:\n",
    "\n",
    "```\n",
    "$ bentoml models import ./shared_cnn.bentomodel\n",
    "```\n",
    "\n",
    "Imaging what a mess the process would be if you were working with pickles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70640d-70da-4e65-8cb2-985746f50825",
   "metadata": {},
   "source": [
    "You might be saying, \"Well, great! I can use BentoML for model registry and still build the API with FastAPI\". In that case, hold off making a decision for a little longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e437278-bf1d-484e-9792-4e454a70e6c9",
   "metadata": {},
   "source": [
    "### Input/Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69306ece-34fd-4374-96ac-07666cce74d1",
   "metadata": {},
   "source": [
    "APIs communicate in JSONs. This is a huge headache for data scientists and ML engineers, most of whom spend their lives working with NumPy arrays and Pandas DataFrames. \n",
    "\n",
    "Let's say you have a model trained on a dataset with four features. Here is how you would write the input/output formats of the model in FastAPI:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Input(BaseModel):\n",
    "    feature_1: float\n",
    "    feature_2: float\n",
    "    feature_3: float\n",
    "    feature_4: float\n",
    "\n",
    "class Output(BaseModel):\n",
    "    prediction: float\n",
    "    \n",
    "# Load the trained model\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "def predict(inputs: Input):\n",
    "    # Convert the input data to a numpy array\n",
    "    data = np.array([inputs.feature_1, inputs.feature_2, inputs.feature_3, inputs.feature_4])\n",
    "\n",
    "    # Make a prediction using the model\n",
    "    prediction = model.predict([data])[0]\n",
    "\n",
    "    # Return the prediction as a response object\n",
    "    return Output(prediction=prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a15cb-d3c5-46dc-bb1a-fc679c13794f",
   "metadata": {},
   "source": [
    "FastAPI uses Pydantic library to provide data validation, which means if the input, its shape and data types don't match the `Input` class, FastAPI will throw an error.\n",
    "\n",
    "But you will only face such easy scenarios in cheap machine learning courses. Real-world problems are much more complicated. For example, how would you define the input if your model was trained on hundreds of features (which is pretty common). You would have to write a class with as many attributes as possible, which is absurd. \n",
    "\n",
    "If you were to use BentoML for validating NumPy array inputs, here is how you'd do it:\n",
    "\n",
    "```python\n",
    "import bentoml\n",
    "from bentoml.io import NumpyNdarray\n",
    "\n",
    "\n",
    "runner = bentoml.sklearn.get(\"model_name:latest\").to_runner()\n",
    "\n",
    "svc = bentoml.Service(\"classifier\", runners=[runner])\n",
    "\n",
    "@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n",
    "def classify(input_series: np.ndarray) -> np.ndarray:\n",
    "    result = runner.predict.run(input_series)\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e77742-69ea-4502-8541-8bd37ce7a3d7",
   "metadata": {},
   "source": [
    "The `NumpyNdarray` class validates inputs against NumPy arrays of any shape. But what if you want to enforce a certain shape to the inputs (which is a best practice)? You could do it with a single parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956a486-86fb-4a25-875d-f36dc2101118",
   "metadata": {},
   "source": [
    "```python\n",
    "@svc.api(input=NumpyNdarray(shape=(-1,15), enforce_shape=True), output=NumpyNdarray())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72baa5-fefd-4809-880d-b6df7ac83ece",
   "metadata": {},
   "source": [
    "Providing (-1, 15) to the `shape` parameter will validate the inputs to have as many rows as possible but always 15 features. Here is the FastAPI and Pydantic equivalent:\n",
    "\n",
    "```python\n",
    "class Input(BaseModel):\n",
    "    data: List[conlist(item_type=float, min_items=15, max_items=15)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17716e6-7269-4e9d-aa26-13806efb5894",
   "metadata": {},
   "source": [
    "If you have trouble understanding the above class, you aren't to blame. I've scoured StackOverflow for hours before I could piece together the last line of code. \n",
    "\n",
    "On top of everything, models don't only deal with NumPy arrays. The data sent to the API could be anything from Pandas DataFrames to binary files like images or audio. The `bentoml.io` module contains validation classes for the most popular types of inputs and outputs:\n",
    "\n",
    "- PandasDataFrame\n",
    "- PandasSeries\n",
    "- JSON\n",
    "- Text\n",
    "- Image\n",
    "- File\n",
    "\n",
    "Engineers must know so many things before they can deploy their models. Learning Pydantic and JSON just to be able to feed the data to the API would just be overwhelming. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7aad45-23de-4c20-87d5-fe607c44e6b9",
   "metadata": {},
   "source": [
    "### Packaging the service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69f223-e07d-4f31-8915-344ef194db3b",
   "metadata": {},
   "source": [
    "Once you have a service script ready, you have to package it for deployment. One of the most popular ways of doing this is creating a Docker image for the API. Having a docker image makes it very easy to host the API on different operating systems and cloud environments.\n",
    "\n",
    "The FastAPI way requires some Docker knowledge, mainly, how to create a Dockerfile like below:\n",
    "\n",
    "\n",
    "```shell\n",
    "# Get the Fast API image with Python version 3.7\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
    "\n",
    "# Create the directory for the container\n",
    "WORKDIR /app\n",
    "COPY requirements.txt ./requirements.txt\n",
    "\n",
    "# Install the dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY ./app.py ./\n",
    "\n",
    "# Copy the serialized model\n",
    "COPY ./models/cnn_model.pkl ./models/cnn_model.pkl\n",
    "\n",
    "# Run by specifying the host and port\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f37ff-901d-442f-bc8c-2445f72f6a3e",
   "metadata": {},
   "source": [
    "Basically, you have to spoon-feed everything inside the Dockerfile so that when you run `docker build -t fastapiapp:latest -f docker/Dockerfile .`, the image should be created without errors.\n",
    "\n",
    "In BentoML, everything is much simpler again. In the root directory, you create a `bentofile.yaml` with the following template:\n",
    "\n",
    "```YAML\n",
    "service: \"service.py:service_nae\"\n",
    "include:\n",
    " - \"*.py\"\n",
    "python:\n",
    "  packages:\n",
    "   - scikit_learn\n",
    "   - numpy\n",
    "   - tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0414f-b6b0-4922-a2ac-799aa70d47ad",
   "metadata": {},
   "source": [
    "In the `include` field, you specify all the files the service script requires to run without errors. Under package, you list your project's dependencies. Then, call `bentoml build` on the terminal:\n",
    "\n",
    "```\n",
    "$ bentoml build\n",
    "```\n",
    "\n",
    "The `build` command packages your entire project into a stand-alone archive inside the `~/bentoml/bentos` folder. \n",
    "\n",
    "To convert the archive into a Docker image, you run:\n",
    "\n",
    "```\n",
    "$ bentoml containerize model_name:latest\n",
    "```\n",
    "\n",
    "Now, you can deploy the image to any environment you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab83f19-e3ed-4423-aa4a-eb3c1d30524b",
   "metadata": {},
   "source": [
    "### Deploying the service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05e346-34bb-4607-aeca-4880363714ca",
   "metadata": {},
   "source": [
    "Once the service is packaged, preferably into a Docker container, it is time to deploy it. Unfortunately, FastAPI's functionality stops there. It gives you the API but doesn't care about how you deploy it. You are supposed to figure that out yourself.\n",
    "\n",
    "In contrast, BentoML has a dedicated helper library called `bentoctl` that allows you to deploy your containerized APIs on any of the most popular cloud platforms (AWS, GCP, Azure, Heroku). For example, it only takes a few commands to deploy one of the models in the model registry to AWS SageMaker:\n",
    "\n",
    "```\n",
    "$ pip install bentoctl terraform\n",
    "$ bentoctl operator install aws-sagemaker\n",
    "$ export AWS_ACCESS_KEY_ID=REPLACE_WITH_YOUR_ACCESS_KEY\n",
    "$ export AWS_SECRET_ACCESS_KEY=REPLACE_WITH_YOUR_SECRET_KEY\n",
    "$ bentoctl init\n",
    "$ bentoctl build -b model_name:latest -f deployment_config.yaml\n",
    "$ terraform init\n",
    "$ terraform apply -var-file=bentoctl.tfvars -auto-approve\n",
    "```\n",
    "\n",
    "If you want a detailed overview of the above steps, check out my comprehensive tutorial:\n",
    "\n",
    "https://towardsdatascience.com/comprehensive-guide-to-deploying-any-ml-model-as-apis-with-python-and-aws-lambda-b441d257f1ec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9924d40a-036a-4d13-a824-d2bd2c29aaf0",
   "metadata": {},
   "source": [
    "### GPU serving\n",
    "Most machine learning models need access to GPUs for heavy workloads. This means you should build the Docker image for the API with GPU support. \n",
    "\n",
    "The GPU support is provided by the CUDA library of NVIDIA and installing CUDA manually is one of the most horrible experiences you will face as a programmer. Installing different CUDA versions for different GPU libraries like TensorFlow, PyTorch or XGBoost is called \"CUDA hell\" (which rightfully sounds like \"Go to hell!\"). \n",
    "\n",
    "To install CUDA in your Docker image, you would need a monster of a Dockerfile like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b8139-7601-4756-a01b-37564948e8a9",
   "metadata": {},
   "source": [
    "```Dockerfile\n",
    "FROM nvidia/cuda:11.2.0-runtime-ubuntu20.04\n",
    "\n",
    "# install utilities\n",
    "RUN apt-get update && \\\n",
    "    apt-get install --no-install-recommends -y curl\n",
    "\n",
    "ENV CONDA_AUTO_UPDATE_CONDA=false \\\n",
    "    PATH=/opt/miniconda/bin:$PATH\n",
    "RUN curl -sLo ~/miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.9.2-Linux-x86_64.sh \\\n",
    "    && chmod +x ~/miniconda.sh \\\n",
    "    && ~/miniconda.sh -b -p /opt/miniconda \\\n",
    "    && rm ~/miniconda.sh \\\n",
    "    && sed -i \"$ a PATH=/opt/miniconda/bin:\\$PATH\" /etc/environment\n",
    "\n",
    "# Installing python dependencies\n",
    "RUN python3 -m pip --no-cache-dir install --upgrade pip && \\\n",
    "    python3 --version && \\\n",
    "    pip3 --version\n",
    "\n",
    "RUN pip3 --timeout=300 --no-cache-dir install torch==1.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "\n",
    "COPY ./requirements.txt .\n",
    "RUN pip3 --timeout=300 --no-cache-dir install -r requirements.txt\n",
    "\n",
    "# Copy model files\n",
    "COPY ./model /model\n",
    "\n",
    "# Copy app files\n",
    "COPY ./app /app\n",
    "WORKDIR /app/\n",
    "ENV PYTHONPATH=/app\n",
    "RUN ls -lah /app/*\n",
    "\n",
    "COPY ./start.sh /start.sh\n",
    "RUN chmod +x /start.sh\n",
    "\n",
    "EXPOSE 80\n",
    "CMD [\"/start.sh\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c0aff-db7e-4b78-bddf-0cf6ce931a6a",
   "metadata": {},
   "source": [
    "In BentoML, installing CUDA is easy as adding a single field to the `bentofile.yaml`:\n",
    "\n",
    "\n",
    "```YAML\n",
    "service: \"service.py:service_name\"\n",
    "include:\n",
    "- \"*.py\"\n",
    "python:\n",
    "    packages:\n",
    "    - torch\n",
    "    - torchvision\n",
    "    - torchaudio\n",
    "docker:\n",
    "    distro: ubuntu\n",
    "    python_version: \"3.8.12\"\n",
    "    cuda_version: \"11.6.2\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660aa33a-ff6a-46d2-b9af-1de979c314e6",
   "metadata": {},
   "source": [
    "Then, you can just repeat the steps outlined in the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084c863-3c9f-4785-a6d4-d9d850b251b6",
   "metadata": {},
   "source": [
    "### Batching inputs\n",
    "https://docs.bentoml.org/en/latest/guides/batching.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
