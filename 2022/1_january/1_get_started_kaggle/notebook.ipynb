{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Get Started on Kaggle in 2022 (Even If You Are Terrified)\n",
    "## Your road to GrandMaster\n",
    "![](images/pixabay.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3849936'>Peter Fischer</a>\n",
    "        on \n",
    "        <a href='https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3849936'>Pixabay</a></strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this place is awesome. Look at all these Kagglers. So much to learn. The competitions sound immense fun. Even Google seems to host them here. I swear, I am gonna crush here. \n",
    "\n",
    "Let me check out some of these gold notebooks. Whoa, it is written by a grandmaster. THAT sounds luxurious. The guy seems to know so damn much. I've never seen a data scientist so cool. Let's see his profile - yep, he is a rocket scientist.\n",
    "\n",
    "What am I thinking? I can't compete with that. I should probably go before I embarrass myself. Yeah, that's right. I will study up for a couple more years on Coursera and come back and show 'em. \n",
    "\n",
    "...\n",
    "\n",
    "That pretty much sums up how I felt when I first set my keyboard on Kaggle. I am sure that's the case for so many others. Feeling that dreadful sinking sensation in your stomach when you realize you are not *just good enough* to play with the big guys. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: The boring setup I must mention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you [create your account](https://www.kaggle.com/account/login) and become a novice (the lowest rank on Kaggle), you are just inches away from becoming an official Kaggler. \n",
    "\n",
    "All you have to do is to make a single submission to a competition and earn your contributor badge. Almost 40% of all Kaggle users are contributors and even larger proportion are novices.\n",
    "\n",
    "Sure, if you are a complete beginner there is no shame in taking a few of [Kaggle's official courses](https://www.kaggle.com/learn) to get your skills up to scratch. I recommend taking the free [Intro to ML course first](https://www.kaggle.com/learn/intro-to-machine-learning) as it will explain the basics of ML and hold your hands as you make your first submission to a competition.\n",
    "\n",
    "After setting up your account, there is a trap you have to avoid so that you won't kill your motivation before you even have it. This brings us to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Be part of the community first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many people go about joining Kaggle the wrong way. As everyone brands Kaggle as a competitions platform, most feel a mounting pressure to join a competition straight away to become a 'Kaggler'. Nothing could be further from the truth. \n",
    "\n",
    "Feeling that you have to join a competition right away is a massive trap. You will just end up miserable if you haven't developed the right skills yet because Kaggle is brutally competitive. Even the smallest challenges attract many skilled specialists. \n",
    "\n",
    "So what is the solution?\n",
    "\n",
    "Be part of the community first. Feel that you are a member of this group of cool people so that you eliminate your impostor syndrome and can honestly say yourself, \"I am a Kaggler\". \n",
    "\n",
    "Well, how do you do that?\n",
    "\n",
    "You start reading notebooks. A lot. By everyone. I suggest picking a competition you are interested in (see the next section for which competitions are suitable for you) and ordering the list of notebooks by number of notebooks. \n",
    "\n",
    "SHOW GIF OF ORDERING THE NOTEBOOKS\n",
    "\n",
    "You scroll down right to the bottom and open the notebooks with lowest upvotes. Often, these notebooks are written by beginners and they need just as much support and feedback as you. By reading their work and giving feedback, you grow together with these people. Once you start communicating with the author in the comments section (there is a very high chance they respond to every comment), you build a connection with them. They will probably follow you back once comment on a few of their notebooks (followers are important on Kaggle. More on this later.)\n",
    "\n",
    "Now, in every notebook, there a few to-dos to be as productive as possible.\n",
    "\n",
    "First, leave at least one comment. It can be about anything - the way the code is written (clean, follows best practices), how the author explains his thought process, the quality of the data visualizations or any imaginable aspect of the notebook. \n",
    "\n",
    "Based on your own experience, you can suggest improvements on parts of the notebook that need change. If the author knows more than you on the topic of the notebook, say that too and tell them you learned something new. \n",
    "\n",
    "MAYBE ADD AN IMAGE\n",
    "\n",
    "If you don't know much of the content of the notebook, fork it and start tinkering with every code cell. Read the docs and tutorials on the unknown functions until you completely understand what the code is doing in every line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you repeat this process for 10-20 notebooks, you will realize you learn much more than you ever will in a course in a similar span of time. And the best part is that now, you aren't just nobody - you know and follow the authors of these notebooks and they probably remember you as well if you left good enough feedback and appreciation for their work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understand how Kaggle hotness system works and its progression system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, you have to look at this table on the Kaggle docs: \n",
    "\n",
    "INSERT A SCREENSHOT\n",
    "\n",
    "It shows how much upvotes to earn medals and how much medals it takes to achieve ranks in four categories. Each category requires different amounts of effort to progress, competitions being the hardest.\n",
    "\n",
    "You might not be in Kaggle for ranks, which is perfectly fine. But if you want build good connections and expand your network, earning a title is a surefire way to earn credibility. The most respectable titles are masters and GMs in either competitions or notebooks category (in my opinion, at least) but any GM title is always enough to earn respect and admiration from many. \n",
    "\n",
    "Now, about the Kaggle network - it is similar to how LinkedIn works (but without the complications of second and third-degree connections). \n",
    "\n",
    "If you publish a dataset or a notebook or make a comment, it will often show up on your followers' feed if it matches their preferences. If they interact with your work, this will similarly display on the feeds of their followers. So, you might imagine what happens if you have many followers with large enough audiences of their own. Your work gets more recognized. \n",
    "\n",
    "But there are no shortcuts - you have to put in the work to get your initial follower base. The best to do it is to publish as many high-quality, public notebooks as possible (will talk more about this later) and interact with others' work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Choose a competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready. Everything has been leading up to this - choosing and entering a competition.\n",
    "\n",
    "Even if you are a complete beginner, I DON'T recommend these *Getting-started* competitions:\n",
    "\n",
    "- [The Titanic - classification]() - add a link\n",
    "- The House Prices regression\n",
    "\n",
    "Almost any beginner starts on Kaggle with these, so they are very saturated. Besides, there is a lot of plagiarism there because people will just copy from top solutions from previous winners to move up the leaderboard. \n",
    "\n",
    "Don't be surprised if you find notebooks with thousands of upvotes - these competitions have been there since the dawn of Kaggle and people've had time to explore their datasets in every way imaginable.\n",
    "\n",
    "Instead, start with the monthly Tabular Playground Series (TPS) competitions, like I did. They are a teensy bit harder than getting-started comps but are much more fun and have larger capacity to teach you something new. \n",
    "\n",
    "Because of their beginner-level nature, they attract very few grandmasters but are still hard to win (I am yet to do it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ultimate 4-week workflow to succeed in a competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define \"success in a competition\". \n",
    "\n",
    "For me, I am pretty happy if my score is within 1% of the first place submission. For example, in September TPS which I was very active in, I scored 0.81708 ROC AUC while the first place scored 0.81775. I took the 332nd place even with a difference of 0.0067. \n",
    "\n",
    "Most of the time, you can land in the reach of the top score even with simple models and a bit of [hyperparameter tuning](https://towardsdatascience.com/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c). But to reach the top, you have to do a lot of tedious work and experimentation. In the end, you will mostly end up with an ensemble of some 10 or more models which will be useless in the real world and is only a marginally better than a much simpler and practical solution. \n",
    "\n",
    "That is what's required to win a competition on Kaggle, at least on TPS competitions. The featured and code competitions (the ones with prize money) are different. They have runtime and resource limits. For example, some competitions don't accept solutions that train for more than 8 hours to keep the solutions practical in case the host wants to productionalize some of them in the future. \n",
    "\n",
    "No matter the type of the comp, there are a few steps you can take to guarantee success. I'll outline them in the case of TPS competitions but you can always extend the ideas depending on the duration of the competition. \n",
    "\n",
    "So, to get good results in TPS which lasts only a month, here is how you should allocate your time:\n",
    "\n",
    "**Week 1: Exploratory Data Analysis (EDA)**. Its importance is undeniable and it will be integral to how you come up with solutions later. You should pay special attention to the features in the data and how you can normalize them ([different distributions need different algorithms](https://towardsdatascience.com/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94?source=your_stories_page----------------------------------------) to do so). EDA also gives you ideas for possible feature engineering ideas, another important part of your success. \n",
    "\n",
    "In week 1, you should also develop your validation strategy. This means that you set up a baseline score with a model ([XGBoost is popular](https://towardsdatascience.com/20-burning-xgboost-faqs-answered-to-use-the-library-like-a-pro-f8013b8df3e4)) and see if you can improve on it by using other methods. \n",
    "\n",
    "**Week 2: Model selection**. Now, you try out different models with default hyperparameters to see which one has the highest score over your baseline. It can be tree-based like XGBoost, LGBM or CatBoost or linear models such as logistic regression, LR or something similar. \n",
    "\n",
    "I also recommend going through some rarer models in Sklearn based on your intuition and knowledge you got from EDA. You may find a surprise or two that could give you even higher scores than tree-based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is important that you judge these models based on [both their training and test scores](https://towardsdatascience.com/6-sklearn-mistakes-that-silently-tell-you-are-a-rookie-84fa55f2b9dd) through [cross-validation](https://towardsdatascience.com/5-cross-validation-techniques-you-need-to-create-models-that-people-trust-62c4629a678b?source=your_stories_page----------------------------------------) (the keywords are linked to relevant tutorials).\n",
    "\n",
    "**Week 3: Feature engineering**. Now, we are onto the secret sauce all the GMs use in their path to the top. Feature engineering is not you learn in a course, it is completely up to your domain knowledge of the problem and your experience as a data scientist. There is a lot that can be said about FE but here it is in brief: \n",
    "\n",
    "> Feature engineering is about shaping and transforming datasets so that the models can learn as much information from them as possible. \n",
    "\n",
    "FE is especially important in deep learning and [time series competitions](https://towardsdatascience.com/top-4-time-series-feature-engineering-lessons-from-kaggle-ca2d4c9cbbe7). \n",
    "\n",
    "Every time you make a change to the data (add a new column, change an existing one, etc.), you should run your best model(s) you found in week 2 to see if this change makes sense. \n",
    "\n",
    "**Week 4: Hyperparameter tuning**: Now that you've identified your top models and added new, juicy features, it is time to squeeze every bit of performance from these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need a good tuning framework. I am especially fond of [Optuna](https://towardsdatascience.com/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c) and so are many other Kagglers.\n",
    "\n",
    "Once you have your best model tuned, you can generate predictions and pack them up for a submission. Or you can combine multiple models as an ensemble to boost your score even more. Ensemble solutions are mostly useless in real world as they are very costly but you will see that Kaggle is full of them. Upwards the 50th percentile of the leaderboard, people often use ensembles. You can learn more about them [here](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python).\n",
    "\n",
    "And finally, all these steps are iterative and can change based on your needs. You might go back to any one of them anytime and stretch or shorten the time you need to complete each step. You will probably do so as you get new ideas from others and come up with your own down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start publishing notebooks - backbone of your success"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
