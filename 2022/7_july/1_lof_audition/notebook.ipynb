{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233a246d-3ad9-4928-ab44-21dee56b86ab",
   "metadata": {},
   "source": [
    "# Local Outlier Factor script for the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b796602-0d84-445d-8748-9b5caba2caf2",
   "metadata": {},
   "source": [
    "## General notes\n",
    "- works well for moderately high-dimensional datasets\n",
    "- The returned LOF score represents abnormality of each data point\n",
    "- LOF is the local density deviation of a sample compared to its neighbors\n",
    "- Outliers are marked if they have substantially lower density than their neighbors\n",
    "- The advantage of LOF is that it takes both local and global properties of the dataset\n",
    "- The aim is not to find isolated data points like in Isolation Forest but only to find isolated data points with respect to its neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1192836-f73e-4e18-8907-00f6094c2973",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. What is LOF?\n",
    "2. How to calculate LOF score for each data point\n",
    "3. Fitting LOF from PyOD using a sample dataset\n",
    "4. Rules for choosing n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48949a-17d2-4813-a9a7-71fc567a9634",
   "metadata": {},
   "source": [
    "## The script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c75a8-6820-4c24-9ead-038942f13289",
   "metadata": {},
   "source": [
    "### What is LOF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf96bb4-f196-477f-aa1d-c6864c7e0529",
   "metadata": {},
   "source": [
    "In the previous lessons, you have learned how to use a distance-based model - KNN. In this lesson, you will learn about a popular density-based algorithm called Local Outlier Factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5e499-cf6e-4d3b-b3a8-caf0d45b7e45",
   "metadata": {},
   "source": [
    "LOF is a well-known algorithm that has existed since 2000. It works well with moderately high-dimensional datasets and is one of the fastest outlier classifiers. \n",
    "\n",
    "LOF classifies data points into inliers and outliers using a local outlier factor score, which is where the name is taken from. \n",
    "\n",
    "The LOF score is based on the concept of local density, where locality is defined by choosing *k* nearest neighbors, like in KNN. The density itself is calculated between a data point and its distances to its chosen neighbors.\n",
    "\n",
    "Data points with similar densities will form a cluster, while samples with substantially lower densities than their local neighborhood are classified as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab238e7e-e862-4b54-9540-ee03fdcc0d2c",
   "metadata": {},
   "source": [
    "Here, the word \"local\" is very important. The LOF score is not compared to the rest of the samples in the dataset, but only to their local neighborhood.\n",
    "\n",
    "In the plot, we can see two clusters of data points and a dozen clear outliers. The size of the red circles represent how anomalous they are compared to their local neighborhood. The higher their LOF score, the bigger the circles. \n",
    "\n",
    "I want you to pay attention to the two points I highlighted. Point A is an outlier but the circle size isn't very large. That's because it is much closer to its local neighborhood compared to point B. Point B is far away and therefore, have much more deviation and less density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d72db4-0eba-4935-9ff6-5a70c1d3282a",
   "metadata": {},
   "source": [
    "### Fitting LOF from PyOD using a sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67be37d-e997-4db4-bbc0-20cefa3bcd00",
   "metadata": {},
   "source": [
    "Now, let's see LOF in action using the Statlog Shuttle dataset from UCI machine learning repository. I have preprocessed the data for this course and turned it into a simple multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74ba2aa9-f6ef-4bc3-9f94-f6da9d550e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>-5</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>-26</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>-4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
       "0    50           21           77            0           28            0   \n",
       "1    53            0           82            0           52           -5   \n",
       "2    37            0           76            0           28           18   \n",
       "3    37            0           79            0           34          -26   \n",
       "4    85            0           88           -4            6            1   \n",
       "\n",
       "   attribute_6  attribute_7  class  \n",
       "0           27           48      2  \n",
       "1           29           30      1  \n",
       "2           40           48      4  \n",
       "3           43           46      1  \n",
       "4            3           83      2  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "shuttle = pd.read_csv(\"data/shuttle_preprocessed.csv\")\n",
    "shuttle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae7286-c455-46c9-ad43-c9d933527d33",
   "metadata": {},
   "source": [
    "The dataset has 8 features and one target column, which is named class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "488a1a70-8968-4d6d-a573-2b65dec84745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49097, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuttle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ed01d-03ac-4376-94bc-136888f9dc37",
   "metadata": {},
   "source": [
    "As LOF uses distance metrics, we are required to normalize the dataset before fitting LOF. Therefore, in lines 1-2, we are importing QuantileTransformer after the LocalOutlierFactor estimator from PyOD. \n",
    "\n",
    "Next, we define a `transform_detect` function with 5 parameters - X and y for feature and target arrays, k for the number of neighbors and two parameters for contamination and distance calculation.\n",
    "\n",
    "Inside `transform_detect`, we initialize QuantileTransformer and use it to normalize the X array. Then, we fit LOF with given hyperapameters and predict outlier labels.\n",
    "\n",
    "Finally, we return the X and y arrays, dropping the found outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3343bd9-96ea-494c-a7c6-2163b1f73908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.lof import LocalOutlierFactor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "def transform_detect(X, y, k=20, contamination=0.1, metric=\"manhattan\"):\n",
    "    qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "    X.loc[:, :] = qt.fit_transform(X)\n",
    "\n",
    "    lof = LocalOutlierFactor(n_neighbors=k, contamination=contamination, metric=metric)\n",
    "    labels = lof.fit_predict(X)\n",
    "\n",
    "    return X[labels == 1], y[labels == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53296c3-66bd-4e2b-a28d-f660fa6e9eed",
   "metadata": {},
   "source": [
    "Let's test the function on the Shuttle dataset. To do so, we first extract the feature and target arrays using Pandas and pass them to transform_detect function with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1f7130f7-a1aa-40a9-b6d9-5bfab690c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shuttle.drop(\"class\", axis=1)\n",
    "y = shuttle[[\"class\"]]\n",
    "\n",
    "X_transformed, y_transformed = transform_detect(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fda5f2-c2b6-40e0-9e35-a6c1e59d157d",
   "metadata": {},
   "source": [
    "By checking the shape of the transformed X array, we learn that LOF dropped about 5000 rows which it found as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bbca872b-25c4-40b8-9da3-3a59928248be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44187, 8)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ead6f1-0f78-4efa-8f0c-0f6882a8cb73",
   "metadata": {},
   "source": [
    "Now, we create another function for evaluating a simple classifier on our outlier-free dataset. In the body of the `evaluate_classifier` function, we first partition the data into training and validation sets. \n",
    "\n",
    "Next, we fit the classifier to the training set and generate predictions. In the final line, we return a balanced accuracy score, which takes class imbalance into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22d0eccb-828f-4a7d-8a4e-63e25a0b0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def evaluate_classifier(clf, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "\n",
    "    return balanced_accuracy_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e82dd90-085b-4149-80f4-5d691c674106",
   "metadata": {},
   "source": [
    "Let's put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b86f811e-511a-4040-b1f2-d7e6ff98b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shuttle.drop(\"class\", axis=1)\n",
    "y = shuttle[[\"class\"]]\n",
    "\n",
    "X_transformed, y_transformed = transform_detect(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de082318-612d-48e1-9f4f-4f9f5015ab94",
   "metadata": {},
   "source": [
    "After we generate the outlier-free dataset with `transform_detect` function once again, we pass it to `evaluate_classifier` with Random Forest as our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8d769db6-8ab9-46e1-9c1c-28d6ed8d060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "b_accuracy = evaluate_classifier(clf, X_transformed, y_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f13f3b2d-fc50-42f0-bfae-616666943d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9780409356032493"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d219fc-aa37-488e-8d40-5913a9a82466",
   "metadata": {},
   "source": [
    "The final accuracy is close-to-perfect 98%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ee1e6-1bbd-4d3b-84cd-8f18ab706b72",
   "metadata": {},
   "source": [
    "### Simple rules for choosing n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc47f5b-33b7-47fe-bedf-edef014ad673",
   "metadata": {},
   "source": [
    "Like in KNN, the most important parameters of LOF are contamination and n_neighbors. As we can only tune contamination by trial and error, we will focus on tuning n_neighbors.\n",
    "\n",
    "For n_neighbors, Sklearn documentation suggests choosing 20 if the contamination level is below 10%. Anything higher than that, you increase n_neighbors accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227fb8f-9b51-440e-85c0-d1d5a997fbbf",
   "metadata": {},
   "source": [
    "Now, it is your turn to fit an LOF estimator and tune its hyperparameters on another dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e80edd-92ab-4918-8574-439e502bb811",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbabb63-be06-4922-bb91-b9b2a3045e40",
   "metadata": {},
   "source": [
    "Let's see how to do that by trying out 6 different values for n_neighbors in a loop. We first create a list of ks and an empty dictionary to store balanced accuracy score for each k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79cdae92-b726-4f21-985c-0cddb0d0db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [25, 30, 40, 50, 80, 100]\n",
    "scores = dict()\n",
    "\n",
    "for k in k_list:\n",
    "    X_transformed, y_transformed = transform_detect(X, y, k=k)\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    b_accuracy = evaluate_classifier(clf, X_transformed, y_transformed)\n",
    "\n",
    "    scores[f\"k={k}\"] = b_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce7b88-4f2c-4367-b6f9-d0f75f8bcdd4",
   "metadata": {},
   "source": [
    "After the for loop finishes, we have a `scores` dictionary that contains n_neighbors and accuracy score key-value pairs. We can see that setting n_neighbors to 100 gives the best score, even though the improvement on the old score is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8d7430f8-fcdf-4570-ba50-15dd77a9fec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k=25': 0.9785592822823691,\n",
       " 'k=30': 0.9789278206214174,\n",
       " 'k=40': 0.9772928732958232,\n",
       " 'k=50': 0.9789704082619046,\n",
       " 'k=80': 0.9794938801789923,\n",
       " 'k=100': 0.9800616132195346}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
