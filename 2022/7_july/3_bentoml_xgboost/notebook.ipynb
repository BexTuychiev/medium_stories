{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a17246-0add-4cec-9f07-000ed7f7c9bc",
   "metadata": {},
   "source": [
    "# Deploying Machine Learning Models as API Services With BentoML And AWS Lambda\n",
    "## Get that model online!\n",
    "![](images/pexels.jpg)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://www.pexels.com/photo/blue-and-red-galaxy-artwork-1629236/'>Suzy Hazelwood</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce1397-b247-4884-b37a-1e0232bfbcae",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d6393-49ee-41c6-8e3b-786deaf3e411",
   "metadata": {},
   "source": [
    "According to ml-ops.org, the current state of MLOps stack looks like the following template:\n",
    "\n",
    "![](https://ml-ops.org/img/mlops-full-stack.png)\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://valohai.com/blog/the-mlops-stack/'>Henrik Skogström</a>\n",
    "        on \n",
    "        <a href='https://ml-ops.org/content/state-of-mlops'>ml-ops.org</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9c266-64f0-48fa-94dd-45dfba3d6e61",
   "metadata": {},
   "source": [
    "The industry is fast-changing, leading to multiple candidates for performing each of the operations in the template.\n",
    "\n",
    "BentoML is a new open-source library that handles the model serving part of the MLOps life cycle. It offers a Python API that allow users to serve their models as APIs in a simple script and get an HTTP server they can send POST requests to generate predictions on unseen data. \n",
    "\n",
    "This lightweight API then can be inserted into any machine learning use case, be it a Docker container or a web app.\n",
    "\n",
    "In this post, we will go deep into how you can use BentoML and its Bentos API and how you can combine it with AWS Lambda to get your models up and running for anyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08528785-2d22-47e8-96a4-58c1f37899cc",
   "metadata": {},
   "source": [
    "## What is BentoML and its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2aca6c-3b40-4abb-848a-84e2154183f9",
   "metadata": {},
   "source": [
    "To maximize the business impact of machine learning, the hand-off between data scientists and engineers from model training to deployment should be fast and iterative. However, data scientists often don't have the skills to properly package trained models and push them to the engineers while engineers struggle with working models that come from dozens of different ML frameworks.\n",
    "\n",
    "BentoML was created to solve these issues and make the hand-off to production deployment as easy and fast as possible. In the coming sections, you will see how BentoML makes it stupidly easy to perform tedious operations. The examples are:\n",
    "- Saving any model of any framework into a unified format\n",
    "- Create an HTTP API endpoint with a single Python function\n",
    "- Containerize everything the model needs using Docker with a single CLI command\n",
    "\n",
    "So, without further ado, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6712872-007f-4152-8df7-25a6e5d7a15b",
   "metadata": {},
   "source": [
    "## Dataset preparation and model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9fd6f-c134-4c5e-b527-eec6ec9f32f8",
   "metadata": {},
   "source": [
    "The crux of the article is about model deployment, so I want to concentrate all your attention on that area only. For that purpose, I will assume you are reading this article with your best trained model already in hand and want to deploy it as soon as possible. \n",
    "\n",
    "To simulate that here, we will simply create a synthetic dataset, train an XGBoost model and move forward as though you have done all the previous steps of the MLOps life cycle like data cleaning, exploration, feature engineering, model experimentation, hyperparameter tuning and found the model that performs best on your problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1e2f33-1ca3-4ff1-a680-40539a70bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d54791c-f2c8-48b8-bf83-911bfcc1f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate the data\n",
    "n_samples, n_features = 10000, 7\n",
    "X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=5)\n",
    "\n",
    "# Save it as a CSV\n",
    "feature_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df[\"target\"] = y\n",
    "\n",
    "df.to_csv(\"data/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc6b34-9085-4455-9b6b-45cd59136706",
   "metadata": {},
   "source": [
    "We create a simple dataset with 7 features and 10k samples with a binary classification target. Now, we load it back into environment and train a vanilla XGBoost classifier and pretend that it is our best tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8453d788-028a-488f-9c7c-50d2b449d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, cross_validate, train_test_split\n",
    "\n",
    "# Load and prep the data\n",
    "data = pd.read_csv(\"data/data.csv\")\n",
    "X, y = data.drop(\"target\", axis=1), data[[\"target\"]]\n",
    "\n",
    "# Initialize a classifier\n",
    "clf = xgb.XGBClassifier(tree_method=\"gpu_hist\")\n",
    "\n",
    "# Cross-validate\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "scores = cross_validate(\n",
    "    clf,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"roc_auc\",\n",
    "    return_train_score=True,\n",
    "    return_estimator=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc2e29-c996-4333-85a9-44aeb849934a",
   "metadata": {},
   "source": [
    "After loading the data, we use 10-fold cross-validation and use ROC AUC score as a metric. For the sake of completeness, let's quickly log the train/validation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0203b5c9-d63e-45aa-a8ad-c3f72318be09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training ROC AUC: 0.999 ± 0.000\n",
      "Average test ROC AUC: 0.971 ± 0.003\n"
     ]
    }
   ],
   "source": [
    "avg_train = scores[\"train_score\"].mean()\n",
    "avg_test = scores[\"test_score\"].mean()\n",
    "\n",
    "std_train = scores[\"train_score\"].std()\n",
    "std_test = scores[\"test_score\"].std()\n",
    "\n",
    "print(f\"Average training ROC AUC: {avg_train:.3f} ± {std_train:.3f}\")\n",
    "print(f\"Average test ROC AUC: {avg_test:.3f} ± {std_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd923473-45af-4a77-8890-475d98e73ae7",
   "metadata": {},
   "source": [
    "We extract one of the models from the folds and save it as `clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec1e4dfb-cf7c-4fc3-9b20-a9b979755deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = scores[\"estimator\"][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0e6fc-45ea-477f-9996-c5663a8e84b0",
   "metadata": {},
   "source": [
    "Great! Now, we are ready for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c8dca-36e2-427d-854d-e6932ec5f7f9",
   "metadata": {},
   "source": [
    "## Saving trained models to BentoML format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4e24a-d9b2-4602-81a2-0a4611dc2146",
   "metadata": {},
   "source": [
    "Saving a trained model into BentoML-compatible format is done calling the framework-specific `save` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da3a5d27-bc53-41f1-a634-90f903c8e512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"xgb_initial:3bt3t6yqw6cnujcl\", path=\"C:\\Users\\bex\\bentoml\\models\\xgb_initial\\3bt3t6yqw6cnujcl\\\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml  # pip install bentoml\n",
    "\n",
    "bento_xgb = bentoml.sklearn.save_model(\"xgb_initial\", clf)\n",
    "bento_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f34ac-3867-49fa-b607-6a7cdb21dd96",
   "metadata": {},
   "source": [
    "Even though we trained an XGBoost classifier, we still use the `sklearn.save_model` command because we initialized the model in Sklearn API. The returned object is an instance of BentoML `Model` class with a label called *tag*. \n",
    "\n",
    "The tag consists of two parts - a name given by the user and a version string to differentiate between models saved at different times. Even if an identical model is saved, a new directory and a version string will be created for it. \n",
    "\n",
    "BentoML supports almost all important ML frameworks:\n",
    "- Classic: Sklearn, XGBoost, CatBoost, LightGBM\n",
    "- Deep learning: TensorFlow, PyTorch, PyTorch Lightning, Keras, Transformers\n",
    "- Others: ONNX, MLFlow, fast.ai, statsmodels, spaCy, h2o, Gluon, etc.\n",
    "\n",
    "Each of the frameworks have a corresponding `framework.save_model` command.\n",
    "\n",
    "When a model is saved, it goes into a local directory called BentoML model store. From the last output, we saw that my model store resides in `C:\\Users\\bex\\bentoml\\models`. You can see the list of all your models by calling the `bentoml models list` command in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e045e2-3a1e-4b44-811f-159b323b7641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tag                          Module           Size        Creation Time       \n",
      " xgb_initial:3bt3t6yqw6cnuj…  bentoml.sklearn  441.21 KiB  2022-07-31 15:02:11 \n",
      " xgb_initial:2y6k6tyqw6i6kj…  bentoml.sklearn  441.21 KiB  2022-07-31 15:02:07 \n",
      " xgb_initial:y7ug7oaqw6kjaj…  bentoml.sklearn  441.21 KiB  2022-07-31 15:01:43 \n",
      " keras_conv2d_smaller:4zngb…  bentoml.keras    54.59 MiB   2022-04-13 15:03:00 \n",
      " conv2d_larger_dropout:rsl6…  bentoml.keras    128.58 MiB  2022-04-12 20:30:57 \n",
      " conv2d_larger_dropout:3ygl…  bentoml.keras    128.58 MiB  2022-04-12 20:18:55 \n",
      " conv2d_larger_dropout:szo4…  bentoml.keras    128.58 MiB  2022-04-09 13:53:55 \n",
      " keras_conv2d:b52h7x5xpk2be…  bentoml.keras    128.58 MiB  2022-04-09 01:25:41 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bex\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (5.0.0)/charset_normalizer (2.0.6) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "!bentoml models list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71758e4a-75e8-47a1-8266-c611849a07cd",
   "metadata": {},
   "source": [
    "You can also see models from my other projects.\n",
    "\n",
    "> Note: in BentoML docs and this article, the names \"model\" and \"tag\" are used interchangeably to refer to saved models in the model store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fd787-b9f7-4477-a413-bdf12825fd35",
   "metadata": {},
   "source": [
    "The `save_model` has other parameters that allow you to pass extra information about the model, from metadata to additional user-defined objects (e.g. weights of your model as a separate object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6351e190-14b9-410c-8861-ff8c9e6e8c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"xgb_custom:hgpjk2iqxk67yjcl\", path=\"C:\\Users\\bex\\bentoml\\models\\xgb_custom\\hgpjk2iqxk67yjcl\\\")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bentoml.sklearn.save_model(\n",
    "    \"xgb_custom\",\n",
    "    clf,\n",
    "    metadata={\"auc\": avg_test, \"cv_scores\": scores},\n",
    "    labels={\"author\": \"Bex\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f068c16-1f9a-4f69-aab9-d7ff0a3fd6f2",
   "metadata": {},
   "source": [
    "## Sharing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00d910-f5ad-4a55-bca0-271ff2e779b9",
   "metadata": {},
   "source": [
    "Models in the BentoML model store can be shared as standalone archives using the `bentoml models export` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1352483-1dd9-4339-9e3e-ae12d1e3480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(tag=\"xgb_custom:hgpjk2iqxk67yjcl\") exported to C:\\Users\\bex\\Desktop\\articles\\2022\\7_july\\3_bentoml_xgboost\\models\\xgb_custom-hgpjk2iqxk67yjcl.bentomodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bex\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (5.0.0)/charset_normalizer (2.0.6) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "!bentoml models export xgb_custom:latest ./models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b136e3-e606-4f58-b26c-d9e15e2c79f7",
   "metadata": {},
   "source": [
    "When you don't know the exact version string of your tag, you can use the \":latest\" suffix to choose the most recent. With the above command, we are exporting the classifier into a `.bentomodel` archive to the models directory. When a teammate sends you a `.bentomodel` archive, you can use the `import` command to send it to your local BentoML model store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfa2f89e-44fb-49ed-8145-b966972497ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bex\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (5.0.0)/charset_normalizer (2.0.6) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "Error: [models] `import` failed: Item 'xgb_custom:hgpjk2iqxk67yjcl' already exists in the store <osfs 'C:\\Users\\bex\\bentoml\\models'>\n"
     ]
    }
   ],
   "source": [
    "!bentoml models import ./models/xgb_custom-hgpjk2iqxk67yjcl.bentomodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28489f8-8a99-461a-92d9-02408c18cceb",
   "metadata": {},
   "source": [
    "## Creating an API service script\n",
    "This section explains how to load a saved XGB model into a prediction script and how to create a service function with the ‘@service.api’ decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6e73f-ca45-400c-b8a2-4465981496e0",
   "metadata": {},
   "source": [
    "## Building a Bento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4968000f-d98d-4191-9ca4-ca0b1bd5a4fe",
   "metadata": {},
   "source": [
    "This section will explain how to use the ‘bentoml build’ command and all the steps required before running it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814390d-c539-4a97-9fc1-354e55abe63c",
   "metadata": {},
   "source": [
    "## Deploying the Bento to AWS Lambda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
