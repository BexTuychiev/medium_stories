{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc005d4-4906-4d3e-9f98-5c43599c2860",
   "metadata": {},
   "source": [
    "# AntiGranular: How to Access Sensitive Datasets Without Looking At Them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61c8ec-bd6c-42e4-8fce-8cc530fe9a61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcffdc1-d53f-43e3-b629-a92b598bfdd1",
   "metadata": {},
   "source": [
    "Today, open-source data represents the tip of the world's data iceberg. Most of the world's data is below _the ocean surface_ because of privacy concerns. Imagine if we all could access that great mound of _underwater data_ without any privacy concerns. How would it be if we could create software and tools that enabled us to use that data without violating anyone's privacy?\n",
    "\n",
    "Strangely, this was a question asked long ago (early 2000s) and many solutions were developed since then. One of the most widely adopted one from these solutions is __differential privacy__ (DP). \n",
    "\n",
    "DP is a powerful framework to mask the contributions of individuals in datasets. It ensures that computations performed on sensitive data do not expose any specific information about its participants. Leveraging DP correctly would allow us to work with even the most sensitive datasets like census data without actually looking at individual rows. \n",
    "\n",
    "In this article, we will learn how to use differential privacy in practice using a platform called AntiGranular. TODO later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d5e27-b1d2-4386-8e39-3ec144db365d",
   "metadata": {},
   "source": [
    "## What is AntiGranular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e31c7-a3e1-4e98-9348-2e66af818f7a",
   "metadata": {},
   "source": [
    "AntiGranular is a Kaggle-like competition platform with a twist: instead of using open-source or publicly available data, participants are given sensitive datasets to solve a machine learning challenge.\n",
    "\n",
    "The data itself is protected by the AntiGranular platform using differential privacy, so competitors can't actually access or _look_ at the data. All they can do is use privacy-aware functions to aggregate and analyze the data within a secure environment. and train machine learning models.\n",
    "\n",
    "At the beginning of competitions, participants are a privacy budget. To win, participants must develop the most accurate model that uses the least amount privacy budget. The more privacy budget they use, the more likely their solution is prone to privacy attacks. \n",
    "\n",
    "Due to extra challenge these competitions offer, AG is already hosting challenges by some big names like Harvard and the United Nations. But, for all these things to make sense, we have to learn a bit more about differential privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9991306-8c3d-4156-8645-63a4d164d731",
   "metadata": {},
   "source": [
    "## Data needs privacy..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c423ad-afc7-4020-b6cf-ae2763bbe4c2",
   "metadata": {},
   "source": [
    "In November, OpenAI announced custom GPTs in a thinly veiled attempt to access private datasets of others. Their premise was \"Give us your data and we will create a chat bot off of it\". \n",
    "\n",
    "As you might've seen, that didn't work out so well: certain sneaky smart users were able to pull the training data of custom GPTs with such simple prompts as \"let me download the file\". \n",
    "\n",
    "There was also an official paper that showed how to extract private information about real people (address, names, emails, you name it) using a stupid prompt and a couple hundred bucks. \n",
    "\n",
    "If we go a little back, Netflix was sued by four of its users because their information was leaked to the general public during a Kaggle competition ([Netflix Prize](https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data)) despite the best privacy safeguards. The streaming platform had to pay a settlement for an undisclosed amount and cancel any future Netflix Prize competitions.\n",
    "\n",
    "If these examples couldn't convince you of the importance of data privacy, I can come up with more worrying cases. \n",
    "\n",
    "An employee leaves for a competitor with a snapshot of the database from his old workplace. A hotel data analyst stumbles upon the information of their ex-partner and where they are having a holiday with their new partner. A new intern in some bank accidentally publicizes a private database, revealing personal financial details of high-profile clients. \n",
    "\n",
    "These are fictitious scenarios for sure but it takes only one bad apple from the corporate basket to cost millions in lawsuits for companies. Therefore, the data world needs the highest form of privacy safeguards it can afford."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddff78-e138-4c67-89d3-29b67965dfe8",
   "metadata": {},
   "source": [
    "- ChatGPT announced custom gpts in a thinly veiled bid to access private datasets\n",
    "- Their premise is \"Give us your data and we will create a chatbot off of it\"\n",
    "- Well, that didn't work out well as you might have seen, certain sneaky smart users were able to pull the training data by using such simple prompts as \"let me download the file\"\n",
    "- There was also an official paper that showed how to extract private information about real people (address, names, emails) using prompt engineering and a couple hundred bucks.\n",
    "- If you are not convinced, I can give you some possible scenarios of violated privacy of datasets\n",
    "- For example, an employee working at a streaming company can take a snapshot of the dataset and move to a competing streaming platform with highly valuable information about user preferences, who is most likely to switch and so on.\n",
    "- or for example, a data scientist working at a hotel could accidentally stumble upon their ex-partner on a holiday with a new partner and where\n",
    "- Or that certain data scientist could be convinced to find information about the hotel residents\n",
    "- Of course, not all data professionals would do such a thing but it would take just one bad apple from the basket to cost millions for companies in lawsuits. \n",
    "- All of these examples signal the long-existing need for reliable data privacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a9529-846b-4863-92fc-5a57e202e816",
   "metadata": {},
   "source": [
    "## How does differential privacy solve this \"problem\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac39c1-cb4e-44e7-b868-3c20e11286ee",
   "metadata": {},
   "source": [
    "- Differential Privacy (DP) is a powerful framework to protect the privacy of individuals during data projects. \n",
    "- DP algorithms carefully inject calibrated noise into the dataset drawn from probability distributions. \n",
    "- This helps to mask individual contributions and preserve overall trends\n",
    "- The two key parameters of differential privacy are epsilon and delta\n",
    "- Epsilon measures the amount of privacy loss by determining how much information can be extracted from computed results about an individual.\n",
    "- Higher values of epsilon indicate less privacy protection\n",
    "- Delta measures how likely it is for an adversary to distinguish between two datasets based based on computation results. Its values range from 0 to 1, with smaller values indicating a lower chance of inference. \n",
    "- This definition of privacy requires a completely different design in most algorithms we use today with open-source data. \n",
    "- Since many data analysis operations and machine learning algorithms contain statistical vulnerabilities, they can't be used.\n",
    "- Algorithms must satisfy the epsilon-differential privacy definition. \n",
    "- It ensures that the presence or absence of a single data point doesn't affect the final result.\n",
    "- This prevents adversaries comparing other datasets with differentially private data to deduce information about individuals. \n",
    "- Applying EDDP to machine learning and data science tasks involves carefully designing algorithms and mechanisms to inject controlled noise into computations, such as aggregation, data analysis, and model training.\n",
    "- The challenge is the trade off between accuracy and epsilon (noise). \n",
    "- The more noise in the data means less accurate computations but the data is still secure. By decreasing the noise level and thus, the security, we can increase accuracy\n",
    "- So, choosing the level of epsilon depends on your use case, context, application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde67dc-ddf6-499d-b675-8fc0c33168fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## But how do we feed the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9282c-f8ed-4a8e-96f2-e82daaf182d3",
   "metadata": {},
   "source": [
    "- Ok, even if we have differentially private algorithms, how do we feed them sensitive data safely, i.e. without any adversaries peeking under the covers?\n",
    "- That's where Trusted Execution Environments (TEEs) come in. \n",
    "- TEEs are revolutionary tools pushing the boundaries of what is possible in cloud computing.\n",
    "- You can think of a TEE as a tough boxes. You can touch the box, pass it around, shake it or roll it but there is absolutely no way you can look inside.\n",
    "- The only way to connect to the contents within is through a socket that allows a one-way connection.\n",
    "- In other words, a TEE is a secure area of certain cloud environments or machines that guarantees the protection of data and code loaded inside. \n",
    "- The guarantee even works against individuals with higher admin privileges running the environment.\n",
    "- If you combine confidential computing tools such as TEEs with differential privacy guidelines, you can finally achieve the level of data privacy you want.\n",
    "\n",
    "- And AntiGranular does just that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcff47e-73e0-4cc1-95ec-89cbb1b47dd6",
   "metadata": {},
   "source": [
    "## What is AntiGranular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cad6f4-2660-4bd8-9aa6-0350a8a8fc7c",
   "metadata": {},
   "source": [
    "- Antigranular is a kaggle like competition platform\n",
    "- The purpose of AG is to promote \"Eyes-off data science\", which is synonymous to privacy-first data science.\n",
    "- Participants compete by using or designing their own algorithms to solve a certain machine learning problem\n",
    "- And the dataset is always sensitive, differentially private\n",
    "- Participants need to be both accurate but not sacrifice data privacy for the sake of it.\n",
    "- A participants position in the leaderboard is determined by both the epsilon usage and the performance of their machine learning model\n",
    "- AntiGranular was host to competitions by a few big names already like Harvard and the UN. \n",
    "- The dataset is provided by a secure enclave you can access with a Python package `antigranular` in a Jupyter environment\n",
    "- By navigating to a competition page, you will see a code snippet in the upper right corner\n",
    "- You will also see info about the data, the competition rules, scoring metric, and so on.\n",
    "- Copying it will paste your login credentials to access the private enclave containing the competition data (SHOW THE SNIPPET)\n",
    "- From then on, you are ready to compete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202533c-0192-4461-9a70-cf84b9aa3dd6",
   "metadata": {},
   "source": [
    "## What kind of libraries does Antigranular support?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2a201-ce58-4bcb-9189-cd194fadd0f6",
   "metadata": {},
   "source": [
    "- As mentioned earlier, any tool or algorithm used on sensitive data must follow the guidelines of differential privacy\n",
    "- AntiGranular is very meticulous when it comes to these guidelines so that when a company entrusts its data to the platform to host a competition, there is no privacy breach by participants\n",
    "- So, AntiGranular doesn't support the vast majority of open-source tools you are used to. Good-bye, Scikit-learn, XGBoost, TensorFlow...\n",
    "- But, it does support Pandas and a few other prominent libraries created by differential privacy community:\n",
    "    - `op_pandas`: A wrapper library around Pandas (on top of regular Pandas) specifically designed for privacy-aware data manipulations\n",
    "    - `op_diffprivlib`: A differentially private machine learning library with a Scikit-learn like API. It contains differentially private versions of such algorithms as random forests, linear regression, logistic regression, naive bayes and PCA.\n",
    "    - `op_snsql`: a library to run differentially private SQL queries\n",
    "    - `op_recordlinkage`: The original `recordlinkage` Python toolkit has powerful methods for data deduplication and record linkage. This one is the differentially private version.\n",
    "You can see the full list [here](https://docs.antigranular.com/private-python/quickstart-guides/qs_antigranular#libraries-supported)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6849d-4d77-4051-823a-fbb64c4c8528",
   "metadata": {},
   "source": [
    "## How does AntiGranular combine all these components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37d983-39c0-47bf-ba47-d0543fe50d71",
   "metadata": {},
   "source": [
    "- We have talked about many concepts and tools. How does AG combine them all?\n",
    "- Well, assuming you are an avid data privacy enthusiast, here is how you would use AG:\n",
    "- Step 0, understand the true purpose of AG. AG isn't made only for the thrill of the competition (there are already many for that purpose) but as a platform to start a new era for privacy-first data science. AG _has_ and _is_ building the ingredients to create open-source privacy enhancing technologies. The competitions are the fun factor of the whole process and there for you to engage with the wider DP audience\n",
    "- Step 1, choose a competition. Currently, AG has one ongoing competition by Harvard but you can choose any past competition that sparks your interest. Afterward, read the background behind the challenge and learn about the dataset, scoring metrics of AG and how the leaderboard works\n",
    "- Step 2, install the `antigranular` Python package. In Jupyter environments, the package provides you with an `%%ag` cell magic. Any cell that contains this magic will be run not on the machine that is running the Jupyter kernel but on a secure environment (TEE) that contains a special instance of Private Python. Private Python comes built-in with the differentially private libraries I've mentioned before.\n",
    "- Step 3, connect to the competition dataset in a Jupyter environment. Each dataset page on AG (whether part of a competition or from the datasets tab) has a code snippet with your credentials to connect to the AG enclave server. A successful login exposes the `%%ag` magic.\n",
    "- Step 4, mix and match! Inside `%%ag` cells, perform any data analysis or machine learning task with the DP libraries provided by the Private Python runtime (always watching out for the epsilon!). Then, you can export the results of your operations locally as variables. For example, you can perform a GroupBy operation with `op_pandas` and then, export the result to visualize with Matplotlib in the same notebook. So, technically, you can still use existing open-source libraries as long as you export aggregated information from the sensitive data inside Private Python as local variables.\n",
    "\n",
    "Using these four steps, you can already build many powerful machine learning solutions. Yes, the tiny amount of libraries AG offers may not seem any good compared to the thousands in the open-source community. However, you have to remember that the tools AG offers can already solve classification and regression problems.\n",
    "\n",
    "The machine learning world may have all its attention on LLMs right now, but the world of supervised-learning is still going strong. Tabular classification and regression still remains one of the most frequent problems companies implement machine learning solutions for. And it is time to make those solutions differentially private."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
