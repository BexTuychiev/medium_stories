{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc005d4-4906-4d3e-9f98-5c43599c2860",
   "metadata": {},
   "source": [
    "# AntiGranular: How to Access Sensitive Datasets Without Looking At Them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61c8ec-bd6c-42e4-8fce-8cc530fe9a61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcffdc1-d53f-43e3-b629-a92b598bfdd1",
   "metadata": {},
   "source": [
    "Today, open-source data represents the tip of the world's data iceberg. Most of the world's data is below _the ocean surface_ because of privacy concerns. Imagine if we all could access that great mound of _underwater data_ without any privacy concerns. How would it be if we could create software and tools that enabled us to use that data without violating anyone's privacy?\n",
    "\n",
    "Strangely, this was a question asked long ago (early 2000s) and many solutions were developed since then. One of the most widely adopted one from these solutions is __differential privacy__ (DP). \n",
    "\n",
    "DP is a powerful framework to mask the contributions of individuals in datasets. It ensures that computations performed on sensitive data do not expose any specific information about its participants. Leveraging DP correctly would allow us to work with even the most sensitive datasets like census data without actually looking at individual rows. \n",
    "\n",
    "In this article, we will learn how to use differential privacy in practice using a platform called AntiGranular. TODO later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d5e27-b1d2-4386-8e39-3ec144db365d",
   "metadata": {},
   "source": [
    "## What is AntiGranular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e31c7-a3e1-4e98-9348-2e66af818f7a",
   "metadata": {},
   "source": [
    "AntiGranular is a Kaggle-like competition platform with a twist: instead of using open-source or publicly available data, participants are given sensitive datasets to solve a machine learning challenge.\n",
    "\n",
    "The data itself is protected by the AntiGranular platform using differential privacy, so competitors can't actually access or _look_ at the data. All they can do is use privacy-aware functions to aggregate and analyze the data within a secure environment. and train machine learning models.\n",
    "\n",
    "At the beginning of competitions, participants are a privacy budget. To win, participants must develop the most accurate model that uses the least amount privacy budget. The more privacy budget they use, the more likely their solution is prone to privacy attacks. \n",
    "\n",
    "Due to extra challenge these competitions offer, AG is already hosting challenges by some big names like Harvard and the United Nations. But, for all these things to make sense, we have to learn a bit more about differential privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9991306-8c3d-4156-8645-63a4d164d731",
   "metadata": {},
   "source": [
    "## Data needs privacy..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c423ad-afc7-4020-b6cf-ae2763bbe4c2",
   "metadata": {},
   "source": [
    "In November, OpenAI announced custom GPTs in a thinly veiled attempt to access private datasets of others. Their premise was \"Give us your data and we will create a chat bot off of it\". \n",
    "\n",
    "As you might've seen, that didn't work out so well: certain sneaky smart users were able to pull the training data of custom GPTs with such simple prompts as \"let me download the file\". \n",
    "\n",
    "There was also an official paper that showed how to extract private information about real people (address, names, emails, you name it) using a stupid prompt and a couple hundred bucks. \n",
    "\n",
    "If we go a little back, Netflix was sued by four of its users because their information was leaked to the general public during a Kaggle competition ([Netflix Prize](https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data)) despite the best privacy safeguards. The streaming platform had to pay a settlement for an undisclosed amount and cancel any future Netflix Prize competitions.\n",
    "\n",
    "If these examples couldn't convince you of the importance of data privacy, I can come up with more worrying cases. \n",
    "\n",
    "An employee leaves for a competitor with a snapshot of the database from his old workplace. A hotel data analyst stumbles upon the information of their ex-partner and where they are having a holiday with their new partner. A new intern in some bank accidentally publicizes a private database, revealing personal financial details of high-profile clients. \n",
    "\n",
    "These are fictitious scenarios for sure but it takes only one bad apple from the corporate basket to cost millions in lawsuits for companies. Therefore, the data world needs the highest form of privacy safeguards it can afford."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a9529-846b-4863-92fc-5a57e202e816",
   "metadata": {},
   "source": [
    "## How does differential privacy solve this \"problem\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ca120-1eed-436f-aca5-a425dc67be08",
   "metadata": {},
   "source": [
    "Differential privacy (DP) is a robust framework to protect the privacy of individuals during data projects. DP algorithms draw calibrated noise from probability distributions and inject them into datasets. This helps mask individual contributions and preserve overall trends. \n",
    "\n",
    "The two key parameters of DP framework is __epsilon__ and __delta__, which quantify the level of protection offered by an algorithm. \n",
    "\n",
    "1. __Epsilon (ε)__:\n",
    "    - Epsilon is a parameter that measures the privacy budget or the level of privacy protection offered by a differentially private algorithm.\n",
    "    - A smaller value of epsilon indicates a stronger guarantee of privacy. A low epsilon value means that the chance of revealing sensitive information about any individual from the output of the algorithm is minimized.\n",
    "    - A higher epsilon value allows for more flexibility in data analysis but might lead to less stronger privacy protection.\n",
    "    - Mathematically, a smaller value of epsilon corresponds to a tighter bound on the amount of information that any single individual's data can contribute to the output of the algorithm.\n",
    "2. __Delta (δ)__:\n",
    "    - Delta is an additional parameter used in some DP definitions, specifically when dealing with algorithms that have a small probability of deviating from perfect privacy.\n",
    "    - A smaller delta value implies a lower probability of privacy breach by adversaries.\n",
    "    - A higher delta value indicates otherwise. \n",
    "    - Typically, algorithms are designed to have an extremely small delta to ensure that the probability of any privacy breach remains , especially in sensitive or high-stakes scenarios.\n",
    "\n",
    "Combined, these parameters allow for a trade-off between data utility (usefulness of data analysis) and privacy preservation. Make these parameters too conservative and you may end up with a dataset filled with noise. Let them breathe a little and you may loosen the level of protection on your sensitive dataset. It is important to strike this balance based on your unique scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde67dc-ddf6-499d-b675-8fc0c33168fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## But how do we feed the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe290c5-5251-44d3-9032-9b94037022e6",
   "metadata": {},
   "source": [
    "OK, even if we have algorithms that strictly follow the Epsilon-Delta definition of differential privacy, how do we feed them our sensitive data without any third parties or even the dataset owners themselves peeking under the covers?\n",
    "\n",
    "That's where Trusted Execution Environments (TEEs) come into play. TEEs are revolutionary tools pushing the boundaries of what is possible in cloud computing. \n",
    "\n",
    "You can think of a TEE as a tough box. You can touch the box, pass it around, shake it or roll it but there is absolutely no way you can look inside. In other words, a TEE is a secure cloud environment or sections of machines that guarantees the protection of data and code loaded inside. This guarantee even works against individuals with higher admin privileges running the environment. \n",
    "\n",
    "If you combine confidential computing tools such as TEEs with differential privacy guidelines, you got yourself a privacy policy to brag to others.\n",
    "\n",
    "And AntiGranular lets you do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcff47e-73e0-4cc1-95ec-89cbb1b47dd6",
   "metadata": {},
   "source": [
    "## What is AntiGranular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cad6f4-2660-4bd8-9aa6-0350a8a8fc7c",
   "metadata": {},
   "source": [
    "- Antigranular is a kaggle like competition platform\n",
    "- The purpose of AG is to promote \"Eyes-off data science\", which is synonymous to privacy-first data science.\n",
    "- Participants compete by using or designing their own algorithms to solve a certain machine learning problem\n",
    "- And the dataset is always sensitive, differentially private\n",
    "- Participants need to be both accurate but not sacrifice data privacy for the sake of it.\n",
    "- A participants position in the leaderboard is determined by both the epsilon usage and the performance of their machine learning model\n",
    "- AntiGranular was host to competitions by a few big names already like Harvard and the UN. \n",
    "- The dataset is provided by a secure enclave you can access with a Python package `antigranular` in a Jupyter environment\n",
    "- By navigating to a competition page, you will see a code snippet in the upper right corner\n",
    "- You will also see info about the data, the competition rules, scoring metric, and so on.\n",
    "- Copying it will paste your login credentials to access the private enclave containing the competition data (SHOW THE SNIPPET)\n",
    "- From then on, you are ready to compete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202533c-0192-4461-9a70-cf84b9aa3dd6",
   "metadata": {},
   "source": [
    "## What kind of libraries does Antigranular support?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2a201-ce58-4bcb-9189-cd194fadd0f6",
   "metadata": {},
   "source": [
    "- As mentioned earlier, any tool or algorithm used on sensitive data must follow the guidelines of differential privacy\n",
    "- AntiGranular is very meticulous when it comes to these guidelines so that when a company entrusts its data to the platform to host a competition, there is no privacy breach by participants\n",
    "- So, AntiGranular doesn't support the vast majority of open-source tools you are used to. Good-bye, Scikit-learn, XGBoost, TensorFlow...\n",
    "- But, it does support Pandas and a few other prominent libraries created by differential privacy community:\n",
    "    - `op_pandas`: A wrapper library around Pandas (on top of regular Pandas) specifically designed for privacy-aware data manipulations\n",
    "    - `op_diffprivlib`: A differentially private machine learning library with a Scikit-learn like API. It contains differentially private versions of such algorithms as random forests, linear regression, logistic regression, naive bayes and PCA.\n",
    "    - `op_snsql`: a library to run differentially private SQL queries\n",
    "    - `op_recordlinkage`: The original `recordlinkage` Python toolkit has powerful methods for data deduplication and record linkage. This one is the differentially private version.\n",
    "You can see the full list [here](https://docs.antigranular.com/private-python/quickstart-guides/qs_antigranular#libraries-supported)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6849d-4d77-4051-823a-fbb64c4c8528",
   "metadata": {},
   "source": [
    "## How does AntiGranular combine all these components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37d983-39c0-47bf-ba47-d0543fe50d71",
   "metadata": {},
   "source": [
    "- We have talked about many concepts and tools. How does AG combine them all?\n",
    "- Well, assuming you are an avid data privacy enthusiast, here is how you would use AG:\n",
    "- Step 0, understand the true purpose of AG. AG isn't made only for the thrill of the competition (there are already many for that purpose) but as a platform to start a new era for privacy-first data science. AG _has_ and _is_ building the ingredients to create open-source privacy enhancing technologies. The competitions are the fun factor of the whole process and there for you to engage with the wider DP audience\n",
    "- Step 1, choose a competition. Currently, AG has one ongoing competition by Harvard but you can choose any past competition that sparks your interest. Afterward, read the background behind the challenge and learn about the dataset, scoring metrics of AG and how the leaderboard works\n",
    "- Step 2, install the `antigranular` Python package. In Jupyter environments, the package provides you with an `%%ag` cell magic. Any cell that contains this magic will be run not on the machine that is running the Jupyter kernel but on a secure environment (TEE) that contains a special instance of Private Python. Private Python comes built-in with the differentially private libraries I've mentioned before.\n",
    "- Step 3, connect to the competition dataset in a Jupyter environment. Each dataset page on AG (whether part of a competition or from the datasets tab) has a code snippet with your credentials to connect to the AG enclave server. A successful login exposes the `%%ag` magic.\n",
    "- Step 4, mix and match! Inside `%%ag` cells, perform any data analysis or machine learning task with the DP libraries provided by the Private Python runtime (always watching out for the epsilon!). Then, you can export the results of your operations locally as variables. For example, you can perform a GroupBy operation with `op_pandas` and then, export the result to visualize with Matplotlib in the same notebook. So, technically, you can still use existing open-source libraries as long as you export aggregated information from the sensitive data inside Private Python as local variables.\n",
    "\n",
    "Using these four steps, you can already build many powerful machine learning solutions. Yes, the tiny amount of libraries AG offers may not seem any good compared to the thousands in the open-source community. However, you have to remember that the tools AG offers can already solve classification and regression problems.\n",
    "\n",
    "The machine learning world may have all its attention on LLMs right now, but the world of supervised-learning is still going strong. Tabular classification and regression still remains one of the most frequent problems companies implement machine learning solutions for. And it is time to make those solutions differentially private."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
