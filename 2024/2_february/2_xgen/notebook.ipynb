{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7be5e5-5650-4be3-aa2d-052ca474e15a",
   "metadata": {},
   "source": [
    "# Salesforce XGen tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e762207-179c-4acd-8926-0e6b93bd63ff",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad0f74-c539-4a8a-adcd-d66c56bf0ca6",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "2. XGen 7B key features and capabilities\n",
    "3. Applications of XGen 7B\n",
    "4. Prerequisites and installation\n",
    "5. Working with XGen 7B\n",
    "6. Maximizing model performance\n",
    "7. Building a summarizer model\n",
    "8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214617a3-eaf2-4700-a65d-8a97d0ac4694",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e07cc-804a-4a3c-b877-24402943f87e",
   "metadata": {},
   "source": [
    "Right now, these six are the hottest open-source LLMs:\n",
    "\n",
    "1. LLaMA2\n",
    "2. BLOOM\n",
    "3. Falcon 180B\n",
    "4. OPT-175B\n",
    "5. GPT-Neox\n",
    "6. Vicuna 13-B\n",
    "\n",
    "And they all have the same disadvantage - very short context length, reaching up to only 2048 tokens. Compared to proprietary models like GPT-3.5 and GPT-4 that offer lengths up to 32k tokens (50 pages of text!), it seems open-source LLMs are at a heavy disadvantage. \n",
    "\n",
    "Context length is essentially the \"memory\" of LLMs. 2048-token context window means the model can only remember 2048 tokens of the conversation at a time. This significantly affects performance in tasks where a large context is crucial such as summarization, translation, code generation, etc. \n",
    "\n",
    "To address this critical issue, Salesforce announced its XGen-7B model with a whopping context length of 8k tokens (4 times longer than other LLMs). This article covers the key characteristics of the model and show to build a text summarizer model using it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb18608-ebc9-4434-92c2-d4aeedc4e074",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
