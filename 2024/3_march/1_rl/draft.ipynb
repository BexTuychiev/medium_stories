{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd094490-34ea-4c59-8ef8-7ff6302ef166",
   "metadata": {},
   "source": [
    "# Learn Reinforcement Learning in Python: Step-by-step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd986e42-658a-4f4f-8607-77843cf41134",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0c943-0bed-44b1-90b7-6e341a3654cf",
   "metadata": {},
   "source": [
    "```\n",
    "$ pip install \"gymnasium[atari]\"\n",
    "$ pip install autorom[accept-rom-license]\n",
    "$ AutoROM --accept-license\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4529fe-6e79-43cb-8205-b86faae83f93",
   "metadata": {},
   "source": [
    "### Exploring the Gymnasium environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "707d4f99-6747-4f3d-a614-6683246a846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59e3eff8-0271-41ad-9a22-e6857b1ce45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.29.1'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6cd69adf-25b0-44cb-b61d-2900195bd22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ec9a253-0380-4f37-a527-b8fbc8f76c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55d1b1cd-406e-4326-8339-1311e825630b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73ba4a2f-ce8e-4d8e-9075-95894380838c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.observation_space.sample()\n",
    "\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8faff674-007b-4107-92ab-ba420f2bdc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[0, 0],\n",
       "        [0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "\n",
    "state[:2, :2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c7cd035-3fca-487a-8a12-f20356d9ccf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 5, 'episode_frame_number': 0, 'frame_number': 0}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942aa44-d6b2-4572-860b-a4fee4c4aa32",
   "metadata": {},
   "source": [
    "### State, action, reward workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0dce3d5d-e99d-42de-9b02-60aae5e6edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f6cf29e5-1304-457c-889e-b480821f06b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 5, 'episode_frame_number': 4, 'frame_number': 4}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9057c91-ec36-49ce-a5d0-b94f0511e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = (\n",
    "        env.action_space.sample()\n",
    "    )  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455ca46-9bf6-4121-8f2a-091d6c3e9734",
   "metadata": {},
   "source": [
    "### Animating the agent's learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a9d99-f07e-4a33-b3ae-ae779a47c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append(\n",
    "        {\n",
    "            \"frame\": env.render(),\n",
    "            \"state\": observation,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    if epochs == 1000:\n",
    "        break\n",
    "\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8ae6034-c9bf-4235-88f3-16b941b53e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/bexgboost/.local/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1000\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "\n",
    "frames = []  # for animation\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append(\n",
    "        {\n",
    "            \"frame\": env.render(),\n",
    "            \"state\": observation,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    if epochs == 1000:\n",
    "        break\n",
    "\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc79d2-b437-460b-8365-e7008a090c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06fb61c0-512c-4579-bdd9-8cb3b39c79db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file animation.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "\n",
    "def create_gif(frames: dict, filename, fps=100):\n",
    "    \"\"\"\n",
    "    Creates a GIF animation from a list of RGBA NumPy arrays.\n",
    "\n",
    "    Args:\n",
    "        frames: A list of RGBA NumPy arrays representing the animation frames.\n",
    "        filename: The output filename for the GIF animation.\n",
    "        fps: The frames per second of the animation (default: 10).\n",
    "    \"\"\"\n",
    "    rgba_frames = [frame[\"frame\"] for frame in frames]\n",
    "\n",
    "    clip = ImageSequenceClip(rgba_frames, fps=fps)\n",
    "    clip.write_gif(filename, fps=fps)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "create_gif(frames, \"animation.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a818db-578c-49cc-ab97-b19dbcc59cf5",
   "metadata": {},
   "source": [
    "### Creating a Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a57377ce-963d-4c63-99a4-164f11bd9cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a81a4d41-dde8-4052-8be3-43833cd95fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f11116cd-5428-48bd-b95a-cefbf444503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "q_table = np.zeros([n_states, n_actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b42f19f-2e48-4f8b-b0bb-4acce7f6bc69",
   "metadata": {},
   "source": [
    "### Running a single episode of Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "162246b8-8bf1-47b7-a492-9406386be9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qlearning_episode(env, qtable, epsilon=0.1, alpha=0.1, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Runs a single episode of Q-learning on the provided environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env :\n",
    "        The environment to interact with.\n",
    "    qtable : np.ndarray\n",
    "        The Q-table to learn and update, with shape (n_states, n_actions).\n",
    "    epsilon : float, optional\n",
    "        The exploration rate (default: 0.1).\n",
    "    alpha : float, optional\n",
    "        The learning rate (default: 0.1).\n",
    "    gamma : float, optional\n",
    "        The discount factor (default: 0.1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    # Define general variables\n",
    "    epochs, reward = 0, 0\n",
    "    done = False\n",
    "\n",
    "    # Reset the environment\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # Start training\n",
    "    while not done:\n",
    "        # Should the agent explore or exploit?\n",
    "        if np.random.uniform() <= epsilon:\n",
    "            # Explore the action space if randomness below epsilon\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Perform the action with the highest q-value\n",
    "            action = np.argmax(q_table[observation])\n",
    "\n",
    "        # Take action\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1811349e-93be-4e6f-ba73-d40f5b437c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8),\n",
       " {'lives': 5, 'episode_frame_number': 0, 'frame_number': 962})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "31b929be-b829-41c0-9e55-b75570aea23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake\", is_slippery=True)\n",
    "num_episodes = 1000\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "num_states, num_actions = env.observation_space.n, env.action_space.n\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "reward_per_random_episode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7bd24373-f239-48fd-995b-d04a21f60f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state, action, reward, new_state):\n",
    "    old_value = Q[state, action]\n",
    "    next_max = max(Q[new_state])\n",
    "    Q[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b7b41ff1-b328-4e8c-a369-c8083129d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        # Random action selection\n",
    "        action = env.action_space.sample()\n",
    "        # Take action and observe new state and reward\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # Update Q-table\n",
    "        update_q_table(state, action, reward, new_state)\n",
    "        episode_reward += reward\n",
    "        state = new_state\n",
    "        reward_per_random_episode.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab70574-161a-4789-9853-76f41fa90a24",
   "metadata": {},
   "source": [
    "### Article plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86b20b-ee1e-4baf-aaaa-727ae28b4601",
   "metadata": {},
   "source": [
    "1. Define what reward, state and actions are for the current problem\n",
    "2. Show how to install gymnasium with cmake and scipy\n",
    "3. Show how to render the env in both human and rgb_array mode\n",
    "4. Explain env.reset, step and render methods\n",
    "5. Pseudo-code for solving the environment without RL:\n",
    "   - Initialize epochs, penalties, reward and an empty list to store frames\n",
    "   - Define the `done` variable\n",
    "   - While not done, get a random action and execute with step\n",
    "   - Increase or decrease the penalty based on the reward\n",
    "   - Append the current frame to frames using rgb_array mode of render\n",
    "   - Increase the number of epochs\n",
    "6. Pseudo-code to display the frames as a GIF\n",
    "   - Using imageio, collect all rgb-arrays in frames and put them together as a gif\n",
    "7. Pseudo-code to solve the environment with Q-learning\n",
    "   - Define the hyperparameters - alpha, gamma, epsilon\n",
    "   - Define the q_table with the same dims as the number of states and the number of actions\n",
    "   - For a large number of epochs:\n",
    "     - Reset the environment\n",
    "     - Initialize epochs, penalties and reward with 0 values\n",
    "     - While not done:\n",
    "       - Generate a random value to compare with epsilon - exp vs. exploit trade-off\n",
    "       - if random value smaller than epsilon, choose a new random action, else, find the argmax of q_table for the state - i.e. choose the action that gives the biggest reward\n",
    "       - Take the action\n",
    "       - Find the old value for the current state and action\n",
    "       - Find the max value for the next state\n",
    "       - Create a new value using the Q-learning formula\n",
    "       - Update the q_table for the current state and action with new_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5daf7df-be23-4a33-941f-31e9949da8bb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
