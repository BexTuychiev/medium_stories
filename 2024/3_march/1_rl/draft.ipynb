{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd094490-34ea-4c59-8ef8-7ff6302ef166",
   "metadata": {},
   "source": [
    "# Learn Reinforcement Learning in Python: Step-by-step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0c943-0bed-44b1-90b7-6e341a3654cf",
   "metadata": {},
   "source": [
    "```\n",
    "$ pip install \"gymnasium[atari]\"\n",
    "$ pip install autorom[accept-rom-license]\n",
    "$ AutoROM --accept-license\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707d4f99-6747-4f3d-a614-6683246a846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd69adf-25b0-44cb-b61d-2900195bd22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec9a253-0380-4f37-a527-b8fbc8f76c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d1b1cd-406e-4326-8339-1311e825630b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73ba4a2f-ce8e-4d8e-9075-95894380838c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.observation_space.sample()\n",
    "\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8faff674-007b-4107-92ab-ba420f2bdc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "\n",
    "state[:3, :3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c7cd035-3fca-487a-8a12-f20356d9ccf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 5, 'episode_frame_number': 0, 'frame_number': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dce3d5d-e99d-42de-9b02-60aae5e6edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6cf29e5-1304-457c-889e-b480821f06b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 5, 'episode_frame_number': 12, 'frame_number': 12}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9057c91-ec36-49ce-a5d0-b94f0511e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = (\n",
    "        env.action_space.sample()\n",
    "    )  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48c7122e-447f-4211-9b16-0da6f913cbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Renders the environment with `kwargs`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8ae6034-c9bf-4235-88f3-16b941b53e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1\n",
      "Penalties incurred: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bexgboost/.local/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = []  # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append(\n",
    "        {\n",
    "            \"frame\": env.render(),\n",
    "            \"state\": observation,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e5af754-c7b7-4d34-acd7-f9b07dbaaf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'frame': array([[[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]]], dtype=uint8),\n",
       "  'state': array([[[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]]], dtype=uint8),\n",
       "  'action': 3,\n",
       "  'reward': 0.0}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137e93f-3ac8-47ab-99b4-556e6c8b6195",
   "metadata": {},
   "source": [
    "env.reset - to start an episode\n",
    "done=False - to see if game is terminated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab70574-161a-4789-9853-76f41fa90a24",
   "metadata": {},
   "source": [
    "### Article plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86b20b-ee1e-4baf-aaaa-727ae28b4601",
   "metadata": {},
   "source": [
    "1. Define what reward, state and actions are for the current problem\n",
    "2. Show how to install gymnasium with cmake and scipy\n",
    "3. Show how to render the env in both human and rgb_array mode\n",
    "4. Explain env.reset, step and render methods\n",
    "5. Pseudo-code for solving the environment without RL:\n",
    "   - Initialize epochs, penalties, reward and an empty list to store frames\n",
    "   - Define the `done` variable\n",
    "   - While not done, get a random action and execute with step\n",
    "   - Increase or decrease the penalty based on the reward\n",
    "   - Append the current frame to frames using rgb_array mode of render\n",
    "   - Increase the number of epochs\n",
    "6. Pseudo-code to display the frames as a GIF\n",
    "   - Using imageio, collect all rgb-arrays in frames and put them together as a gif\n",
    "7. Pseudo-code to solve the environment with Q-learning\n",
    "   - Define the hyperparameters - alpha, gamma, epsilon\n",
    "   - Define the q_table with the same dims as the number of states and the number of actions\n",
    "   - For a large number of epochs:\n",
    "     - Reset the environment\n",
    "     - Initialize epochs, penalties and reward with 0 values\n",
    "     - While not done:\n",
    "       - Generate a random value to compare with epsilon - exp vs. exploit trade-off\n",
    "       - if random value smaller than epsilon, choose a new random action, else, find the argmax of q_table for the state - i.e. choose the action that gives the biggest reward\n",
    "       - Take the action\n",
    "       - Find the old value for the current state and action\n",
    "       - Find the max value for the next state\n",
    "       - Create a new value using the Q-learning formula\n",
    "       - Update the q_table for the current state and action with new_value\n",
    "       - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5daf7df-be23-4a33-941f-31e9949da8bb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
