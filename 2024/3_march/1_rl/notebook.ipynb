{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd094490-34ea-4c59-8ef8-7ff6302ef166",
   "metadata": {},
   "source": [
    "# Learn Reinforcement Learning in Python: Step-by-step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d459b7-1e67-4c04-9870-1d6858cd5182",
   "metadata": {},
   "source": [
    "## Why learn Reinforcement Learning (RL)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03064c97-4a58-4a6b-b0c4-2bb3b098c4a8",
   "metadata": {},
   "source": [
    "To me, the most basic reinforcement learning model resembles science-fiction AI more than any large language model of today. Just take a look at how an RL agent is playing (and finishing) an insanely difficult level of Super Mario:\n",
    "\n",
    "![](images/super_mario.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1d17d-fa52-4168-b487-4a048916db34",
   "metadata": {},
   "source": [
    "In the beginning, this agent has no idea of what the controls are, how to progress through the game, what the obstacles are or what finishes the game. The agent learns all these things without any human intervention - all through the power of reinforcement learning algorithms. \n",
    "\n",
    "RL agents excel in situations where traditional machine learning algorithms struggle. They can solve problems without predefined solutions or explicitly programmed actions and most importantly, without mounds and mounds of data. That's why RL is having significant impact on many fields. For instance, it's used in:\n",
    "\n",
    "- Self-driving cars: RL agents can learn optimal driving strategies based on traffic conditions and road rules.\n",
    "- Robotics: Robots can be trained to perform complex tasks in dynamic environments through RL.\n",
    "- Game playing: AI agents can learn complex strategies in games like Go or StarCraft II using RL techniques.\n",
    "\n",
    "Reinforcement learning is a rapidly evolving field with vast potential. As research progresses, we can expect even more groundbreaking applications in areas like resource management, healthcare, and personalized learning. \n",
    "\n",
    "That's why now is the best time to learn this fascinating field of computer science. This tutorial will help you get started with the fundamental ideas and concepts in RL and introduce how to apply it in practice using Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550221c9-3f3e-49c8-8ddb-205e0da6f368",
   "metadata": {},
   "source": [
    "## 1. Agent and environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec655b-e39f-4c4d-96ad-f05aa4ea19e9",
   "metadata": {},
   "source": [
    "Imagine you just got your cat, Bob, a fancy new scratching post. You want Bob to learn to use it instead of clawing up your furniture. This situation is a great way to understand the basics of reinforcement learning (RL), a type of AI where an agent learns from trial and error.\n",
    "\n",
    "Bob, the curious cat, is the **agent** in this RL scenario. The agent is the learner and decision-maker. In fancier terms, it's the one who interacts with the world and figures things out. Bob needs to learn which things are okay to scratch (the post) and which are not (the expensive drapes!).\n",
    "\n",
    "\n",
    "The room where Bob explores his scratching desires is the **environment**. It's everything outside the agent that it can interact with. The environment provides challenges (like that comfy-looking couch) and opportunities (the satisfying-on-the-nails scratching post!). Here, the room has furniture and, of course, the all-important scratching post.\n",
    "\n",
    "There are two main types of environments in RL:\n",
    "\n",
    "* **Discrete Environments:** Imagine a classic video game where the world is like a grid, and Bob can only move up, down, left, or right. These environments have a limited number of options for both Bob (his actions)  and the room (its states, like where Bob and the post are).\n",
    "* **Continuous Environments:** Now picture a super high-tech room where Bob can move in any direction, and maybe even the scratching post can be moved around! This is a continuous environment, with endless possibilities for both Bob and the room.\n",
    "\n",
    "Our current room with furniture is a **static environment**. The furniture doesn't move, and the scratching post stays put. But imagine if the furniture and scratching post magically switched places every few hours! That would be a **dynamic environment**, which is trickier for an agent to learn in because things keep changing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcff0e4-c72b-473b-9331-7d8cfa825c40",
   "metadata": {},
   "source": [
    "## 2. Actions and states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e152329-885b-420d-85ae-05be7617563f",
   "metadata": {},
   "source": [
    "Imagine Bob's world like a giant video game. Everything Bob can see, smell, and hear - the furniture, the scratching post, even the dangling string on your curtains - all this information makes up the **state space**. \n",
    "\n",
    "The size of this state space depends on the environment:\n",
    "\n",
    "* **Discrete Environments:** In classic video games with grids, Bob can only be in a limited number of places (states), like in front of the post or next to the couch. This means the state space, and the information Bob gets, is also limited.\n",
    "* **Continuous Environments:** Now picture a super high-tech room where Bob can be anywhere and even move the scratching post. This creates a **continuous state space** with endless possibilities for Bob to explore.\n",
    "\n",
    "The **action space** is all the things Bob can do in the state space. In our scratching post example, Bob's actions could be scratching the post, napping on the couch (lazy), or even chasing butterflies (distracted kitty).\n",
    "\n",
    "Similar to the state space, the number of actions Bob can take depends on the environment:\n",
    "\n",
    "* **Discrete Environments:** In a grid-world game, Bob might have a limited number of actions, like moving up, down, left, or right.\n",
    "* **Continuous Environments:** In our high-tech room, Bob might have a wider range of actions, like moving in any direction, jumping, or even (hopefully not) chewing on wires.\n",
    "\n",
    "**The Starting Point: state 0**\n",
    "\n",
    "When Bob starts his scratching post adventure, the environment is in a default state, let's call it state 0. In our case, this might be the room with the scratching post all set up.\n",
    "\n",
    "Everything gets interesting when Bob takes an action. Walking towards the post, napping on the couch, scratching furniture, or chasing butterflies - each action changes the environment and moves Bob to a new state.\n",
    "\n",
    "So, Bob scratches the post (action) - this makes the environment change (new state)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa992b-d131-49fb-a7a1-f3bd3d9bfe06",
   "metadata": {},
   "source": [
    "## 3. Time step and rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598eca34-aeaf-4e8b-82c7-4a611fb5edb6",
   "metadata": {},
   "source": [
    "Imagine Bob's scratching post adventure like a movie. This movie isn't shown all at once, but rather in short snippets called **time steps**. Each time step is like a single frame in the movie, capturing a snapshot of what Bob is doing (action) and what the environment is like (state).\n",
    "\n",
    "These time steps help us understand the flow of events. We can see how Bob's actions in one time step (scratching something) affect the environment (maybe some satisfying furniture shredding) and lead to a new state in the next time step. The number of time steps can vary depending on the situation. Maybe Bob learns the joy of scratching posts quickly, or maybe it takes a while. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a6249-6ca7-4256-b908-c6bc85f4d711",
   "metadata": {},
   "source": [
    "Learning is all about getting good at something, and reinforcement learning is no different. But how does Bob know if he's doing a good job scratching that post (good kitty) or a bad job clawing the couch (naughty kitty)? This is where **rewards** come in.\n",
    "\n",
    "Rewards are like little treats Bob gets for taking the right actions. In our example, when Bob scratches the post (action), he might receive a positive reward. This reward could be a feeling of accomplishment, a yummy treat, or even just your pat on the head (good kitty!).\n",
    "\n",
    "On the other hand, if Bob scratches the couch, he might not get a reward, or he might even get a negative reward (like a water squirt in the face). These rewards help Bob learn which actions are good and which ones to avoid.\n",
    "\n",
    "By understanding time steps and rewards, we can see the big picture of Bob's learning process. Each time step captures Bob's action, the state of the environment, and the reward he receives. Over many time steps, Bob learns through trial and error, figuring out which actions lead to the most rewards (and hopefully, fewer scratches on your furniture!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6d7e9-8f13-4803-8900-a53bb349c563",
   "metadata": {},
   "source": [
    "## 4. Exploration vs. exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691e39f-f2ed-4c94-91cd-2544d0b665da",
   "metadata": {},
   "source": [
    "Bob's on his way to becoming a scratching post pro! But there's one more challenge to overcome: the exploration-exploitation dilemma. Let's break it down.\n",
    "\n",
    "Imagine Bob is finally getting the hang of using the scratching post. It feels good and it gets treats, it's a win-win! But what if there's an even better scratching spot he hasn't discovered yet, like the one with sleeping shelves? This is the essence of the exploration-exploitation dilemma.\n",
    "\n",
    "* **Exploration:** This is like Bob venturing out, trying new things (like scratching the curtains) and seeing what happens (water in the face or a fish treat). It helps him discover potentially better options in the environment.\n",
    "* **Exploitation:** This is like Bob sticking to what works - the scratching post! It guarantees a reward (praise and satisfaction) he already knows about.\n",
    "\n",
    "The challenge is finding a balance. Exploring too much might waste time, especially in continuous environments, while exploiting too much might make Bob miss out on something even better.\n",
    "\n",
    "There are a few tricks to help Bob explore smartly:\n",
    "\n",
    "* **Epsilon-greedy:** Imagine Bob flips a coin (or has some internal feline process) before taking an action. With a small chance (epsilon), he'll explore and try something new. But most of the time (1-epsilon), he'll exploit and go for the reliable scratching post.\n",
    "\n",
    "* **Boltzmann Exploration:** This strategy is like Bob getting more likely to explore when things are going poorly (consecutive negative rewards for scratching various things). As he gets more rewards from the post, he becomes more likely to exploit that winning strategy.\n",
    "\n",
    "By using these strategies (and many others we haven't explained), Bob can find a balance between exploring the unknown and sticking to the good stuff. Over time, he'll become a scratching post master, with minimal furniture casualties."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
