{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd094490-34ea-4c59-8ef8-7ff6302ef166",
   "metadata": {},
   "source": [
    "# Learn Reinforcement Learning in Python: Step-by-step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d459b7-1e67-4c04-9870-1d6858cd5182",
   "metadata": {},
   "source": [
    "## Why learn Reinforcement Learning (RL)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03064c97-4a58-4a6b-b0c4-2bb3b098c4a8",
   "metadata": {},
   "source": [
    "To me, the most basic reinforcement learning model resembles science-fiction AI more than any large language model of today. Just take a look at how an RL agent is playing (and finishing) an insanely difficult level of Super Mario:\n",
    "\n",
    "![](images/super_mario.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1d17d-fa52-4168-b487-4a048916db34",
   "metadata": {},
   "source": [
    "In the beginning, this agent has no idea of what the controls are, how to progress through the game, what the obstacles are or what finishes the game. The agent learns all these things without any human intervention - all through the power of reinforcement learning algorithms. \n",
    "\n",
    "RL agents excel in situations where traditional machine learning algorithms struggle. They can solve problems without predefined solutions or explicitly programmed actions and most importantly, without mounds and mounds of data. That's why RL is having significant impact on many fields. For instance, it's used in:\n",
    "\n",
    "- Self-driving cars: RL agents can learn optimal driving strategies based on traffic conditions and road rules.\n",
    "- Robotics: Robots can be trained to perform complex tasks in dynamic environments through RL.\n",
    "- Game playing: AI agents can learn complex strategies in games like Go or StarCraft II using RL techniques.\n",
    "\n",
    "Reinforcement learning is a rapidly evolving field with vast potential. As research progresses, we can expect even more groundbreaking applications in areas like resource management, healthcare, and personalized learning. \n",
    "\n",
    "That's why now is the best time to learn this fascinating field of computer science. This tutorial will help you get started with the fundamental ideas and concepts in RL and introduce how to apply it in practice using Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550221c9-3f3e-49c8-8ddb-205e0da6f368",
   "metadata": {},
   "source": [
    "## 1. Agent and environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec655b-e39f-4c4d-96ad-f05aa4ea19e9",
   "metadata": {},
   "source": [
    "Imagine you just got your cat, Bob, a fancy new scratching post. You want Bob to learn to use it instead of clawing up your furniture. This situation is a great way to understand the basics of reinforcement learning (RL), a type of AI where an agent learns from trial and error.\n",
    "\n",
    "Bob, the curious cat, is the **agent** in this RL scenario. The agent is the learner and decision-maker. In fancier terms, it's the one who interacts with the world and figures things out. Bob needs to learn which things are okay to scratch (the post) and which are not (the expensive drapes!).\n",
    "\n",
    "\n",
    "The room where Bob explores his scratching desires is the **environment**. It's everything outside the agent that it can interact with. The environment provides challenges (like that comfy-looking couch) and opportunities (the satisfying-on-the-nails scratching post!). Here, the room has furniture and, of course, the all-important scratching post.\n",
    "\n",
    "There are two main types of environments in RL:\n",
    "\n",
    "* **Discrete Environments:** Imagine a classic video game where the world is like a grid, and Bob can only move up, down, left, or right. These environments have a limited number of options for both Bob (his actions)  and the room (its states, like where Bob and the post are).\n",
    "* **Continuous Environments:** Now picture a super high-tech room where Bob can move in any direction, and maybe even the scratching post can be moved around! This is a continuous environment, with endless possibilities for both Bob and the room.\n",
    "\n",
    "Our current room with furniture is a **static environment**. The furniture doesn't move, and the scratching post stays put. But imagine if the furniture and scratching post magically switched places every few hours! That would be a **dynamic environment**, which is trickier for an agent to learn in because things keep changing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcff0e4-c72b-473b-9331-7d8cfa825c40",
   "metadata": {},
   "source": [
    "## 2. Actions and states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e152329-885b-420d-85ae-05be7617563f",
   "metadata": {},
   "source": [
    "Imagine Bob's world like a giant video game. Everything Bob can see, smell, and hear - the furniture, the scratching post, even the dangling string on your curtains - all this information makes up the **state space**. \n",
    "\n",
    "The size of this state space depends on the environment:\n",
    "\n",
    "* **Discrete Environments:** In classic video games with grids, Bob can only be in a limited number of places (states), like in front of the post or next to the couch. This means the state space, and the information Bob gets, is also limited.\n",
    "* **Continuous Environments:** Now picture a super high-tech room where Bob can be anywhere and even move the scratching post. This creates a **continuous state space** with endless possibilities for Bob to explore.\n",
    "\n",
    "The **action space** is all the things Bob can do in the state space. In our scratching post example, Bob's actions could be scratching the post, napping on the couch (lazy), or even chasing butterflies (distracted kitty).\n",
    "\n",
    "Similar to the state space, the number of actions Bob can take depends on the environment:\n",
    "\n",
    "* **Discrete Environments:** In a grid-world game, Bob might have a limited number of actions, like moving up, down, left, or right.\n",
    "* **Continuous Environments:** In our high-tech room, Bob might have a wider range of actions, like moving in any direction, jumping, or even (hopefully not) chewing on wires.\n",
    "\n",
    "**The Starting Point: state 0**\n",
    "\n",
    "When Bob starts his scratching post adventure, the environment is in a default state, let's call it state 0. In our case, this might be the room with the scratching post all set up.\n",
    "\n",
    "Everything gets interesting when Bob takes an action. Walking towards the post, napping on the couch, scratching furniture, or chasing butterflies - each action changes the environment and moves Bob to a new state.\n",
    "\n",
    "So, Bob scratches the post (action) - this makes the environment change (new state)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa992b-d131-49fb-a7a1-f3bd3d9bfe06",
   "metadata": {},
   "source": [
    "## 3. Time step and rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598eca34-aeaf-4e8b-82c7-4a611fb5edb6",
   "metadata": {},
   "source": [
    "Imagine Bob's scratching post adventure like a movie. This movie isn't shown all at once, but rather in short snippets called **time steps**. Each time step is like a single frame in the movie, capturing a snapshot of what Bob is doing (action) and what the environment is like (state).\n",
    "\n",
    "These time steps help us understand the flow of events. We can see how Bob's actions in one time step (scratching something) affect the environment (maybe some satisfying furniture shredding) and lead to a new state in the next time step. The number of time steps can vary depending on the situation. Maybe Bob learns the joy of scratching posts quickly, or maybe it takes a while. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a6249-6ca7-4256-b908-c6bc85f4d711",
   "metadata": {},
   "source": [
    "Learning is all about getting good at something, and reinforcement learning is no different. But how does Bob know if he's doing a good job scratching that post (good kitty) or a bad job clawing the couch (naughty kitty)? This is where **rewards** come in.\n",
    "\n",
    "Rewards are like little treats Bob gets for taking the right actions. In our example, when Bob scratches the post (action), he might receive a positive reward. This reward could be a feeling of accomplishment, a yummy treat, or even just your pat on the head (good kitty!).\n",
    "\n",
    "On the other hand, if Bob scratches the couch, he might not get a reward, or he might even get a negative reward (like a water squirt in the face). These rewards help Bob learn which actions are good and which ones to avoid.\n",
    "\n",
    "By understanding time steps and rewards, we can see the big picture of Bob's learning process. Each time step captures Bob's action, the state of the environment, and the reward he receives. Over many time steps, Bob learns through trial and error, figuring out which actions lead to the most rewards (and hopefully, fewer scratches on your furniture!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6d7e9-8f13-4803-8900-a53bb349c563",
   "metadata": {},
   "source": [
    "## 4. Exploration vs. exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691e39f-f2ed-4c94-91cd-2544d0b665da",
   "metadata": {},
   "source": [
    "Bob's on his way to becoming a scratching post pro! But there's one more challenge to overcome: the exploration-exploitation dilemma. Let's break it down.\n",
    "\n",
    "Imagine Bob is finally getting the hang of using the scratching post. It feels good and it gets treats, it's a win-win! But what if there's an even better scratching spot he hasn't discovered yet, like the one with sleeping shelves? This is the essence of the exploration-exploitation dilemma.\n",
    "\n",
    "* **Exploration:** This is like Bob venturing out, trying new things (like scratching the curtains) and seeing what happens (water in the face or a fish treat). It helps him discover potentially better options in the environment.\n",
    "* **Exploitation:** This is like Bob sticking to what works - the scratching post! It guarantees a reward (praise and satisfaction) he already knows about.\n",
    "\n",
    "The challenge is finding a balance. Exploring too much might waste time, especially in continuous environments, while exploiting too much might make Bob miss out on something even better.\n",
    "\n",
    "There are a few tricks to help Bob explore smartly:\n",
    "\n",
    "* **Epsilon-greedy:** Imagine Bob flips a coin (or has some internal feline process) before taking an action. With a small chance (epsilon), he'll explore and try something new. But most of the time (1-epsilon), he'll exploit and go for the reliable scratching post.\n",
    "\n",
    "* **Boltzmann Exploration:** This strategy is like Bob getting more likely to explore when things are going poorly (consecutive negative rewards for scratching various things). As he gets more rewards from the post, he becomes more likely to exploit that winning strategy.\n",
    "\n",
    "By using these strategies (and many others we haven't explained), Bob can find a balance between exploring the unknown and sticking to the good stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0652475-0b69-43b2-b84a-aa0f8d84e7a5",
   "metadata": {},
   "source": [
    "## 5. The discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc5b49-1f5e-461e-a7ae-378517433a42",
   "metadata": {},
   "source": [
    "Let's talk about the __discount factor__ now. It lowers the value of future rewards compared to immediate ones.\n",
    "\n",
    "Imagine Bob discovers an amazing scratching spot later. Great, but the praise you gave him earlier for using the scratching post seems less exciting now, right? The discount factor reflects this.\n",
    "\n",
    "A high discount factor prioritizes future rewards, making exploration for potentially better scratching spots less appealing. Bob might stick to the good-but-not-great scratching post, missing the ultimate spot!\n",
    "\n",
    "Changing discount factor is how you balance exploration vs. exploitation. A high discount factor encourages long-term rewards but might miss immediate wins. A low one keeps Bob in the present but hinders future discoveries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7536b-05c1-4a4b-82d3-1d3d1f85d7e5",
   "metadata": {},
   "source": [
    "## 6. Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1441582-d435-4455-8123-413c3b8846c0",
   "metadata": {},
   "source": [
    "\r\n",
    "Our curious cat Bob is well on his way to scratching post mastery! But how exactly does he learn which actions lead to the most praise and treats (and the fewest water squirts)? This is where Q-learning comes in. Imagine Q-learning as Bob's internal strategy guide, constantly updated based on his scratching adventures.**\r\n",
    "\r\n",
    "Let's sadiscoversb sees the scratchin behind a couch or somethingg post (state 1). Q-learning assigns a value, called a Q-value, to each possible action Bob can take in that state. Scratching the post (action 1) might have a high Q-value because it leads to rewards (praise, treats). On the other hand, scratching the couch (action 2) might have a low Q-value because it leads to no rewards (or worse, punihment!).\r\n",
    "\r\n",
    "These Q-values are like a currency for Bob. The higher the Q-value for an action in a particular state, the more attractive that action seems to him. So, initially, Bob might explore by scratching both the post and the couch (trial and error). But as he receives rewards (or punishments), the Q-values get updated. The good scratching action (scratching the post) gets a higher and higher Q-value, while the bad scratching action (couch) gets a lower and lower Q-Ledger**\r\n",
    "\r\n",
    "Imaginewhere  a notebook Bob keeps track of all his scratching experiences. This notebook is like a special Q-table, a table that stores all the Q-values for every state-action pair Bob encounters. Each row in the Q-table represents a state (like seeing the scratching post), and each column represents an action (like scratching it or the couch). The cells of the table hold the Q-values, constantly being updated as Bobrning Rule**\r\n",
    "\r\n",
    "So how exactly does Bob update these Q-values in his personal Q-table? Here's the core idea of Q-learning:\r\n",
    "\r\n",
    "1. **Bob takes an action (let's say scratching the post).**\r\n",
    "2. **He observes the new stavisible behind a curtaina little less fuzzy).**\r\n",
    "3. **Hetreatsves a rew based on how close Bob is to the postrd (praise from you!).**\r\n",
    "4. **Based on these four things (state, action, reward, and new state), Bob updates the Q-value for the action he just took in the previous state (scratching the post in state 1).**\r\n",
    "5. **The update considers the reward he received, the Q-value of the best action he could take in the new state (which might be exploring other parts of the post),r (more on that later).**\r\n",
    "\r\n",
    "This update rule ensures that Bob learns from his experiences. Good scratching actions in previous states (like scratching the post in state 1) get their Q-values boosted because they led to rewards. Over time, the Q-table becomes a treasure trove of knowledge for Bob, guiding him towards the most rewarding scratches and away from th \n",
    "\n",
    "__The role of discount factor in Q-learning__\n",
    "  Instant Gratification**\r\n",
    "\r\n",
    "Remember how sometimes a yummy treat seems less exciting if you know you'll get another one later? That's the discount factor at play. It tells Q-learning to valuelessediate rewards slightly more than future ones. This discourages Bob from getting stuck exploring every tiny scratch mark on the post (potentially missing treats) and keeps him focused on the overall goal of getting the most rewards.\r\n",
    "\r\n",
    "By following these steps, Bob (and our Q-learning agent) conalues to decide what to do next!\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0037cb0-1bf3-4fcb-8acb-995c1b77d8fb",
   "metadata": {},
   "source": [
    "## 7. Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55d6a6-99fe-486f-912d-3ba9aff6d018",
   "metadata": {},
   "source": [
    "The Q-table, while informative, doesn't directly guide action. Here's where policy comes in - the agent's decision engine influenced by the Q-table. A policy dictates the action an agent takes in a given state. An optimal policy maximizes rewards.\n",
    "\n",
    "* **Greedy Policies:** A confident agent might prioritize actions with the highest Q-values (e.g., scratching the post for treats).\n",
    "* **Exploratory Policies:** These encourage exploration of less-valued actions for potential long-term rewards (e.g., exploring other parts of the room).\n",
    "\n",
    "* **Epsilon-Greedy:** The agent explores with a small probability (epsilon) and exploits its knowledge (1-epsilon) by following the greedy policy.\n",
    "* **Boltzmann Exploration:** Exploration increases during negative reward periods, and decreases as the agent finds success.\n",
    "\n",
    "By refining its policy based on experience (Q-table updates), the agent learns to balance exploiting known rewards with exploring the unknown, ultimately maximizing its success.\n",
    "\n",
    "In the next section, we dive into implementing reinforcement learning with Python and creating simulated environments for agent learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
