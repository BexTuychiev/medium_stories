{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd094490-34ea-4c59-8ef8-7ff6302ef166",
   "metadata": {},
   "source": [
    "# Understanding Reinforcement Learning in N Easy Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d459b7-1e67-4c04-9870-1d6858cd5182",
   "metadata": {},
   "source": [
    "## Why learn Reinforcement Learning (RL)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03064c97-4a58-4a6b-b0c4-2bb3b098c4a8",
   "metadata": {},
   "source": [
    "To me, the most basic reinforcement learning model resembles science-fiction AI more than any large language model of today. Just take a look at how an RL agent is playing (and finishing) an insanely difficult level of Super Mario:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1d17d-fa52-4168-b487-4a048916db34",
   "metadata": {},
   "source": [
    "In the beginning, this agent has no idea of what the controls are, how to progress through the game, what the obstacles are or what finishes the game. The agent must learn all these things without any human intervention - all through the power of reinforcement learning algorithms. \n",
    "\n",
    "RL agents excel in situations where traditional machine learning algorithms struggle. They can solve problems without predefined solutions or explicitly programmed actions and most importantly, without mounds and mounds of data. That's why RL is having significant impact on many fields. For instance, it's used in:\n",
    "\n",
    "- Self-driving cars: RL agents can learn optimal driving strategies based on traffic conditions and road rules.\n",
    "- Robotics: Robots can be trained to perform complex tasks in dynamic environments through RL.\n",
    "- Game playing: AI agents can learn complex strategies in games like Go or StarCraft II using RL techniques.\n",
    "\n",
    "Reinforcement learning is a rapidly evolving field with vast potential. As research progresses, we can expect even more groundbreaking applications in areas like resource management, healthcare, and personalized learning. \n",
    "\n",
    "That's why now is the best time to learn this fascinating field of computer science. This tutorial will help you get started with the fundamental ideas and concepts in RL step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550221c9-3f3e-49c8-8ddb-205e0da6f368",
   "metadata": {},
   "source": [
    "## 1. Agent and environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59085ea7-673f-40c2-a42f-46bbc59ee013",
   "metadata": {},
   "source": [
    "Imagine you just got your cat, Bob, a fancy new scratching post. You want Bob to learn to use it instead of clawing up your furniture. This situation is a great way to understand the basics of reinforcement learning (RL), a type of problem where an agent learns from trial and error.\n",
    "\n",
    "Bob, the curious cat, is the **agent** in this RL scenario. The agent is the learner and decision-maker. Bob needs to learn which things are okay to scratch (the post) and which are not (the expensive drapes!).\n",
    "\n",
    "The room where Bob explores his scratching desires is the **environment**. It's everything outside the agent that it can interact with. The environment provides challenges (like that comfy-looking couch) and opportunities (the satisfying-on-the-nails scratching post!).\n",
    "\n",
    "There are two main types of environments in RL:\n",
    "\n",
    "* **Discrete Environments:** Imagine a classic video game where the world is like a grid, and Bob can only move up, down, left, or right. These environments have a limited number of options for both Bob (his actions)  and the room (its states, like where Bob and the post are).\n",
    "* **Continuous Environments:** Now picture a super high-tech room where Bob can move in any direction, and the scratching post can be placed at any position and height. This is a continuous environment, with endless possibilities for both Bob and the room.\n",
    "\n",
    "Our current room with furniture is a **static environment**. The furniture doesn't move, and the scratching post stays put. But imagine if the furniture and scratching post magically switched places every few hours. That would be a **dynamic environment**, which is trickier for an agent to learn in because things keep changing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcff0e4-c72b-473b-9331-7d8cfa825c40",
   "metadata": {},
   "source": [
    "## 2. Actions and states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592cc67-b7a7-4f46-805c-98c37943b9e2",
   "metadata": {},
   "source": [
    "Everything Bob can see, smell, and hear - the furniture, the scratching post, even the dangling string on your curtains - all this information makes up the **state space**. \n",
    "\n",
    "The size of this state space depends on the environment:\n",
    "\n",
    "* **Discrete Environments:** In classic video games with grids, Bob can only be in a limited number of places (states), like in front of the post or next to the couch. This means the state space, and the information Bob gets, is also limited.\n",
    "* **Continuous Environments:** Now picture a super high-tech room where Bob can be anywhere and even move the scratching post. This creates a **continuous state space** with endless possibilities for Bob to explore.\n",
    "\n",
    "The **action space** is all the things Bob can do in the state space. In our scratching post example, Bob's actions could be scratching the post, napping on the couch, or even chasing its tail.\n",
    "\n",
    "Similar to the state space, the number of actions Bob can take depends on the environment:\n",
    "\n",
    "* **Discrete Environments:** In a grid-world game, Bob might have a limited number of actions, like moving up, down, left, or right and scratching.\n",
    "* **Continuous Environments:** In our high-tech room, Bob might have a wider range of actions, like moving in any direction, jumping, or even (hopefully not) chewing on wires.\n",
    "\n",
    "When Bob starts his scratching post adventure, the environment is in a default state, let's call it state 0. In our case, this might be the room with the scratching post all set up.\n",
    "\n",
    "Everything gets interesting when Bob takes an action. Walking towards the post, napping on the couch, scratching furniture, or chasing butterflies - each action changes the environment and moves Bob to a new state.\n",
    "\n",
    "So, Bob scratches the post (action) - this makes the environment change (new state)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa992b-d131-49fb-a7a1-f3bd3d9bfe06",
   "metadata": {},
   "source": [
    "## 3. Rewards, time steps and episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecfbdf1-bf88-4c54-a7bf-ba921c0be74d",
   "metadata": {},
   "source": [
    "Once we let Bob loose in the environment (the room), a single training episode starts. The following scenarios might happen:\n",
    "- __The episode may run forever or too long__: Bob may not be interested in scratching his nails at all, so he just keeps sleeping and playing. He doesn't receive any reward for doing this. \n",
    "- __Bob scratches furniture__: \n",
    "\n",
    "These time steps link together to form the episode. We see how Bob's actions in one moment (scratching) affect the environment and lead to a new state in the next. Episode length varies based on Bob's success. A quick learner might master the post in a short episode, while others can take longer.\n",
    "\n",
    "**Rewards**, like points in a game, signal good (give fish treats) or bad (get water squirted in the face) actions for Bob. By experiencing rewards across episodes, Bob learns which actions lead to success (and hopefully fewer furniture casualties).  This understanding of episodes, time steps, and rewards is key to reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6d7e9-8f13-4803-8900-a53bb349c563",
   "metadata": {},
   "source": [
    "## 4. Exploration vs. exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691e39f-f2ed-4c94-91cd-2544d0b665da",
   "metadata": {},
   "source": [
    "Bob's on his way to becoming a scratching post pro! But there's one more challenge to overcome: the exploration-exploitation dilemma. Let's break it down.\n",
    "\n",
    "Imagine Bob is finally getting the hang of using the scratching post. It feels good and it gets treats, it's a win-win! But what if there's an even better scratching spot he hasn't discovered yet, like the one with sleeping shelves? This is the essence of the exploration-exploitation dilemma.\n",
    "\n",
    "* **Exploration:** This is like Bob venturing out, trying new things (like scratching the curtains) and seeing what happens (water in the face or a fish treat). It helps him discover potentially better options in the environment.\n",
    "* **Exploitation:** This is like Bob sticking to what works - the scratching post! It guarantees a reward (praise and satisfaction) he already knows about.\n",
    "\n",
    "The challenge is finding a balance. Exploring too much might waste time, especially in continuous environments, while exploiting too much might make Bob miss out on something even better.\n",
    "\n",
    "There are a few tricks to help Bob explore smartly:\n",
    "\n",
    "* **Epsilon-greedy:** Imagine Bob flips a coin (or has some internal feline process) before taking an action. With a small chance (epsilon), he'll explore and try something new. But most of the time (1-epsilon), he'll exploit and go for the reliable scratching post.\n",
    "\n",
    "* **Boltzmann Exploration:** This strategy is like Bob getting more likely to explore when things are going poorly (consecutive negative rewards for scratching various things). As he gets more rewards from the post, he becomes more likely to exploit that winning strategy.\n",
    "\n",
    "By using these strategies (and many others we haven't explained), Bob can find a balance between exploring the unknown and sticking to the good stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0652475-0b69-43b2-b84a-aa0f8d84e7a5",
   "metadata": {},
   "source": [
    "## 5. The discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc5b49-1f5e-461e-a7ae-378517433a42",
   "metadata": {},
   "source": [
    "Let's talk about the __discount factor__ now. It lowers the value of future rewards compared to immediate ones.\n",
    "\n",
    "Imagine Bob discovers an amazing scratching spot later. Great, but the praise you gave him earlier for using the scratching post seems less exciting now, right? The discount factor reflects this.\n",
    "\n",
    "A high discount factor prioritizes future rewards, making exploration for potentially better scratching spots less appealing. Bob might stick to the good-but-not-great scratching post, missing the ultimate spot!\n",
    "\n",
    "Changing discount factor is how you balance exploration vs. exploitation. A high discount factor encourages long-term rewards but might miss immediate wins. A low one keeps Bob in the present but hinders future discoveries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7536b-05c1-4a4b-82d3-1d3d1f85d7e5",
   "metadata": {},
   "source": [
    "## 6. Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ef363-13fe-4030-84b5-01961d0b499c",
   "metadata": {},
   "source": [
    "Our curious cat Bob is well on his way to scratching post mastery! But how exactly does he learn which actions lead to the most praise and treats (and the fewest water squirts)? This is where Q-learning comes in. Imagine Q-learning as Bob's internal strategy guide, constantly updated based on his scratching adventures.\n",
    "\n",
    "Let's say Bob discovers the scratching post behind a couch or something (state 1). Q-learning assigns a value, called a Q-value, to each possible action Bob can take in that state. Scratching the post (action 1) might have a high Q-value because it leads to rewards (praise, treats). On the other hand, scratching the couch (action 2) might have a low Q-value because it leads to no rewards (or worse, punishment).\n",
    "\n",
    "These Q-values are like a currency for Bob. The higher the Q-value for an action in a particular state, the more attractive that action seems to him. So, initially, Bob might explore by scratching both the post and the couch (trial and error). But as he receives rewards (or punishments), the Q-values get updated. The good scratching action (scratching the post) gets a higher and higher Q-value, while the bad scratching action (couch) gets a lower and lower Q-value.\n",
    "\n",
    "Imagine a notebook where Bob keeps track of all his scratching experiences. This notebook is like a special Q-table, a table that stores all the Q-values for every state-action pair Bob encounters. Each row in the Q-table represents a state (like seeing the scratching post), and each column represents an action (like scratching it or the couch). The cells of the table hold the Q-values, constantly being updated as Bob explores.\n",
    "\n",
    "So how exactly does Bob update these Q-values in his personal Q-table? Here's the core idea of Q-learning:\n",
    "\n",
    "1. **Bob takes an action (let's say scratching the post).**\n",
    "2. **He observes the new state (maybe the post visible behind a curtain).**\n",
    "3. **He receives a reward (treats from you based on how close Bob is to the post).**\n",
    "4. **Based on these four things (state, action, reward, and new state), Bob updates the Q-value for the action he just took in the previous state (scratching the post in state 1).**\n",
    "5. **The update considers the reward he received, the Q-value of the best action he could take in the new state (which might be exploring other parts of the post), and a discount factor.**\n",
    "\n",
    "This update rule ensures that Bob learns from his experiences. Good scratching actions in previous states (like scratching the post in state 1) get their Q-values boosted because they led to rewards. Over time, the Q-table becomes a treasure trove of knowledge for Bob, guiding him towards the most rewarding scratches and away from the dreaded water squirts. \n",
    "\n",
    "__The role of discount factor in Q-learning__\n",
    "\n",
    "Remember how sometimes a yummy treat seems less exciting if you know you'll get another one later? That's the discount factor at play. It tells Q-learning to value immediate rewards slightly less than future ones. This discourages Bob from getting stuck exploring every tiny scratch mark on the post (potentially missing treats) and keeps him focused on the overall goal of getting the most rewards.\n",
    "\n",
    "By following these steps, Bob (and our Q-learning agent) continuously learns and improves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0037cb0-1bf3-4fcb-8acb-995c1b77d8fb",
   "metadata": {},
   "source": [
    "## 7. Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a903c-df41-4cef-bf5e-00163121def5",
   "metadata": {},
   "source": [
    "But how does Bob use the Q-table? After all, he is just a smart cat. This is where __policy__ comes in - his personal \"playbook\" for action. And policies can be **stochastic** or **deterministic**.\n",
    "\n",
    "First, imagine a confident Bob, always picking the highest Q-value action in each state. This is like a clear-cut playbook: see the scratching post - scratch it (assuming it leads to the most rewards)! This ensures consistency and avoids needless exploration.\n",
    "\n",
    "Now, imagine adventurous Bob? Enter **stochastic policies**. These introduce randomness. Bob might explore with a small chance, trying something new (like the curtains) even if the Q-value is low. But most of the time,  he'll still rely on his Q-table knowledge, following his deterministic side and giving the post a good rub.\n",
    "\n",
    "The key is balance between **exploitation** (sticking to proven good actions) and **exploration** (trying new things). A purely deterministic policy might miss out on better rewards. Stochastic policies, with their randomness, help Bob explore while leveraging his knowledge. This balance is crucial for long-term success.\n",
    "\n",
    "**Epsilon-Greedy: A Simple Balance**\n",
    "\n",
    "One way to achieve this is the **epsilon-greedy policy**. Imagine Bob flips a coin (epsilon) before acting. With a small probability (epsilon), he explores. But most of the time (1-epsilon), he exploits his knowledge. This way, Bob can learn about new possibilities while still reaping the rewards of his past experiences.\n",
    "\n",
    "Understanding these policy types and the exploration-exploitation balance is key to Bob (and our agents) continuously improving their decision-making and navigating their environments effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a2f02-72a4-4f25-a5f1-df95e1d84d3b",
   "metadata": {},
   "source": [
    "## 8. Other types of reinforcement learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3486f-fd16-415f-b6fb-0dc940be4c20",
   "metadata": {},
   "source": [
    "From a big picture perspective, reinforcement learning algorithms can be broadly categorized based on how they interact with the environment and learn from experience. Here's a breakdown of the two main categories and some popular algorithms within each:\n",
    "\n",
    "1. Model-based Reinforcement Learning:\n",
    "\n",
    "In this approach, the agent builds an internal model of the environment. This model represents the dynamics of the environment, including state transitions and reward probabilities. The agent can then use this model to plan and evaluate different actions before taking them in the real environment.\n",
    "\n",
    "- Advantages:\n",
    "    - Can be more sample-efficient, especially in complex environments.\n",
    "    - Allows for planning and evaluation before taking actions.\n",
    "- Disadvantages:\n",
    "    - Building an accurate model can be challenging, especially for complex environments.\n",
    "    - The model may not perfectly reflect the real environment, leading to suboptimal behavior.\n",
    "\n",
    "Common model-based RL algorithms:\n",
    "\n",
    "- Dyna-Q: This algorithm combines model-based and model-free learning. It learns a model of the environment and uses it to plan actions while also directly learning from experience through Q-learning (explained under Model-Free).\n",
    "\n",
    "2. Model-Free Reinforcement Learning:\n",
    "\n",
    "This approach focuses on learning directly from interaction with the environment without explicitly building an internal model. The agent learns the value of states and actions or the optimal policy through trial and error.\n",
    "\n",
    "- Advantages:\n",
    "    - Easier to implement in complex environments where building a model is difficult.\n",
    "    - More flexible and adaptable to changes in the environment.\n",
    "- Disadvantages:\n",
    "    - Can be more sample-efficient, requiring more interaction with the environment to learn effectively.\n",
    "\n",
    "Common Model-Free RL Algorithms:\n",
    "\n",
    "- Q-Learning: This is the popular algorithm we focused in this article that learns a Q-value for each state-action pair. The Q-value represents the expected future reward of taking a specific action in a particular state. The agent can then choose the action with the highest Q-value to maximize its long-term reward.\n",
    "\n",
    "- SARSA (State-Action-Reward-State-Action): Similar to Q-learning, but it learns a value function for each state-action pair. It updates the value based on the reward received after taking an action and the next state observed.\n",
    "\n",
    "- Policy Gradient Methods: These algorithms directly learn the policy function, which maps states to actions. They use gradients to update the policy in the direction that is expected to lead to higher rewards. Examples include REINFORCE and Proximal Policy Optimization (PPO).\n",
    "\n",
    "- Deep Q-Networks (DQN): This algorithm combines Q-learning with deep neural networks to handle high-dimensional state spaces, often encountered in complex environments like video games.\n",
    "\n",
    "The choice of reinforcement learning algorithm depends on various factors, including the complexity of the environment, the availability of resources, and the desired level of interpretability. Model-based approaches might be preferable for simpler environments where building an accurate model is feasible. On the other hand, model-free approaches are often more practical for complex, real-world scenarios.\n",
    "\n",
    "Additionally, with the rise of deep learning, Deep Q-Networks (DQN) and other deep reinforcement learning algorithms are becoming increasingly popular for tackling complex tasks with high-dimensional state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d011b1f9-071f-4688-8d32-29745e06d686",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69ac54-a967-4a8d-93de-9c25dd92916d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
