{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Stochastic Gradient Descent Algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are trying to find the lowest point among the hills while blindfolded. Since you are limited by your touch, you can only feel the ground immediately around you to determine which way is down. This is essentially what machine learning algorithms do when they are trying to find the best solution to a problem. They frame the problem into a mathematical function whose inputs and outputs represent a hilly surface. Finding the minimum of this function means you've reached the best solution to the problem. One of the most popular algorithms for doing this process is called Stochastic Gradient Descend (SGD).\n",
    "\n",
    "In this tutorial, you will learn everything you should know about the algorithm including:\n",
    "\n",
    "- Initial intuition without the math\n",
    "- The mathematical details\n",
    "- Implementation in Python\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Optimization in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to clear straight away is that stochastic gradient descent (SGD) is not a machine learning algorithm. Rather, it is merely an optimization technique that can be applied _to_ ML algorithms. \n",
    "\n",
    "So, what is optimization? To understand this, let's work our way up from the problem statement stage of machine learning. \n",
    "\n",
    "Let's say we are trying to predict diamond prices based on their carat value (a carat is 0.2 grams). This is a regression problem as the model produces numeric values. \n",
    "\n",
    "To solve the problem, we have a wide range of algorithms at our disposal but let's choose Simple Linear Regression, which has the simple formula of `f(x) = mx + b`. Here:\n",
    "- `b` is the base diamond price\n",
    "- `m` is the price increase per carat\n",
    "- `x` is the carat value of the diamond\n",
    "- `f(x)` is the predicted price of the diamond\n",
    "\n",
    "This linear equation represents our model. Our goal is to find the best values for `m` and `b` that will make our predictions as accurate as possible across all the diamonds in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had another variable in the picture like diamond volume, our formula would change to `f(x1, x2) = m1*x1 + m2*x2 + b`, where:\n",
    "- `b` is the base diamond price\n",
    "- `m1` is the price increase per carat\n",
    "- `m2` is the price increase per unit volume\n",
    "- `x1` is the carat value of the diamond\n",
    "- `x2` is the volume of the diamond\n",
    "- `f(x)` is the predicted price of the diamond\n",
    "\n",
    "Now, we would need to find the optimal values for `m1, m2`, and `b`. \n",
    "\n",
    "In general, all machine learning models have equations like the ones above with one or more parameters. Thus, the definition of optimization in this context becomes: \"Given this model and this dataset, find the optimal values for the parameters in the equation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we determine what \"best\" means in this context? This is where the concept of a loss function comes in. A loss function measures how far off our predictions are from the actual diamond prices in our dataset. A common loss function for regression problems is the Mean Squared Error (MSE), which we calculate as follows:\n",
    "\n",
    "MSE = (1/n) * Σ(y - f(x))²\n",
    "\n",
    "Where:\n",
    "- n is the number of diamonds in our dataset\n",
    "- y is the actual price of a diamond\n",
    "- f(x) is our predicted price for that diamond\n",
    "- Σ means we sum this up for all diamonds\n",
    "\n",
    "The lower the MSE, the better our model is performing. So, our optimization problem becomes: find the values of `m` and `b` that minimize the MSE.\n",
    "\n",
    "This is where optimization algorithms like gradient descent and its variant, stochastic gradient descent, come into play. These algorithms provide a systematic way to adjust `m` and `b` iteratively, gradually moving towards the values that give us the lowest possible MSE.\n",
    "\n",
    "In essence, optimization in machine learning is the process of finding the best parameters for our model that minimize (or sometimes maximize) a specific objective function - in this case, minimizing our loss function (MSE).\n",
    "\n",
    "Gradient descent and SGD are two approaches to solving this optimization problem. They differ in how they use the data to make these parameter adjustments, which we'll explore in more detail as we dive deeper into each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. The Concept of Error in Machine Learning\n",
    "\n",
    "To make sense of stochastic gradient descent, we need to go over some fundamental ideas behind it, starting with the concept of error in machine learning.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an error or loss?\n",
    "ML algorithms usually guess what the correct answer is to a problem. We call this answer a __prediction__ and it is not always accurate. So, we introduce a new term called \"error\" or \"loss\" that represent the difference between the actual answer and the model's prediction. Our goal is to build a model that minimizes this error. \n",
    "\n",
    "Let's make this concrete through an example: predicting diamond prices. \n",
    "\n",
    "Imagine you are trying to predict diamond prices given its carat value (a carat is 0.2 grams). If our model guesses $10,000 for a diamond that actually costs $12,000, the error is $2000. We should adjust our model to decrease this error.\n",
    "\n",
    "But the model's predictions must be good for any diamond, not just a single one. So, we need a way to combine the error for all diamonds available to us. This is where cost functions come in.\n",
    "\n",
    "A cost function combines all individual errors into one number that represents the overall performance of our model. Lower cost means our model's predictions are better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost functions in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost functions change based on what kind of problem we are solving. \n",
    "\n",
    "In regression problems, the model predicts numeric values like how much a diamond costs or how much time it takes to swim a lap. \n",
    "\n",
    "In classification, the model predicts the category to which something belongs. For example, is a mushroom edible or not, or is the object in the image a cat, dog, or horse?\n",
    "\n",
    "There are other types of problems but the important point is that each problem requires different cost functions. In this tutorial, we will focus on Mean Squared Error (MSE), which is often used in regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The difference between the actual values (ground truth) and the model's predictions is called an error or loss. Consequently, a function that combines all these errors or losses is referred to as an _error function_, _loss function_, or _cost function_. Different sources may use these terms interchangeably; this tutorial will use the term _loss function_ from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression problems, it is common to see the following graph that plots actual values (ground truth) against model's predictions. \n",
    "\n",
    "MAKE UP A CHART HERE\n",
    "\n",
    "The closer the points are to the straight line, the better model predictions are. Therefore, most regression algorithms try to minimize the average distance from the points to the perfect line. And as we mentioned earlier, the minimization happens using a cost function like Mean Squared Error (MSE).\n",
    "\n",
    "MSE takes the actual and predicted values as inputs and produces the squared average distance to the perfect line. \n",
    "\n",
    "MAKE UP A CHART FOR THE VISUAL INTUITION FOR MSE\n",
    "\n",
    "MSE's popularity as a cost function is due to its simple formula:\n",
    "\n",
    "WRITE THE FORMULA HERE DESCRIBING THE VARIABLES\n",
    "\n",
    "If you are familiar with calculus, you know how straightforward it is to differentiate a function like above. And differentiation is at the heart of stochastic gradient descent. \n",
    "\n",
    "Besides, squaring the differences makes them positive and emphasizes bigger errors, penalizing the model more for making large mistakes. \n",
    "\n",
    "That's why MSE is preferred to other alternative functions such as Mean Absolute Error (MAE), which on the surface look simpler (it finds the average absolute distance, rather than squared distance) but is harder to differentiate.\n",
    "\n",
    "SHOW A GRAPH OF MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. The Gradient\n",
    "\n",
    "The next piece in the puzzle is the gradient. Let's go back to our \"down the hill\" analogy to better understand it. \n",
    "\n",
    "### Gradient as the steepest path\n",
    "\n",
    "We were standing on top of the hill blindfolded and wanted to reach the bottom as quickly as possible. If we poured water at our feet, which way it would flow? It would flow downhill in the direction of the steepest descent. \n",
    "\n",
    "This is exactly what the gradient tells us, but in the opposite direction. The gradient points uphill - in the direction of steepest ascent. When we are trying to minimize the error, we simply go the opposite direction of the gradient to find the quickest way down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient and the derivative of a function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's bring this back to our diamond prices prediction problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Gradient Descent: Taking Steps Towards the Solution\n",
    "\n",
    "A. The basic idea: Follow the gradient downhill\n",
    "B. Steps of gradient descent\n",
    "\n",
    "1. Initialize parameters\n",
    "2. Calculate the gradient\n",
    "3. Update parameters\n",
    "4. Repeat\n",
    "\n",
    "C. Learning rate: Controlling our step size\n",
    "D. Convergence: Knowing when to stop\n",
    "E. Advantages and challenges of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Enter Stochastic Gradient Descent\n",
    "A. The problem with regular gradient descent for large datasets\n",
    "B. The stochastic approach: Randomness to the rescue\n",
    "C. How SGD differs from regular gradient descent\n",
    "1. Using one sample at a time\n",
    "2. Faster but noisier progress\n",
    "D. The math behind SGD\n",
    "1. Stochastic cost function\n",
    "2. Stochastic gradient\n",
    "3. Parameter updates in SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. SGD in Action: A Walkthrough Example\n",
    "\n",
    "A. Setting up a simple problem: Linear regression\n",
    "B. Implementing SGD step by step\n",
    "C. Visualizing the progress of SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Practical Considerations and Variations\n",
    "\n",
    "A. Choosing the learning rate\n",
    "B. Mini-batch gradient descent: A middle ground\n",
    "C. Dealing with noisy updates\n",
    "D. When to use SGD vs. regular gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Real-World Applications of SGD\n",
    "A. Large-scale machine learning problems\n",
    "B. Deep learning and neural networks\n",
    "C. Online learning scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Conclusion\n",
    "A. Recap of key concepts\n",
    "B. The power and limitations of SGD\n",
    "C. Encouragement for further exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
