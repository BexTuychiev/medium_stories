{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Stochastic Gradient Descent Algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are trying to find the lowest point among the hills while being blindfolded. Since you are limited by your touch, you can only feel the ground immediately around you to determine which way is down. This is essentially what machine learning algorithms do when they are trying to find the best solution to a problem. They frame the problem into a mathematical function whose inputs and outputs represent a hilly surface. Finding the minimum of this function means you've reached the best solution to the problem. One of the most popular algorithms for doing this process is called Stochastic Gradient Descend (SGD).\n",
    "\n",
    "In this tutorial, you will learn everything you should know about the algorithm including:\n",
    "\n",
    "- The intuition without the math\n",
    "- The mathematical details\n",
    "- Implementation in Python\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. The Concept of Error in Machine Learning\n",
    "\n",
    "To make sense of stochastic gradient descent, we need to go over some fundamental ideas behind it, starting with the concept of error in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an error or loss?\n",
    "ML algorithms usually guess what the correct answer is to a problem. We call this answer a __prediction__ and it is not always accurate. So, we introduce a new term called \"error\" or \"loss\" that represent the difference between the actual answer and the model's prediction. Our goal is to build a model that minimizes this error. \n",
    "\n",
    "Let's make this concrete through an example: predicting diamond prices. \n",
    "\n",
    "Imagine you are trying to predict diamond prices based on their physical measurements (like carat, size or color). If our model guesses $10,000 for a diamond that actually costs $12,000, the error is $2000. We should adjust our model to decrease this error.\n",
    "\n",
    "But the model's predictions must be good for any diamond, not just a single one. So, we need a way to combine the error for all diamonds available to us. This is where cost functions come in.\n",
    "\n",
    "A cost function combines all individual errors into one number that represents the overall performance of our model. Lower cost means our model's predictions are better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost functions in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost functions change based on what kind of problem we are solving. \n",
    "\n",
    "In regression problems, the model predicts numeric values like how much a diamond costs or how much time it takes to swim a lap. \n",
    "\n",
    "In classification, the model predicts the category to which something belongs. For example, is a mushroom edible or not, or is the object in the image a cat, dog, or horse?\n",
    "\n",
    "There are other types of problems but the important point is that each problem requires different cost functions. In this tutorial, we will focus on a particular one often used in regression - Mean Squared Error (MSE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The difference between the actual values (ground truth) and the model's predictions is called an error or loss. Consequently, a function that combines all these errors or losses is referred to as an _error function_, _loss function_, or _cost function_. Different sources may use these terms interchangeably; this tutorial will use the term _loss function_ from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression problems, it is common to see the following graph that plots actual values (ground truth) against model's predictions. \n",
    "\n",
    "MAKE UP A CHART HERE\n",
    "\n",
    "The closer the points are to the straight line, the better model predictions are. Therefore, most regression algorithms try to minimize the average distance from the points to the perfect line. And as we mentioned earlier, the minimization happens using a cost function like Mean Squared Error (MSE).\n",
    "\n",
    "MSE takes the actual and predicted values as inputs and produces the squared average distance to the perfect line. \n",
    "\n",
    "MAKE UP A CHART FOR THE VISUAL INTUITION FOR MSE\n",
    "\n",
    "MSE's popularity as a cost function is due to its simple formula:\n",
    "\n",
    "WRITE THE FORMULA HERE DESCRIBING THE VARIABLES\n",
    "\n",
    "If you are familiar with calculus, you know how straightforward it is to differentiate a function like above. And differentiation is at the heart of stochastic gradient descent. \n",
    "\n",
    "That's why MSE is preferred to other alternative functions such as Mean Absolute Error (MAE), which on the surface look simpler (it finds the average absolute distance, rather than squared distance) but is harder to differentiate.\n",
    "\n",
    "SHOW A GRAPH OF MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. The Gradient\n",
    "\n",
    "A. What is a gradient?\n",
    "B. Gradient as the steepest path\n",
    "C. Partial derivatives: Rate of change for each parameter\n",
    "D. Calculating the gradient\n",
    "1. Intuitive explanation\n",
    "2. Mathematical formulas\n",
    "3. Visual representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Gradient Descent: Taking Steps Towards the Solution\n",
    "\n",
    "A. The basic idea: Follow the gradient downhill\n",
    "B. Steps of gradient descent\n",
    "\n",
    "1. Initialize parameters\n",
    "2. Calculate the gradient\n",
    "3. Update parameters\n",
    "4. Repeat\n",
    "\n",
    "C. Learning rate: Controlling our step size\n",
    "D. Convergence: Knowing when to stop\n",
    "E. Advantages and challenges of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Enter Stochastic Gradient Descent\n",
    "A. The problem with regular gradient descent for large datasets\n",
    "B. The stochastic approach: Randomness to the rescue\n",
    "C. How SGD differs from regular gradient descent\n",
    "1. Using one sample at a time\n",
    "2. Faster but noisier progress\n",
    "D. The math behind SGD\n",
    "1. Stochastic cost function\n",
    "2. Stochastic gradient\n",
    "3. Parameter updates in SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. SGD in Action: A Walkthrough Example\n",
    "\n",
    "A. Setting up a simple problem: Linear regression\n",
    "B. Implementing SGD step by step\n",
    "C. Visualizing the progress of SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Practical Considerations and Variations\n",
    "\n",
    "A. Choosing the learning rate\n",
    "B. Mini-batch gradient descent: A middle ground\n",
    "C. Dealing with noisy updates\n",
    "D. When to use SGD vs. regular gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Real-World Applications of SGD\n",
    "A. Large-scale machine learning problems\n",
    "B. Deep learning and neural networks\n",
    "C. Online learning scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Conclusion\n",
    "A. Recap of key concepts\n",
    "B. The power and limitations of SGD\n",
    "C. Encouragement for further exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
