{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LangChain Agents to Automate Tasks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's 90k GitHub stars is all the credibility it needs - right now, it is the hottest framework to build LLM-based applications. Its comprehensive set of tools and components allows you to build end-to-end AI solutions using almost any LLM. \n",
    "\n",
    "Perhaps, at the heart of LangChain's capabilities are LangChain Agents. These agents are autonomous or semi-autonomous tools that can perform tasks, make decisions, and interact with other tools and APIs. They represent a significant leap forward in automating complex workflows with LLMs. \n",
    "\n",
    "In this article, you will learn how to build your own LangChain agents that can perform tasks not strictly possible with todays chat applications like ChatGPT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are LangChain Agents?\n",
    "\n",
    "Before we write code, let's understand the __agent framework__ and why choose it over traditional programming paradigms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains vs. Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defining trait of agents is their ability to choose the best order of actions to solve a problem given a set of tools.\n",
    "\n",
    "For example, let's say we have the following:\n",
    "\n",
    "- A weather API\n",
    "- ML model for clothing recommendations\n",
    "- Strava API for biking routes\n",
    "- User preferences database\n",
    "- Image recognition model\n",
    "- Language model (text generation)\n",
    "\n",
    "Traditional problem-solving would involve using a chain of select tools from the list:\n",
    "\n",
    "Chain 1: Weather-based clothing recommender\n",
    "1. Call weather API\n",
    "2. Input weather data into ML Clothing Model\n",
    "3. Generate clothing recommendations\n",
    "4. Present results to user\n",
    "\n",
    "Chain 2: Weather-based biking route suggester\n",
    "1. Call weather API\n",
    "2. Call Strava API for popular routes\n",
    "3. Filter routes based on weather conditions\n",
    "4. Present suitable routes to user\n",
    "\n",
    "Chain 3: Outfit Photo Analyzer\n",
    "1. Receive user's outfit photo\n",
    "2. Use Image Recognition Model to identify clothing items\n",
    "3. Compare with User Preference Database\n",
    "4. Generate feedback using Text Generation Model\n",
    "5. Present analysis to user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chain solves a specific problem, using a predetermined sequence of steps and a subset of the available tools. They cannot adapt beyond their defined scope. They also require three separate branches of development, which is inefficient in terms of time and resources.\n",
    "\n",
    "Now, imagine an agentic system (agent) with access to all these tools. It would be able to:\n",
    "\n",
    "1. Understand user's query or problem\n",
    "2. Assess which tools are relevant to the problem\n",
    "3. Dynamically create a workflow using the most appropriate tools\n",
    "4. Execute the workflow, making real-time adjustments if needed\n",
    "5. Evaluate the outcome and learn for future interactions\n",
    "\n",
    "For example, if a user asks \"What should I wear for my bike ride today?\", the agent might check the weather API, analyze suitable biking routes through Strava, recommend appropriate clothing considering user's past preferences and generate a personalized response. \n",
    "\n",
    "The agent can:\n",
    "- Handle a wide variety of problems using the same set of tools\n",
    "- Create custom workflows for each unique situation\n",
    "- Adapt its approach based on the specific context and user needs\n",
    "- Learn from interactions to improve future performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's capacity to transform language models—which, by themselves, only produce text—into reasoning engines that can use the resources at their disposal to take appropriate action is one of its main applications. Put differently, LangChain enables the development of strong autonomous agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Chat models__\n",
    "\n",
    "There are a lot of moving parts involved in creating a LangChain agent. The first and most obvious is a language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Prime number: 73 is a prime number, which means it is only divisible by 1 and itself. This makes\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(api_key=api_key, model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "question = \"What is special about the number 73?\"\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models, like OpenAI's GPT-3.5 Turbo, take and generate strings. They are typically older and work best to answer individual user queries. \n",
    "\n",
    "For this reason, agents usually use chat models, which can take a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessageChunk, SystemMessage\n",
    "\n",
    "# Initialize the model\n",
    "chat_model = ChatOpenAI(api_key=api_key, model='gpt-4o-mini')\n",
    "\n",
    "# Write the messages\n",
    "messages = [SystemMessage(content='You are a grumpy pirate.'),\n",
    "            HumanMessage(content=\"What's up?\")]\n",
    "\n",
    "output = chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put differently, they allow us to have conversations with chat models. Above, we are initializing GPT-4o-mini with a system message followed by a user query. Note the use of `SystemMessage` and `HumanMessage` classes. \n",
    "\n",
    "Chat models return messages as outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, nothin' much but the same ol' waves and the smell of salt in the air! Just tryin' to avoid scallywags and keep me treasure safe. What be ye wantin', eh? Spit it out!\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, they return other useful metadata accessible with dot-notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"Arrr, nothin' much but the same ol' waves and the smell of salt in the air! Just tryin' to avoid scallywags and keep me treasure safe. What be ye wantin', eh? Spit it out!\",\n",
       " 'additional_kwargs': {},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 51,\n",
       "   'prompt_tokens': 21,\n",
       "   'total_tokens': 72},\n",
       "  'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "  'system_fingerprint': 'fp_0f03d4f0ee',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-6a931b68-1755-45d2-a5f5-4955d342c6a5-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 21,\n",
       "  'output_tokens': 51,\n",
       "  'total_tokens': 72}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we mentioned that agents can choose a combination of tools at their disposal to solve a particular problem, with LLMs as reasoning engines under the hood. \n",
    "\n",
    "LangChain offers integrations with dozens of popular APIs and services as agent-compatible tools. Most of them are available under the `langchain_community` package while some are inside `langchain_core`. \n",
    "\n",
    "For example, here is how you can use the ArXiv tool to answer scientific questions based on papers submitted to arXiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq langchain_community arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2012-08-22\\nTitle: Reconstruction of the wavefunctions of coupled nanoscopic emitters using a coherent optical technique\\nAuthors: M. Richter, F. Schlosser, M. Schoth, S. Burger, F. Schmidt, A. Knorr, S. Mukamel\\nSummary: We show that using coherent, spatially resolved spectroscopy, complex hybrid\\nwave functions can be disentangled into the individual wave functions of the\\nindividual emitters. This way, detailed information on the coupling of the\\nindividual emitters, not available in far-field spectroscopy can be obtained.\\nThe proposed quantum state tomography relies on the ability to selectively\\nexcite each emitter individually by spatially localized pulses. Simulations of\\ncoupled semiconductor Ga/InAs quantum dots, using light fields available in\\ncurrent nanoplasmonics, show that even undesired resonances can be removed from\\nmeasured spectra. The method can also be applied to study the internal coupling\\nof pigments in photosythesis and artificial light harvesting.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "tool = ArxivQueryRun()\n",
    "\n",
    "tool.invoke('Photosythesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a few other tools when building our own agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to query chat models is by using prompt templates. They allow you to structure your queries consistently and dynamically insert variables, making your interactions with the model more flexible and reusable.\n",
    "\n",
    "In LangChain, there are many types of prompt templates with the most basic one being `PromptTemplate` class. It can be used with language (plain text) models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='When was Audi founded?')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "query_template = \"When was {car_brand} founded?\"\n",
    "prompt = PromptTemplate(input_variables=[\"car_brand\"], template=query_template)\n",
    "\n",
    "prompt.invoke({\"car_brand\": \"Audi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this prompt template by chaining it to our language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Chevrolet was founded on November 3, 1911.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(api_key=api_key)\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain\n",
    "output = chain.invoke({'car_brand': 'Chevrolet'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipe operator (`|`) is part of LangChain Expression Language (LCEL) designed to chain multiple LangChain components and tools (more on LCEL later). \n",
    "\n",
    "Another prompt template class works with chat models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_model = ChatOpenAI(api_key=api_key, model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2: Setting Up Your Environment\n",
    "### H3: Prerequisites\n",
    "- Required knowledge and tools for working with LangChain Agents.\n",
    "- Necessary libraries and setup instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2: Building a Basic LangChain Agent\n",
    "### H3: Initializing tools and models\n",
    "- Step-by-step guide to setting up tools like search engines and web fetchers.\n",
    "### H3: Creating and configuring an agent\n",
    "- Instructions on creating an agent, defining its actions, and configuring it with tools.\n",
    "- Example code snippets and explanations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2: Advanced Agent Configuration\n",
    "### H3: Adding memory and context\n",
    "- How to incorporate conversational memory to make agents more interactive.\n",
    "- Examples of different memory options and their applications.\n",
    "### H3: Using multiple tools and actions\n",
    "- Configuring agents to utilize multiple tools for more complex tasks.\n",
    "- Code examples demonstrating multi-tool interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2: Optimizing and Debugging Agents\n",
    "### H3: Performance tuning\n",
    "- Tips and techniques for optimizing the performance of LangChain Agents.\n",
    "- Common issues and troubleshooting strategies.\n",
    "### H3: Debugging tools and methods\n",
    "- Utilizing LangSmith and other tools for monitoring and debugging agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2: Applications and Use Cases\n",
    "### H3: Industry-specific applications\n",
    "- Examples of how LangChain Agents can be applied in different industries such as finance, healthcare, and marketing.\n",
    "### H3: Case studies\n",
    "- Detailed case studies showcasing successful implementations of LangChain Agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2: Future Trends and Developments\n",
    "### H3: Emerging features\n",
    "- Upcoming updates and new features in LangChain.\n",
    "### H3: Long-term vision\n",
    "- The future potential of LangChain Agents in AI and their role in advancing automation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2: Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ conda create -n langchain python=3.9 -y\n",
    "$ conda activate langchain\n",
    "$ pip install langchain langchain_openai langchain_community ipykernel python-dotenv\n",
    "$ ipython kernel install --user --name=langchain\n",
    "```\n",
    "\n",
    "```bash\n",
    "$ touch .env \n",
    "$ vim .env  # Paste your OPENAI key\n",
    "```\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Many people consider Lionel Messi to be one of the greatest footballers of all time. He has won numerous individual and team awards, broken multiple records, and has consistently performed at a high level for both club and country. However, the title of \"best footballer of all time\" is subjective and there are many other players who have also been considered for this title, such as Diego Maradona, Pele, and Cristiano Ronaldo. Ultimately, it is a matter of personal opinion.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=api_key)\n",
    "\n",
    "question = \"Is Messi the best footballer of all time?\"\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff I should cover\n",
    "- Difference between an LLMs and chat models in langchain. [use this link](https://python.langchain.com/v0.1/docs/modules/model_io/)\n",
    "- Prompt templates (see link above)\n",
    "- Output parses (see link above)\n",
    "- [Retrieval](https://python.langchain.com/v0.1/docs/modules/data_connection/)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we explored what makes LangChain agents distinct from chains and the important building blocks used in constructing them. We first introduced what agents are and how they differ from the more traditional chain constructs regarding flexibility and capability of making decisions. \n",
    "\n",
    "Then we looked at the key components you need to know about in order to build an agent: chat models, tools, and prompt templates. Finally, we ran through two examples demonstrating how to build simple and advanced agents. Natural language processing is developing continually, and LangChain agents are at the forefront of this progression, paving the way for an even more intelligent and versatile family of AI.\n",
    "\n",
    "Here are some related resources to increase your LangChain:\n",
    "\n",
    "- https://www.datacamp.com/courses/developing-llm-applications-with-langchain\n",
    "- https://www.datacamp.com/tutorial/prompt-engineering-with-langchain\n",
    "- https://www.datacamp.com/tutorial/how-to-build-llm-applications-with-langchain\n",
    "- https://www.datacamp.com/tutorial/building-a-gpt-model-with-browsing-capabilities-using-lang-chain-tools\n",
    "\n",
    "Thank you for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
