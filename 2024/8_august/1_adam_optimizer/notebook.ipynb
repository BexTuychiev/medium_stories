{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM Optimizer Tutorial: Intuition And Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever tried navigating your way down a hilly area blindfolded? That's somewhat analogous to what machine learning models do when they are trying to improve. They continually search for the lowest point (best solution) without really seeing the whole picture. This is where optimization algorithms come in handy, and ADAM is like the smart flashlight in this journey. \n",
    "\n",
    "ADAM, short for Adaptive Moment Estimation, is a popular optimization technique, especially in deep learning. In this article, you'll see why this is the case. We will cover the intuition behind it, dive into some math (don't worry, we will keep it friendly), its Python implementation and how to use it in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is ADAM Optimizer? The Short Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The popular optimization algorithm used in machine learning and, most often, in deep learning is called ADAM, which stands for Adaptive Moment Estimation. \n",
    "\n",
    "ADAM combines ideas from two other robust optimization techniques: momentum and RMSprop. It is named _adaptive_ as it adjusts the learning rate for each parameter. \n",
    "\n",
    "Here are its key features and advantages:\n",
    "\n",
    "- Adaptivity: ADAM adapts the learning rate for each parameter, which can speed up learning in many cases.\n",
    "- Momentum: It uses a form of momentum, helping it navigate complex surfaces such as ravines and saddle points more effectively.\n",
    "- Bias correction: ADAM includes bias correction terms, which help it perform well even in the initial stages of training.\n",
    "- Computational efficiency: It's relatively computationally efficient and has low memory requirements.\n",
    "- Hyperparameter robustness: While the learning rate may need tuning, ADAM is often less sensitive to hyperparameter choices than some other optimizers.\n",
    "\n",
    "To summarize, ADAM makes models learn more efficiently by continuously adjusting the learning rate of each parameter and, as a consequence, tends to converge much more quickly than standard stochastic gradient descent. For many deep learning applications, it is therefore a strong default choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Concepts for Understanding ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAM builds on fundamental ideas from a few other optimization algorithms. To fully understand it and implement it in Python later, we need to revisit these ideas in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The definition of optimization in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's recap what optimization is in the context of machine learning. \n",
    "\n",
    "Supervised learning starts with a problem statement such as \"Given this dataset of diamond carats, predict their price\" which is a regression task. To solve it, we choose a training algorithm (a model) we think best suited to capture the information inside our dataset. Let's say we choose Simple Linear Regression, which has the formula of `f(x) = mx + b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(m, x, b):\n",
    "    \"\"\"\n",
    "    Simple Linear Regression model. Here:\n",
    "    - b is the base diamond price\n",
    "    - m is the price increase per carat\n",
    "    - x is the carat value of the diamond\n",
    "    \n",
    "    f(x) is the result of this function, which is the ...\n",
    "    predicted price of the diamond.\n",
    "    \"\"\"\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `m` and `b` are the parameters of our regression model. And it is the job of the optimization algorithm to find their best values so that our model performs as well as it can on the training and testing datasets. \n",
    "\n",
    "In other words, the definition of optimization becomes: \"Given this model and this dataset, find the optimal values for the parameters in the model equation\".\n",
    "\n",
    "In deep learning, ADAM is frequently chosen to carry out this optimization process for its adaptivity and fast convergence. \n",
    "\n",
    "> Terminology note: when an optimization algorithm _converges_, it means the optimization stopped and the best values for parameters are found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization is always an iterative process. Algorithms usually take small steps to figuring out the optimal values for model parameters.  At each step, they measure the differences between model's predictions and the ground truth values (labels of the dataset) using a __loss function__.\n",
    "\n",
    "Loss functions produce a single value called _the loss_ or _cost_ that tells how badly or well our model is performing. For example, in regression problems, Mean Squared Error (MSE) function is often used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_squared_error(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Mean Squared Error - computes the average squared ...\n",
    "    difference between model predictions and ground truth values.\n",
    "    \n",
    "    \"\"\"\n",
    "    loss = np.mean((ground_truth - predictions) ** 2)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification, we usually have Categorical Cross Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Categorical Cross Entropy - computes the cross entropy loss between the\n",
    "    ground truth labels and the predicted probabilities.\n",
    "    \"\"\"\n",
    "    loss = -np.mean(np.sum(ground_truth * np.log(predictions), axis=1))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whichever loss function we are using, we always want to make the loss as small as possible. That's what optimization algorithms do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD With Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
