{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM Optimizer Tutorial: Intuition And Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever tried navigating your way down a hilly area blindfolded? That's somewhat analogous to what machine learning models do when they are trying to improve. They continually search for the lowest point (best solution) without really seeing the whole picture. This is where optimization algorithms come in handy, and ADAM is like the smart flashlight in this journey. \n",
    "\n",
    "ADAM, short for Adaptive Moment Estimation, is a popular optimization technique, especially in deep learning. In this article, you'll see why this is the case. We will cover the intuition behind it, dive into some math (don't worry, we will keep it friendly), its Python implementation and how to use it in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is ADAM Optimizer? The Short Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The popular optimization algorithm used in machine learning and, most often, in deep learning is called ADAM, which stands for Adaptive Moment Estimation. \n",
    "\n",
    "ADAM combines ideas from two other robust optimization techniques: momentum and RMSprop. It is named _adaptive_ as it adjusts the learning rate for each parameter. \n",
    "\n",
    "Here are its key features and advantages:\n",
    "\n",
    "- Adaptivity: ADAM adapts the learning rate for each parameter, which can speed up learning in many cases.\n",
    "- Momentum: It uses a form of momentum, helping it navigate complex surfaces such as ravines and saddle points more effectively.\n",
    "- Bias correction: ADAM includes bias correction terms, which help it perform well even in the initial stages of training.\n",
    "- Computational efficiency: It's relatively computationally efficient and has low memory requirements.\n",
    "- Hyperparameter robustness: While the learning rate may need tuning, ADAM is often less sensitive to hyperparameter choices than some other optimizers.\n",
    "\n",
    "To summarize, ADAM makes models learn more efficiently by continuously adjusting the learning rate of each parameter and, as a consequence, tends to converge much more quickly than standard stochastic gradient descent. For many deep learning applications, it is therefore a strong default choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAM unifies key ideas from a few other critical optimization algorithms, strengthening their advantages while also addressing their shortcomings. We will need to review them before we can grasp the intuition behind ADAM and implement it in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a common analogy to understand the intuition behind these optimization algorithms. Picture yourself being blindfolded in a complex hilly region and told to locate the lowest point. This hilly terrain represents the loss function in machine learning, where the global lowest point is the optimal solution (minimum loss).\n",
    "\n",
    "In this analogy:\n",
    "\n",
    "- Your position represents the current state of the model's parameters.\n",
    "- The height at any point represents the loss value for those parameters.\n",
    "- Moving around corresponds to adjusting the model's parameters.\n",
    "- The goal is to reach the lowest point, which represents the best set of parameters to minimizing the loss function.\n",
    "\n",
    "Each optimization algorithm is like a strategy for navigating this landscape effectively, telling you where to step next and how large those steps should be. Some of them scan the entire area before taking a step while others act on limited information. There are also ones that use specialized tools like momentum and adaptive step sizes.\n",
    "\n",
    "This analogy is perfect to visualize the challenges in optimization such as avoiding local minima (small valleys that aren't the deepest points) and dealing with steep or flat regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is the holy grail of optimization in machine learning, as it sets the foundation many algorithms build upon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It Starts With SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article assumes you are familiar with the fundamental optimization algorithm, Gradient Descent (GD) and its more popular variant, Stochastic Gradient Descent (SGD). Both play crucial roles in understanding ADAM, the reason for its invention and how to implement it in Python. If you are completely new, we have written a [separate article](https://www.datacamp.com/tutorial/stochastic-gradient-descent) explaining both regular and stochastic gradient descent.\n",
    "\n",
    "In any case, in this section, we will go through a basic Python implementation of SGD as a kind of background for what we will do in the next sections. \n",
    "\n",
    "First, we import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the Diamonds dataset from Seaborn, take a sample from it and build the feature and target arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "dataset_size = 10_000\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "# Extract the target and the feature\n",
    "xy = diamonds[[\"carat\", \"price\"]].values\n",
    "np.random.shuffle(xy)  # Shuffle the data\n",
    "xy = xy[:dataset_size]\n",
    "\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are setting up a basic regression problem - given diamond carats (their weight), predict their price. \n",
    "\n",
    "Now, let's split the data to create training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data\n",
    "np.random.shuffle(xy)\n",
    "\n",
    "train_size = int(0.8 * dataset_size)\n",
    "train_xy, test_xy = xy[:train_size], xy[train_size:]\n",
    "\n",
    "train_xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the task, we have a range of models at our disposal, but to keep things simple, we will chose Linear Regression and define it as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(m, x, b):\n",
    "    \"\"\"\n",
    "    Simple Linear Regression: f(x) = m * x + b, where\n",
    "    - x: diamond carat\n",
    "    - m: price increase per carat\n",
    "    - b: base diamond price\n",
    "    - f(x): predicted diamond price\n",
    "    \"\"\"\n",
    "    \n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Linear Regression model has only two parameters, `m` and `b`, so the task for SGD (and later, for ADAM) is to find optimal values for them. \n",
    "\n",
    "We should also define the loss function, Mean Squared Error, which will be used by SGD for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    MSE as a loss function. It is defined as:\n",
    "\n",
    "    Loss = (1/n) * Σ((y - f(x))²), where:\n",
    "    - n: the length of the dataset\n",
    "    - y: the true diamond price\n",
    "    - f(x): predicted diamond price, i.e. m*x + b\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function called `stochastic_gradient_descent` that accepts six arguments:\n",
    "\n",
    "- `x` and `y` represent the single feature and target in our problem\n",
    "- `epochs` denotes how many times we want to perform the descent (more on this later)\n",
    "- `learning_rate` is the step size\n",
    "- `batch_size` to control how frequently we make parameter updates\n",
    "- `stopping_threshold` sets the minimum value the loss should decrease at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(\n",
    "    x, y, epochs=100, learning_rate=0.01, batch_size=32, stopping_threshold=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    SGD with support for mini-batches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the model parameters randomly\n",
    "    m = np.random.randn()\n",
    "    b = np.random.randn()\n",
    "    n = len(x)  # The number of data points\n",
    "    previous_loss = np.inf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the function, we first initialize the parameters we want to optimize with random values. We also set initial loss to infinity, representing the unsolved state of our problem.\n",
    "\n",
    "Then, we start a `for` loop that runs for `epochs` iterations. Inside the loop, we shuffle the data to prevent learning order-dependent patters in the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(...):\n",
    "    ...\n",
    "    for i in range(epochs):\n",
    "        # Shuffle the data\n",
    "        indices = np.random.permutation(n)\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we start another loop controlled by the `batch_size` parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(...):\n",
    "    ...\n",
    "    for i in range(epochs):\n",
    "        ...\n",
    "        for j in range(0, n, batch_size):\n",
    "            # Extract the current batch\n",
    "            x_batch = x[j:j + batch_size]\n",
    "            y_batch = y[j:j + batch_size]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this inner loop, we calculate the gradients (partial derivatives) for both parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(...):\n",
    "    ...\n",
    "    for i in range(epochs):\n",
    "        ...\n",
    "        for j in range(0, n, batch_size):\n",
    "            # Extract the current batch\n",
    "            ...\n",
    "            # Compute the negative gradients\n",
    "            y_pred = model(m, x_batch, b)  # Make predictions with current m and b\n",
    "            m_gradient = -2 * np.mean(x_batch * (y_batch - y_pred))\n",
    "            b_gradient = -2 * np.mean(y_batch - y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the gradients, we update the parameters using the learning rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(...):\n",
    "    ...\n",
    "    for i in range(epochs):\n",
    "        ...\n",
    "        for j in range(0, n, batch_size):\n",
    "            # Extract the current batch\n",
    "            ...\n",
    "            # Compute the negative gradients\n",
    "            ...\n",
    "            # Update the model parameters\n",
    "            m -= learning_rate * m_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, under the parent loop, we calculate the loss for the current epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(...):\n",
    "    ...\n",
    "    for i in range(epochs):\n",
    "        ...\n",
    "        for j in range(0, n, batch_size):\n",
    "            # Extract the current batch\n",
    "            ...\n",
    "            # Compute the negative gradients\n",
    "            ...\n",
    "            # Update the model parameters\n",
    "            ...\n",
    "        # Compute the epoch loss\n",
    "        y_pred = model(m, x, b)\n",
    "        current_loss = loss(y, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the epoch loss is smaller than the `stopping_threshold`, we stop the entire process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(...):\n",
    "    ...\n",
    "    for i in range(epochs):\n",
    "        ...\n",
    "        for j in range(0, n, batch_size):\n",
    "            # Extract the current batch\n",
    "            ...\n",
    "            # Compute the negative gradients\n",
    "            ...\n",
    "            # Update the model parameters\n",
    "            ...\n",
    "        # Compute the epoch loss\n",
    "        ...\n",
    "        # Check against the stopping threshold\n",
    "        if previous_loss - current_loss < stopping_threshold:\n",
    "            break\n",
    "        previous_loss = current_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end (after epochs run out or the stopping threshold is met), we return `m` and `b` which are now optimized:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(...):\n",
    "    ...\n",
    "    for i in range(epochs):\n",
    "        ...\n",
    "        for j in range(0, n, batch_size):\n",
    "            # Extract the current batch\n",
    "            ...\n",
    "            # Compute the negative gradients\n",
    "            ...\n",
    "            # Update the model parameters\n",
    "            ...\n",
    "        # Compute the epoch loss\n",
    "        ...\n",
    "        # Check against the stopping threshold\n",
    "        ...\n",
    "    return m, b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ve pasted the entire code into [this GitHub gist](https://gist.github.com/BexTuychiev/645dcd35ef12dad323b6c0182f29be74) so that you can look at the whole picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD With Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, ADAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
