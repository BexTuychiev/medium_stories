{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Tutorial on Building a RAG Application Using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the amount of information today's LLMs have access is growing constantly, there are mounds of private data not included in their training. For this reason, one of the most popular applications of LLMs in enterprise settings is retrieval-augmented generation (RAG). When you design a RAG system, you retrieve information from a private data source and feed it to a language model to generate contextually relevant responses. \n",
    "\n",
    "In this tutorial, you will learn how to develop RAG applications using a massively popular framework - LangChain. I'll guide you through the process step-by-step, from setting up your environment to implementing the core components of a RAG system. By the end of this tutorial, you'll have created your own chatbot capable of answering questions using any outside source, opening up a world of possibilities for leveraging private data in your AI applications.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clarify what RAG is, let's consider a simle example.\n",
    "\n",
    "A first-year college student, Chandler, is consider to skip a few classes but wants to ensure he isn't violating the university attendance policy. Like with anything these days, he asks ChatGPT the question.\n",
    "\n",
    "Of course, ChatGPT can't answer it. The chatbot isn't dumb - it just doesn't have access to Chandler's university documents. So, Chandler finds the policy document himself and discovers that it is a long, technical read he doesn't want to wade through.\n",
    "\n",
    "Instead, he gives the entire document to ChatGPT and asks the question again. This time, he gets his answer. \n",
    "\n",
    "This is an individual case of retrieval augmented generation. The language model's answer (generation) is augmented (enriched) by context retrieved from a source not part of its original training. \n",
    "\n",
    "A scalable version of a RAG system would be able to answer any student question by searching university documents itself, finding the relevant ones and retrieving chunks of text that most likely contain the answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a system, despite sounding straightforward, would have a lot of moving components. Before building one ourselves, we need to review what they are and how they play together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component is a document or a collection of documents. Based on the type of RAG system we are building, the documents can be text files, PDFs, web pages (RAG over unstructured data) or graph, SQL, NoSQL databases (RAG over structured data). They are used to ingest various types of data into the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain implements hundreds of classes called _document loaders_ to read data from various document sources such as PDFs, Slack, Notion, Google Drive, and so on. \n",
    "\n",
    "Each DocumentLoader class is unique but they all share the same `.load()` method. For example, here is how you can load a PDF document and a webpage in LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader  # pip install langchain-community\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"framework_docs.pdf\")\n",
    "web_loader = WebBaseLoader(\n",
    "    \"https://python.langchain.com/v0.2/docs/concepts/#document-loaders\"\n",
    ")\n",
    "\n",
    "pdf_docs = pdf_loader.load()\n",
    "web_docs = web_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyPDFLoader class handles PDF files using the PyPDF2 package under the hood, while the `WebBaseLoader` scrapes the given webpage contents. \n",
    "\n",
    "`pdf_docs` contains four document objects, one for each page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `web_docs` contain only one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the v0.1 docs here.IntegrationsAPI referenceLatestLegacyMorePeopleContributingCookbooks3rd party tutorialsYouTubearXivv0.2v0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangCh\n"
     ]
    }
   ],
   "source": [
    "print(web_docs[0].page_content[125:300].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These document objects are later given to embedding models to understand the semantic meaning behind their text. \n",
    "\n",
    "For specifics on other types of document loaders, LangChain offers a [dedicated how-to page](https://python.langchain.com/v0.2/docs/how_to/#document-loaders)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have loaded your documents, it is crucial to break them down into smaller and more manageable chunks of text. Here are the main reasons:\n",
    "\n",
    "1. Many embedding models (more on them later) have a maximum token limit.\n",
    "2. Retrieval is more accurate when you have smaller chunks.\n",
    "3. The language model is fed the exact context.\n",
    "\n",
    "LangChain offers many types of text splitters under its `langchain_text_splitters` package and they differ based on document type. \n",
    "\n",
    "Here is how to use `RecursiveCharacterTextSplitter` to split plain text based on a list of separators and chunk size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: RAG systems combine the power of large language\n",
      "Chunk 2: language models with external knowledge sources.\n",
      "Chunk 3: This allows them to provide up-to-date and\n",
      "Chunk 4: and context-specific information.\n",
      "Chunk 5: The process involves several steps including\n",
      "Chunk 6: including document loading, text splitting, and\n",
      "Chunk 7: and embedding.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "RAG systems combine the power of large language models with external knowledge sources.\n",
    "This allows them to provide up-to-date and context-specific information.\n",
    "The process involves several steps including document loading, text splitting, and embedding.\n",
    "\"\"\"\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Print the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splitter is versatile and works well for many uses cases. It creates each chunk with a character count as close to `chunk_size` as possible. It can recursively switch between which separators to split at to keep the character count.\n",
    "\n",
    "In the above example, our splitter tries to split on newlines first, then single spaces, and finally between any characters to reach the desired chunk size.\n",
    "\n",
    "There are many other splitters inside `langchain_text_splitters` package. Here are some:\n",
    "- HTMLSectionSplitter\n",
    "- PythonCodeTexSplitter\n",
    "- RecursiveJsonSplitter\n",
    "\n",
    "and so on. Some of the splitters create semantically meaningful chunks by using a transformer model under the hood. \n",
    "\n",
    "The right text splitter has a significant impact on the performance of a RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Workflow to Building a RAG App in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
