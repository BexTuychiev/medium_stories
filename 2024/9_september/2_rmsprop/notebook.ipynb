{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop Optimizer Tutorial: Intuition and Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Root-mean squared propagation__ (RMSprop) is a powerful optimization algorithm used in machine learning to find the model parameters that correspond to the best fit between actual values and model predictions. The algorithm is widely used in deep learning in combination with backpropagation during neural network training.\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- The intuition behind RMSprop optimizer\n",
    "- How to use RMSprop in PyTorch\n",
    "- How to implement it in pure NumPy for deeper understanding\n",
    "- Its differences from other optimization algorithms such as SGD and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RMSprop? The Short Answer\n",
    "\n",
    "RMSprop (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm designed to accelerate the convergence of gradient descent. Key features of RMSprop include:\n",
    "\n",
    "1. Adaptive learning rates: It adjusts the learning rate for each parameter based on the historical gradient information.\n",
    "\n",
    "2. Moving average of squared gradients: RMSprop maintains a moving average of squared gradients for each parameter, which helps to normalize the gradient updates.\n",
    "\n",
    "3. Momentum-like behavior: By using the moving average, RMSprop achieves a momentum-like effect without explicitly incorporating momentum terms.\n",
    "\n",
    "4. Improved performance on non-stationary problems: RMSprop is particularly effective for optimizing non-convex loss functions and handling non-stationary objectives.\n",
    "\n",
    "5. Hyperparameter sensitivity reduction: Compared to standard SGD, RMSprop reduces the need for manual tuning of the learning rate hyperparameter.\n",
    "\n",
    "In essence, RMSprop addresses the diminishing learning rates problem of AdaGrad while providing adaptive per-parameter learning rates, making it a popular choice for training deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Intuition Behind RMSprop Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps to think of optimization as finding the lowest point in a hilly terrain while being blindfolded. Since you are limited to your touch, you can find which way is down by only feeling the ground immediately around you. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RMSprop in PyTorch and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you rarely have to implement RMSprop manually. Since it is a widely used algorithm, it is available in popular frameworks such as PyTorch and Tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the algorithm is implemented under the `optim` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.rmsprop.RMSprop"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.optim.RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can use it to optimize (find the minimum) of any function `f(x)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = 361.0000\n",
      "Iteration 100: x = 302.5840\n",
      "Iteration 200: x = 268.0156\n",
      "Iteration 300: x = 236.2579\n",
      "Iteration 400: x = 205.2928\n",
      "Iteration 500: x = 174.5349\n",
      "Iteration 600: x = 143.7643\n",
      "Iteration 700: x = 112.8619\n",
      "Iteration 800: x = 81.7036\n",
      "Iteration 900: x = 50.0471\n",
      "Iteration 1000: x = 17.0591\n",
      "Iteration 1100: x = 2.7183\n",
      "Iteration 1200: x = 2.7183\n",
      "Iteration 1300: x = 2.7183\n",
      "Iteration 1400: x = 2.7183\n",
      "Final result: x = 2.7183\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the function: f(x) = x - log(x)\n",
    "def f(x):\n",
    "    return x / torch.log(x)\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor([364.0], requires_grad=True)\n",
    "\n",
    "# Create an RMSprop optimizer\n",
    "optimizer = optim.RMSprop([x], lr=0.3)\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(1500):\n",
    "    # Forward pass: compute the loss\n",
    "    loss = f(x)\n",
    "    \n",
    "    # Backward pass: compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameter\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i}: x = {x.item():.4f}')\n",
    "\n",
    "print(f'Final result: x = {x.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are optimizing `x - log(x)` function which has a minimum at _e_. When we are initializing the `RMSprop` class, we are giving it an arbitrary start value, 364. After about 1100 iterations, the minimum is correctly found.\n",
    "\n",
    "To use `RMSprop` in supervised learning problems, you refer to our [Introduction to PyTorch course](https://www.datacamp.com/courses/introduction-to-deep-learning-with-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop in Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to optimize the same function with RMSprop in Tensorflow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = 363.0513\n",
      "Iteration 100: x = 330.9281\n",
      "Iteration 200: x = 300.8935\n",
      "Iteration 300: x = 270.8544\n",
      "Iteration 400: x = 240.8105\n",
      "Iteration 500: x = 210.7600\n",
      "Iteration 600: x = 180.7007\n",
      "Iteration 700: x = 150.6295\n",
      "Iteration 800: x = 120.5406\n",
      "Iteration 900: x = 90.4234\n",
      "Iteration 1000: x = 60.2539\n",
      "Iteration 1100: x = 29.9579\n",
      "Iteration 1200: x = 2.7183\n",
      "Iteration 1300: x = 2.7183\n",
      "Iteration 1400: x = 2.7183\n",
      "Final result: x = 2.7183\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the function: f(x) = x / log(x)\n",
    "def f(x):\n",
    "    return x / tf.math.log(x)\n",
    "\n",
    "# Create a variable\n",
    "x = tf.Variable([364.0])\n",
    "\n",
    "# Create an RMSprop optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.3)\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(1500):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = f(x)\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [x])\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, [x]))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i}: x = {x.numpy()[0]:.4f}')\n",
    "\n",
    "print(f'Final result: x = {x.numpy()[0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the optimizer class is located at `tf.keras.optimizers` module. As you can see, the algorithm correctly converged at _e_ just like PyTorch but took slightly more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RMSprop in Python Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop vs. SGD vs. Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
