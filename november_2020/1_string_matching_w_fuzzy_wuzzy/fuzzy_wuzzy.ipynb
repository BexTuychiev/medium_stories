{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FuzzyWuzzy: Fuzzy String Matching in Python, Beginner's Guide\n",
    "## ... and a hands-on practice on a real-world dataset\n",
    "<img src=\"images/repo.jpg\"></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://pixabay.com/users/stephennorris-7555778/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3052477'>Steve Norris</a>\n",
    "        on \n",
    "        <a href='https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3052477'>Pixabay</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <small id='intro'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have dealt with text data before, you know that its issues are the hardest to deal with. There is just no one-size-fits-all solution to text problems and for each dataset, you have to come up with new ways to clean your data. In one of my [previous](https://towardsdatascience.com/master-the-most-hated-task-in-ds-ml-3b9779276d7c?source=your_stories_page-------------------------------------) articles, I talked about the worst-case scenario of such problems:\n",
    "> For example, consider this worst-case scenario: you are working on a survey data conducted across the USA and there is a state column for the state of each observation in the dataset. There are 50 states in the USA and imagine all the damn variations of state names people can come up with. You are in even bigger problem if data collectors decide to use abbreviations:\n",
    "CA, ca, Ca, Caliphornia, Californa, Calfornia, calipornia, CAL, CALI, â€¦\n",
    "Such columns will always be filled with typos, errors, inconsistencies.\n",
    "\n",
    "The problems related to text often arise because of free-text during data collection. They will be full of typos, inconsistencies, whatever you can name. Of course, the most basic problems can be solved using simple regular expressions or built-in Python functions but for cases like above, which occur very often, you have to arm yourself with more complex tools.\n",
    "\n",
    "Today's special is `fuzzywuzzy`, a package with a very simple API which helps us to calculate string similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "1. [Introduction](#intro)\n",
    "1. [Setup](#setup)\n",
    "1. [How String Matching Is Performed](#comparison)\n",
    "1. [Installation](#install)\n",
    "1. [FuzzyWuzzy: The Basics with WRatio](#wratio)\n",
    "1. [FuzzyWuzzy: Comparison of Different Methods ](#methods)\n",
    "1. [Using `fuzzywuzzy.process` to Extract Best Matches to a String from a List of Options](#process)\n",
    "1. [Text Cleaning With FuzzWuzzy On a Real Dataset](#real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup <small id='setup'></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "# fuzzywuzzy to be imported later\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable multiple cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How String Matching Is Performed <small id='comparison'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand string matching, let's get you up to speed with Minimum Edit Distance. As humans, we have no trouble at all if two or more strings are similar or not. To create this ability in computers, many algorithms were created and almost all of them depend on Minimum Edit Distance. \n",
    "\n",
    "Minimum Edit Distance (MED) is the least possible amount of steps needed to transition from one string to another. MED is calculated using only 4 operations:\n",
    "- Insertion\n",
    "- Deletion\n",
    "- Substitution\n",
    "- Replacing consecutive characters\n",
    "\n",
    "Consider these two words: **Program** and **Sonogram**:\n",
    "<img src='images/1.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get from Program to Sonogram, we need 3 steps:\n",
    "1. Add letter 'S' to the beginning of 'Program'.\n",
    "2. Substitute 'P' with 'O'.\n",
    "3. Substitute 'R' with 'N'.\n",
    "<img src='images/2.png'></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Minimum Edit Distance of 3\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said, there are many algorithms to calculate MED:\n",
    "- Damerau-Levenshtein\n",
    "- Levenshtein\n",
    "- Hamming\n",
    "- Jaro Distance\n",
    "\n",
    "Also, there are packages that use these algorithms: `nltk`, `fuzzywuzzy`, `textdistance`, `difflib`, ...\n",
    "\n",
    "In this article, we will only cover `fuzzywuzzy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FuzzWuzzy: Installation <small id='install'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the basic installation can be done easily with `pip`, there are some other options or caveats to `fuzzwuzzy`'s installation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using PIP via PyPI (standard):\n",
    "\n",
    "```pip install fuzzywuzzy```\n",
    "\n",
    "The above method installs the default up-to-date version of the package. At first, I installed it using this method. But whenever I imported it, it started giving a warning saying that the package itself is very slow and I should install `python-Levenshtein` package for more speed. If you hate warnings in your Jupyter Notebook like me, here is how you can install extra dependencies:\n",
    "- Directly install `python-Levenshtein`:\n",
    "\n",
    "```pip install python-Levenshtein```\n",
    "\n",
    "or\n",
    "\n",
    "```pip install fuzzywuzzy[speedup]```\n",
    "\n",
    "**Warning for Windows users**: if you don't have Microsoft Visual Studio build tools installed, installing `python-Levenshtein` fails. You can download MVS Build Tools from [here](https://visualstudio.microsoft.com/downloads/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FuzzyWuzzy: The Basics with WRatio <small id='wratio'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with `fuzzywuzzy`, we first import `fuzz` sub-module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sub-module, there are 5 functions for different methods of comparison between 2 strings. The most flexible and best one for everyday use is `WRatio` (Weighted Ratio) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('Python', 'Cython')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are comparing 'Python' to 'Cython'. The output returns a percentage between 0 and 100, 0 being not similar at all and 100 being identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('program', 'sonogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('insert', 'concert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('notebook', 'note')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functions of `fuzzywuzzy` are case-insensitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('Data Science', 'data science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WRatio` is also very good for partial strings with different orderings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('data science', 'science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('United States', 'United States of America')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('Barcelona, Spain', 'ESP, Barcelona')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FuzzyWuzzy: Comparison of Different Methods <small id='methods'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from `WRatio`, there are 4 other functions to compute string similarity:\n",
    "- fuzz.ratio\n",
    "- fuzz.partial_ratio\n",
    "- fuzz.token_sort_ratio\n",
    "- fuzz.token_set_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fuzz.ratio` is perfect for strings with similar lengths and order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('program', 'sonogram')\n",
    "fuzz.ratio('response', 'respond')\n",
    "fuzz.ratio('plant', 'grant')\n",
    "fuzz.ratio('word', 'world')\n",
    "fuzz.ratio('data science', 'data sience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison with WRatio\n",
    "fuzz.WRatio('program', 'sonogram')\n",
    "fuzz.WRatio('response', 'respond')\n",
    "fuzz.WRatio('plant', 'grant')\n",
    "fuzz.WRatio('word', 'world')\n",
    "fuzz.WRatio('data science', 'data sience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For strings with differing lengths, it is better to use `fuzz.patial_ratio':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('maths', 'mathematics')\n",
    "fuzz.partial_ratio('maths', 'mathematics')\n",
    "fuzz.WRatio('maths', 'mathematics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('barcelona', 'barca')\n",
    "fuzz.partial_ratio('barcelona', 'barca')\n",
    "fuzz.WRatio('barcelona', 'barca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the strings have the same meaning but their order is different, use `fuzz.token_sort_ratio':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('Barcelona vs. Real Madrid', 'Real Madrid vs. Barcelona')\n",
    "fuzz.partial_ratio('Barcelona vs. Real Madrid', 'Real Madrid vs. Barcelona')\n",
    "fuzz.WRatio('Barcelona vs. Real Madrid', 'Real Madrid vs. Barcelona')\n",
    "fuzz.token_sort_ratio('Barcelona vs. Real Madrid', 'Real Madrid vs. Barcelona')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more edge cases, there is `fuzz.token_set_ratio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('Manchester United vs Manchester City', 'United vs City')\n",
    "fuzz.partial_ratio('Manchester United vs Manchester City', 'United vs City')\n",
    "fuzz.WRatio('Manchester United vs Manchester City', 'United vs City')\n",
    "fuzz.token_set_ratio('Manchester United vs Manchester City', 'City vs United')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, these 5 functions are full with caveats. Their comparison is a whole another topic so I am leaving you a link to the [article](https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/) written by the package creators which explains their difference beautifully. \n",
    "> I think you already saw that `WRatio` function gives the middle ground for all the functions of `fuzzywuzzy`. For many edge cases and different issues, it is best to use `WRatio` for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `fuzzywuzzy.process` to Extract Best Matches to a String from a List of Options <small id='process'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some understanding `fuzzywuzzy`'s different functions, we can move on to more complex problems. With real life data, most of the time you have to find the most similar value to your string from a list of options. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_match = 'Mercedez-Benz'\n",
    "options = ['Ford', 'Mustang', 'mersedez benz', 'MAZDA', 'Mercedez']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to find best matches to `Mercedez-Benz` to replace them with the correct spelling of the cars. We can loop over each value but such process could take too long if there are millions of options to choose from. Since this operation is so commonly used, `fuzzywuzzy` provides us with a helpful sub-module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this sub-module, you can extract best matches to your string from a sequence of strings. Let's solve our initial problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mersedez benz', 92), ('Mercedez', 90), ('Ford', 45)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extract(query=string_to_match, choices=options, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of interest in `process.extract` are `query`, `choices` and `limit`. This function computes the similarity of strings given in `query` from a sequence of options given in `choices` and returns a list of tuples. `limit` controls the number of tuples to return. Each of these tuples contain two elements, first one is the matching string and the second one is the similarity score.\n",
    "\n",
    "Under the hood, `process.extract` uses default `WRatio` function. However, depending on your case and knowing the differences between the 5 functions you can change the scoring function with `scorer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mersedez benz', 92), ('Mercedez', 76), ('Ford', 24)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extract(query=string_to_match,\n",
    "                choices=options,\n",
    "                limit=3,\n",
    "                scorer=fuzz.ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have many options, it is best to stick with `WRatio` because it is the most flexible.\n",
    "\n",
    "In the `process` module, there are other functions which perform similar operation. `process.extractOne` returns only one output which contains the string with the highest matching score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mersedez benz', 92)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extractOne(string_to_match, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning With FuzzWuzzy On a Real Dataset <small id='real'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to tackle a real-world problem. I will load the raw data to practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/raw.csv',\n",
    "                   usecols=['zip', 'year', 'vehicle_make', 'vehicle_model'],\n",
    "                   na_values=0)\n",
    "cars.dropna(inplace=True)\n",
    "cars.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>year</th>\n",
       "      <th>vehicle_make</th>\n",
       "      <th>vehicle_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>94541</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>MERCEDES-BENZ</td>\n",
       "      <td>C-CLASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6501</th>\n",
       "      <td>94587</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>COROLLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212</th>\n",
       "      <td>94607</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>JETTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>94705</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>MODEL S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>94539</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>CR-V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        zip    year   vehicle_make vehicle_model\n",
       "1712  94541  1997.0  MERCEDES-BENZ       C-CLASS\n",
       "6501  94587  2016.0         TOYOTA       COROLLA\n",
       "8212  94607  2007.0     VOLKSWAGEN         JETTA\n",
       "9890  94705  2013.0          TESLA       MODEL S\n",
       "1380  94539  2007.0          HONDA          CR-V"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8504, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used this dataset in one of my personal projects and the task was to correct the spelling of each vehicle make and model according to the correct values given in another file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle library to load pickle files\n",
    "import pickle\n",
    "# load data file\n",
    "with open('data/make_model.pkl', 'rb') as file:\n",
    "    make_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the pickle file, `make_model` is now a dictionary containing the correct spelling of each car make as keys and the correct spelling of models under each key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's see the spellings of makes and models of `Toyota` cars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4Runner',\n",
       " '86',\n",
       " 'Avalon',\n",
       " 'Avalon Hybrid',\n",
       " 'C-HR',\n",
       " 'Camry',\n",
       " 'Camry Hybrid',\n",
       " 'Celica',\n",
       " 'Corolla',\n",
       " 'Corolla Hatchback',\n",
       " 'Corolla Hybrid',\n",
       " 'Corolla iM',\n",
       " 'Cressida',\n",
       " 'Echo',\n",
       " 'FJ Cruiser',\n",
       " 'GR Supra',\n",
       " 'Highlander',\n",
       " 'Highlander Hybrid',\n",
       " 'Land Cruiser',\n",
       " 'MR2',\n",
       " 'Matrix',\n",
       " 'Mirai',\n",
       " 'Paseo',\n",
       " 'Previa',\n",
       " 'Prius',\n",
       " 'Prius Plug-in Hybrid',\n",
       " 'Prius Prime',\n",
       " 'Prius c',\n",
       " 'Prius v',\n",
       " 'RAV4',\n",
       " 'RAV4 Hybrid',\n",
       " 'Regular Cab',\n",
       " 'Sequoia',\n",
       " 'Sienna',\n",
       " 'Solara',\n",
       " 'Supra',\n",
       " 'T100 Regular Cab',\n",
       " 'T100 Xtracab',\n",
       " 'Tacoma Access Cab',\n",
       " 'Tacoma Double Cab',\n",
       " 'Tacoma Regular Cab',\n",
       " 'Tacoma Xtracab',\n",
       " 'Tercel',\n",
       " 'Tundra Access Cab',\n",
       " 'Tundra CrewMax',\n",
       " 'Tundra Double Cab',\n",
       " 'Tundra Regular Cab',\n",
       " 'Venza',\n",
       " 'Xtra Cab',\n",
       " 'Yaris',\n",
       " 'Yaris Hatchback',\n",
       " 'Yaris iA'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_model['Toyota']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's subset the raw data for `Toyota` cars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>year</th>\n",
       "      <th>vehicle_make</th>\n",
       "      <th>vehicle_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94612</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS PLUG-IN HYBRID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>94612</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>TUNDRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>94706</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>94706</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>94706</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>94707</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>94707</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS PLUG-IN HYBRID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>94707</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS PRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>94707</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>COROLLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>94707</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1722 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        zip    year vehicle_make         vehicle_model\n",
       "4     94612  2014.0       TOYOTA  PRIUS PLUG-IN HYBRID\n",
       "7     94612  2003.0       TOYOTA                TUNDRA\n",
       "10    94706  2017.0       TOYOTA                 PRIUS\n",
       "11    94706  2008.0       TOYOTA                 PRIUS\n",
       "12    94706  2012.0       TOYOTA               PRIUS V\n",
       "...     ...     ...          ...                   ...\n",
       "9992  94707  2016.0       TOYOTA                 PRIUS\n",
       "9995  94707  2012.0       TOYOTA  PRIUS PLUG-IN HYBRID\n",
       "9997  94707  2018.0       TOYOTA           PRIUS PRIME\n",
       "9998  94707  2004.0       TOYOTA               COROLLA\n",
       "9999  94707  2007.0       TOYOTA                 PRIUS\n",
       "\n",
       "[1722 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars[cars['vehicle_make'] == 'TOYOTA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains up to a hundred unique car makes like Audi, Bentley, BMW and each one contains several models which are full of edge cases. We cannot just convert each one to title case or lower case. We also don't know if these contain any spelling errors or inconsistencies and visual search is not an option for such big datasets. There are also some cases where make labels with more than one word divide the name with a `space` while others with a `dash`. If you have this many inconsistencies and there is not a clear pattern, use string matching.\n",
    "\n",
    "Let's start by cleaning up car make labels. For comparison, here are the make labels in both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FORD', 'DODGE', 'CHEVROLET', 'SUBARU', 'TOYOTA', 'MERCEDES-BENZ',\n",
       "       'BMW', 'HONDA', 'HYUNDAI', 'LEXUS', 'SCION', 'NISSAN', 'SAAB',\n",
       "       'PORSCHE', 'KIA', 'JEEP', 'MITSUBISHI', 'VOLKSWAGEN', 'ACURA',\n",
       "       'GMC', 'MINI', 'MAZDA', 'CHRYSLER', 'MERCURY', 'CADILLAC',\n",
       "       'INFINITI', 'VOLVO', 'LINCOLN', 'AUDI', 'TESLA', 'BUICK', 'FIAT',\n",
       "       'SATURN', 'LAND ROVER', 'FREIGHTLINER', 'PONTIAC', 'JAGUAR',\n",
       "       'PETERBILT', 'GEO', 'RAM', 'ISUZU', 'PLYMOUTH', 'MASERATI',\n",
       "       'ASTON MARTIN', 'INTERNATIONAL', 'HINO', 'OLDSMOBILE', 'SUZUKI',\n",
       "       'UD TRUCKS', 'HUMMER', 'WORKHORSE', 'COUNTRY COACH', 'LAMBORGHINI',\n",
       "       'MG', 'SMART', 'GENESIS', 'KENWORTH', 'BENTLEY', 'OSHKOSH'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars['vehicle_make'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Acura', 'Alfa Romeo', 'Aston Martin', 'Audi', 'Bentley', 'BMW', 'Buick', 'Cadillac', 'Chevrolet', 'Chrysler', 'Dodge', 'Ferrari', 'FIAT', 'Ford', 'Freightliner', 'Genesis', 'GMC', 'Honda', 'Hyundai', 'INFINITI', 'Jaguar', 'Jeep', 'Kia', 'Lamborghini', 'Land Rover', 'Lexus', 'Lincoln', 'Lotus', 'Maserati', 'MAZDA', 'McLaren', 'Mercedes-Benz', 'MINI', 'Mitsubishi', 'Nissan', 'Porsche', 'Ram', 'Rolls-Royce', 'smart', 'Subaru', 'Tesla', 'Toyota', 'Volkswagen', 'Volvo', 'HUMMER', 'Maybach', 'Mercury', 'Pontiac', 'Saab', 'Saturn', 'Scion', 'Suzuki'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the differences are obvious. We will use `process.extract` to match each make with the correct spelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each correct make:\n",
    "for make in make_model.keys():\n",
    "    # find potential matches\n",
    "    matches = process.extract(make, cars['vehicle_make'], limit=cars.shape[0])\n",
    "    # for each match\n",
    "    for match in matches:\n",
    "        # if high similarity score\n",
    "        if match[1] >= 90:\n",
    "            # replace the incorrect spelling with the make\n",
    "            cars.loc[cars['vehicle_make'] == match[0], 'vehicle_make'] = make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ford', 'Dodge', 'Chevrolet', 'Subaru', 'Toyota', 'Mercedes-Benz',\n",
       "       'BMW', 'Honda', 'Hyundai', 'Lexus', 'Scion', 'Nissan', 'Saab',\n",
       "       'Porsche', 'Kia', 'Jeep', 'Mitsubishi', 'Volkswagen', 'Acura',\n",
       "       'GMC', 'MINI', 'MAZDA', 'Chrysler', 'Mercury', 'Cadillac',\n",
       "       'INFINITI', 'Volvo', 'Lincoln', 'Audi', 'Tesla', 'Buick', 'FIAT',\n",
       "       'Saturn', 'Land Rover', 'Freightliner', 'Pontiac', 'Jaguar',\n",
       "       'PETERBILT', 'GEO', 'Ram', 'ISUZU', 'PLYMOUTH', 'Maserati',\n",
       "       'Aston Martin', 'INTERNATIONAL', 'HINO', 'OLDSMOBILE', 'Suzuki',\n",
       "       'UD TRUCKS', 'HUMMER', 'WORKHORSE', 'COUNTRY COACH', 'Lamborghini',\n",
       "       'MG', 'smart', 'Genesis', 'KENWORTH', 'Bentley', 'OSHKOSH'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars['vehicle_make'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the make labels which exist in the `make_model` got converted into their correct spelling. Now, it is time for model labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each make\n",
    "for make in make_model:\n",
    "    # if make exists in the main data\n",
    "    if make in cars['vehicle_make'].unique():\n",
    "        # for each model\n",
    "        for model in make_model[make]:\n",
    "            # subset main data for current make and get its models\n",
    "            options = cars[cars['vehicle_make'] == make]['vehicle_model']\n",
    "            # find motential matches\n",
    "            matches = process.extract(model, options, limit=options.shape[0])\n",
    "            # for each match\n",
    "            for match in matches:\n",
    "                # if high similarity score\n",
    "                if match[1] >= 90:\n",
    "                    # replace incorrect spelling with the correct one\n",
    "                    cars.loc[((cars['vehicle_make'] == make) &\n",
    "                              (cars['vehicle_model'] == match[0])),\n",
    "                             'vehicle_model'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>year</th>\n",
       "      <th>vehicle_make</th>\n",
       "      <th>vehicle_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6421</th>\n",
       "      <td>94587</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7506</th>\n",
       "      <td>94603</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Silverado 3500 HD Crew Cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9282</th>\n",
       "      <td>94619</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>3 Series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6512</th>\n",
       "      <td>94587</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Corolla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7363</th>\n",
       "      <td>94602</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>Cherokee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8735</th>\n",
       "      <td>94610</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Accord Hybrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>94544</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>INFINITI</td>\n",
       "      <td>G37 SEDAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>94608</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>RAV4 Hybrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9493</th>\n",
       "      <td>94621</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>Caravan Cargo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8825</th>\n",
       "      <td>94611</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>M2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6797</th>\n",
       "      <td>94587</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>Mercedes-AMG GLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559</th>\n",
       "      <td>94550</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Mustang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>94603</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Mustang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>94544</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>94577</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Ford</td>\n",
       "      <td>F150 SuperCrew Cab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        zip    year   vehicle_make               vehicle_model\n",
       "6421  94587  2016.0           Ford                    Explorer\n",
       "7506  94603  1999.0      Chevrolet  Silverado 3500 HD Crew Cab\n",
       "9282  94619  2016.0            BMW                    3 Series\n",
       "6512  94587  2000.0         Toyota                     Corolla\n",
       "7363  94602  2001.0           Jeep                    Cherokee\n",
       "8735  94610  1991.0          Honda               Accord Hybrid\n",
       "2248  94544  2013.0       INFINITI                   G37 SEDAN\n",
       "8334  94608  2017.0         Toyota                 RAV4 Hybrid\n",
       "9493  94621  2016.0          Dodge               Caravan Cargo\n",
       "8825  94611  2014.0            BMW                          M2\n",
       "6797  94587  2012.0  Mercedes-Benz            Mercedes-AMG GLE\n",
       "3559  94550  2015.0           Ford                     Mustang\n",
       "7544  94603  1990.0           Ford                     Mustang\n",
       "2228  94544  2011.0          Honda                       Civic\n",
       "5373  94577  2002.0           Ford          F150 SuperCrew Cab"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The last two code snippets were a little hairy. To fully understand how they are working, you should get some practice on `process.extract`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! If you did not know string matching, the task would have been impossible and even Regular Expressions would not have been able to help you. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
