{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Perform Fuzzy Dataframe Row Matching With RecordLinkage\n",
    "## An elite skill for hardest of the problems\n",
    "<img src='images/chain.jpg'></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://www.pexels.com/@joey-kyber-31917?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Joey Kyber</a>\n",
    "        on \n",
    "        <a href='https://www.pexels.com/photo/sea-nature-sunset-water-119562/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Unsplash</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <small id='intro'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one of my previous articles, I wrote about how to perform string similarity to clean text data using `fuzzywuzzy` package. Learning about the package and performing it in practice was really awesome. But wouldn't be even greater if we could perform the same process between rows of dataframes? \n",
    "\n",
    "Actually, the question should be why would we even need it? Today data is never collected in the same place but across several locations. A common challenge in this process is to convert all the little pieces of data into the same format so that when you merge them they work smoothly with data manipulation softwares such as SQL or `pandas`. \n",
    "\n",
    "But it is just not always possible. Consider these two fake tables:\n",
    "<img src='images/1.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume they are schedules for NBA games and they were scraped from different sites. If we want to merge them together, the merge would result in duplicates because even though not exact, there are fuzzy duplicates:\n",
    "<img src='images/3.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge them you would have to perform serious data cleaning operations to get the merge working. However, this dataset could have easily been thousands of rows and you would not be able to find all the edge cases. \n",
    "\n",
    "Real-world cases will be much more complex. Fuzzy row matching helps to remove duplicates and introduces consistency to your data. \n",
    "\n",
    "With that goal in mind, let me introduce you to `recordlinkage` package. It provides all the tools needed for record linkage and deduplication. In the next sections, we will see case studies to perform record linkage and will build a solid foundation for your future data cleaning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "1. [Introduction](#intro)\n",
    "1. [Setup](#setup)\n",
    "1. [Indexing](#indexing)\n",
    "1. [Case Study 1](#case1)\n",
    "1. [Case Study 2](#case2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Installation <small id='setup'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`recordlinkage` can be installed using `pip`:\n",
    "\n",
    "```pip install recordlinkage```\n",
    "\n",
    "For it to work, you need to import it with `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import pandas as pd\n",
    "import recordlinkage as rl\n",
    "import time\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage, Indexing <small id='indexing'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next examples, we will load one of the built-in datasets of `recordlinkage` to showcase its powers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage.datasets import load_febrl4\n",
    "\n",
    "census_a, census_b = load_febrl4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two datasets contain census data generated by [Febrl](https://sourceforge.net/projects/febrl/) project. It was divided into two with 5k rows in each and each are suited to perform record linkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy illustration, I will just take a random sample from both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_a = census_a.sample(5)\n",
    "rand_b = census_b.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>soc_sec_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rec-178-org</th>\n",
       "      <td>kirra</td>\n",
       "      <td>okoniewski</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>molyneux hse</td>\n",
       "      <td>kardinya</td>\n",
       "      <td>5330</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19700823</td>\n",
       "      <td>2077704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-3812-org</th>\n",
       "      <td>dean</td>\n",
       "      <td>webb</td>\n",
       "      <td>52</td>\n",
       "      <td>buggy crescent</td>\n",
       "      <td>mount patrick</td>\n",
       "      <td>stratton</td>\n",
       "      <td>3138</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19171128</td>\n",
       "      <td>4129591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-3510-org</th>\n",
       "      <td>kimberly</td>\n",
       "      <td>mason</td>\n",
       "      <td>150</td>\n",
       "      <td>tauss place</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rosewater</td>\n",
       "      <td>7008</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19840507</td>\n",
       "      <td>5663441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-424-org</th>\n",
       "      <td>nicholas</td>\n",
       "      <td>george</td>\n",
       "      <td>25</td>\n",
       "      <td>kambah vlge</td>\n",
       "      <td>casuarina shoppingsqre</td>\n",
       "      <td>fish creek</td>\n",
       "      <td>3585</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19370405</td>\n",
       "      <td>8589026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-271-org</th>\n",
       "      <td>abby</td>\n",
       "      <td>sporn</td>\n",
       "      <td>29</td>\n",
       "      <td>nivison place</td>\n",
       "      <td>key</td>\n",
       "      <td>joyner</td>\n",
       "      <td>2340</td>\n",
       "      <td>sa</td>\n",
       "      <td>19830505</td>\n",
       "      <td>5733964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             given_name     surname street_number       address_1  \\\n",
       "rec_id                                                              \n",
       "rec-178-org       kirra  okoniewski            11             NaN   \n",
       "rec-3812-org       dean        webb            52  buggy crescent   \n",
       "rec-3510-org   kimberly       mason           150     tauss place   \n",
       "rec-424-org    nicholas      george            25     kambah vlge   \n",
       "rec-271-org        abby       sporn            29   nivison place   \n",
       "\n",
       "                           address_2      suburb postcode state date_of_birth  \\\n",
       "rec_id                                                                          \n",
       "rec-178-org             molyneux hse    kardinya     5330   nsw      19700823   \n",
       "rec-3812-org           mount patrick    stratton     3138   nsw      19171128   \n",
       "rec-3510-org                     NaN   rosewater     7008   nsw      19840507   \n",
       "rec-424-org   casuarina shoppingsqre  fish creek     3585   nsw      19370405   \n",
       "rec-271-org                      key      joyner     2340    sa      19830505   \n",
       "\n",
       "             soc_sec_id  \n",
       "rec_id                   \n",
       "rec-178-org     2077704  \n",
       "rec-3812-org    4129591  \n",
       "rec-3510-org    5663441  \n",
       "rec-424-org     8589026  \n",
       "rec-271-org     5733964  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>soc_sec_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rec-492-dup-0</th>\n",
       "      <td>matthews</td>\n",
       "      <td>carky</td>\n",
       "      <td>14</td>\n",
       "      <td>mcnamara street</td>\n",
       "      <td>henry kendqll hostel</td>\n",
       "      <td>aroona</td>\n",
       "      <td>2275</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19460115</td>\n",
       "      <td>6001116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1698-dup-0</th>\n",
       "      <td>joshua</td>\n",
       "      <td>choi-lundberg</td>\n",
       "      <td>31</td>\n",
       "      <td>vest place</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leslie vale</td>\n",
       "      <td>7000</td>\n",
       "      <td>qld</td>\n",
       "      <td>19380903</td>\n",
       "      <td>1002096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-2937-dup-0</th>\n",
       "      <td>connor</td>\n",
       "      <td>trait</td>\n",
       "      <td>6</td>\n",
       "      <td>lane place</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leichhardt</td>\n",
       "      <td>3337</td>\n",
       "      <td>vic</td>\n",
       "      <td>19150208</td>\n",
       "      <td>1776479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-2407-dup-0</th>\n",
       "      <td>logan</td>\n",
       "      <td>hazelk</td>\n",
       "      <td>57</td>\n",
       "      <td>chewings pstreet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elsternwick</td>\n",
       "      <td>3350</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19510911</td>\n",
       "      <td>9448381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-752-dup-0</th>\n",
       "      <td>georgia</td>\n",
       "      <td>weidengbach</td>\n",
       "      <td>4</td>\n",
       "      <td>marou place</td>\n",
       "      <td>blueberry</td>\n",
       "      <td>innaloo</td>\n",
       "      <td>4068</td>\n",
       "      <td>vic</td>\n",
       "      <td>19280919</td>\n",
       "      <td>5368290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               given_name        surname street_number         address_1  \\\n",
       "rec_id                                                                     \n",
       "rec-492-dup-0    matthews          carky            14   mcnamara street   \n",
       "rec-1698-dup-0     joshua  choi-lundberg            31        vest place   \n",
       "rec-2937-dup-0     connor          trait             6        lane place   \n",
       "rec-2407-dup-0      logan         hazelk            57  chewings pstreet   \n",
       "rec-752-dup-0     georgia    weidengbach             4       marou place   \n",
       "\n",
       "                           address_2       suburb postcode state  \\\n",
       "rec_id                                                             \n",
       "rec-492-dup-0   henry kendqll hostel       aroona     2275   nsw   \n",
       "rec-1698-dup-0                   NaN  leslie vale     7000   qld   \n",
       "rec-2937-dup-0                   NaN   leichhardt     3337   vic   \n",
       "rec-2407-dup-0                   NaN  elsternwick     3350   nsw   \n",
       "rec-752-dup-0              blueberry      innaloo     4068   vic   \n",
       "\n",
       "               date_of_birth soc_sec_id  \n",
       "rec_id                                   \n",
       "rec-492-dup-0       19460115    6001116  \n",
       "rec-1698-dup-0      19380903    1002096  \n",
       "rec-2937-dup-0      19150208    1776479  \n",
       "rec-2407-dup-0      19510911    9448381  \n",
       "rec-752-dup-0       19280919    5368290  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we want to link the records of the two datasets without introducing duplication. To start the process, we would have to generate pairs for possible matches. Obviously, we cannot know which rows match so we would have to take all the possible pairs. Generating pairs to calculate similarity is done using the indexes of the two datasets. That's why it is also called `indexing`. `recordlinkage` package makes this process very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexing object\n",
    "indexer = rl.Index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the process, we will create an indexing object. Next, we should specify the mode of generating the pairs. Since we need to generate all the possible combinations of indexes, we will use `.full()` method on the indexing object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Index>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the mode of generation to full\n",
    "indexer.full()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will input the datasets to generate the pairs, also called candidates and assign the result to a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([( 'rec-178-org',  'rec-492-dup-0'),\n",
       "            ( 'rec-178-org', 'rec-1698-dup-0'),\n",
       "            ( 'rec-178-org', 'rec-2937-dup-0'),\n",
       "            ( 'rec-178-org', 'rec-2407-dup-0'),\n",
       "            ( 'rec-178-org',  'rec-752-dup-0'),\n",
       "            ('rec-3812-org',  'rec-492-dup-0'),\n",
       "            ('rec-3812-org', 'rec-1698-dup-0'),\n",
       "            ('rec-3812-org', 'rec-2937-dup-0'),\n",
       "            ('rec-3812-org', 'rec-2407-dup-0'),\n",
       "            ('rec-3812-org',  'rec-752-dup-0'),\n",
       "            ('rec-3510-org',  'rec-492-dup-0'),\n",
       "            ('rec-3510-org', 'rec-1698-dup-0'),\n",
       "            ('rec-3510-org', 'rec-2937-dup-0'),\n",
       "            ('rec-3510-org', 'rec-2407-dup-0'),\n",
       "            ('rec-3510-org',  'rec-752-dup-0'),\n",
       "            ( 'rec-424-org',  'rec-492-dup-0'),\n",
       "            ( 'rec-424-org', 'rec-1698-dup-0'),\n",
       "            ( 'rec-424-org', 'rec-2937-dup-0'),\n",
       "            ( 'rec-424-org', 'rec-2407-dup-0'),\n",
       "            ( 'rec-424-org',  'rec-752-dup-0'),\n",
       "            ( 'rec-271-org',  'rec-492-dup-0'),\n",
       "            ( 'rec-271-org', 'rec-1698-dup-0'),\n",
       "            ( 'rec-271-org', 'rec-2937-dup-0'),\n",
       "            ( 'rec-271-org', 'rec-2407-dup-0'),\n",
       "            ( 'rec-271-org',  'rec-752-dup-0')],\n",
       "           names=['rec_id_1', 'rec_id_2'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = indexer.index(rand_a, rand_b)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be a `pandas.MultiIndex` object. The first level contains the indexes from the first dataset and similarly, the second level indexes contain the indexes for the second dataset.\n",
    "\n",
    "The length of the resulting `series` will always be the product of the lengths of datasets. Because for our 5-row datasets, each index from the first table will have 5 pairs of indexes from the second:\n",
    "<img src='images/4.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if our datasets are large, generating all the possible pairs will be very computationally expensive. To avoid generating all the possible pairs, we should choose one column which has consistent values from both datasets. For our small datasets, there is a state column:\n",
    "```\n",
    ">>> rand_a[['state']], rand_b[['state']]\n",
    "```\n",
    "<img src='images/5.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pay attention, the unique values of `state` is consistent in both datasets. Meaning, one state name is not different in the other. This helps us very much because now we can exclude all the pairs that does not have a matching state value. To do this with `recordlinkage`, we have to change the mode from `full` to `blocking`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([( 'rec-178-org',  'rec-492-dup-0'),\n",
       "            ( 'rec-178-org', 'rec-2407-dup-0'),\n",
       "            ('rec-3812-org',  'rec-492-dup-0'),\n",
       "            ('rec-3812-org', 'rec-2407-dup-0'),\n",
       "            ('rec-3510-org',  'rec-492-dup-0'),\n",
       "            ('rec-3510-org', 'rec-2407-dup-0'),\n",
       "            ( 'rec-424-org',  'rec-492-dup-0'),\n",
       "            ( 'rec-424-org', 'rec-2407-dup-0')],\n",
       "           names=['rec_id_1', 'rec_id_2'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From scratch\n",
    "indexer = rl.Index()\n",
    "# Set the mode to blocking with `state`\n",
    "indexer.block('state')\n",
    "# Generate pairs\n",
    "pairs = indexer.index(rand_a, rand_b)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember, the logic behind blocking on a certain column is that we expect duplicate values to have the same or similar values across the columns of both datasets and if the rows do not match on some certain column, we can exclude that pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the number of pairs (6) got reduced significantly. These index pairs are also the ones that have the same values for `state`. Let's check some of the pairs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use blocking on a consistent common column, the number of pairs will be much less. We can even use multiple columns to block as long as the unique values of those columns are inconsistent in both tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage, Case Study <small id='case1'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an understanding of indexing, we can start record linkage with the full datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5458951\n"
     ]
    }
   ],
   "source": [
    "# Create an indexing object\n",
    "indexer = rl.Index()\n",
    "# Block on state\n",
    "indexer.block('state')\n",
    "# Generate candidate pairs\n",
    "pairs = indexer.index(census_a, census_b)\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full datasets, almost 5.5 million pairs are returned. Remember, if we used full indexing, it would have been 25 million. \n",
    "Now, using these candidate pairs, we will perform comparison of each column values. To start comparing, we should create a comparing object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparing object\n",
    "compare = rl.Compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object has many useful functions to match exact or fuzzy values of the columns. First, let's start by matching exact matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compare>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the exact matches of state\n",
    "compare.exact('state', 'state', label='state')\n",
    "# Query the exact matches of date of birth\n",
    "compare.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "# Query the exact matches of date of birth\n",
    "compare.exact('soc_sec_id', 'soc_sec_id', label='soc_sec_id')\n",
    "# Query the exact matches of date of birth\n",
    "compare.exact('postcode', 'postcode', label='postcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `exact` for certain fields, we expect row pairs have exactly the same values for these fields. The parameters of `exact`:\n",
    "- `left_on`:  the column name of the left dataset\n",
    "- `right_on`: the column name of the right dataset\n",
    "- `label`: the column name of the resulting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we perform all the comparisons, the result will be a `pandas` dataframe and `label` controls the name of the appropriate column name in the resulting dataframe. \n",
    "\n",
    "Why did we choose exact matching? Because the postcode, social security ID, the date of birth and the state columns have to be an exact match to be a duplicate. This also depends on the values of those columns. If the unique values are consistent among the datasets, we should use `exact`.\n",
    "\n",
    "Now, for fuzzy matching. The given name, surname, address columns will probably have typos and inconsistencies, so we will use fuzzy string matching for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compare>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the fuzzy matches for given name\n",
    "compare.string('given_name', 'given_name', threshold=0.75, method='levenshtein', label='given_name')\n",
    "# Query the fuzzy matches for surname\n",
    "compare.string('surname', 'surname', threshold=0.75, method='levenshtein', label='surname')\n",
    "# Query the fuzzy matches for address\n",
    "compare.string('address_1', 'address_1', threshold=0.75, method='levenshtein', label='address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fuzzy `string` matching, we will use `.string` method. The parameters for column names are the same. Other parameters:\n",
    "- `method`: controls the algorithm used to calculate string similarity\n",
    "- `threshold`: the similarity score threshold. If similarity is higher than the given score, it is a match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are other methods of matching values depending on the data type: `.numeric` and `.date`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the methods in place, it is time to compute them and assign the result to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the matches, this will take a while\n",
    "matches = compare.compute(pairs, census_a, census_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.compute` takes three arguments. First one is the `MultiIndex` object of potential indexes. The next two are the two data frames we are using. Note that the order of their input should be the same as `indexer.index()`.\n",
    "\n",
    "After the computation is done, we will have a dataset of this sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting data frame also has multi-level index, first one being `census_a`, second one being `census_b`. The rest of the columns will have either 1 for a match or 0 for not a match. Let's interpret the first row of the above sample:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows with indexes `rec-3254-org` and `rec-1416-dup-0` only matched on the `state` column because there is 1 in that field. These rows failed to match in other fields.\n",
    "\n",
    "Now, let's set when we decide that two rows are duplicate. For our dataset, I think if the rows match on at least 4 columns, there is pretty high chance that they are duplicates. We can easily subset for rows with overall matching score of at least 4 with `sum` and boolean indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query matches with score over 4\n",
    "full_matches = matches[matches.sum(axis='columns') >= 4]\n",
    "full_matches.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you use `.sum()` with `axis` set to 1 or `columns`, it will take the sum of numeric values across columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, almost 4676 rows matched out of 5.5 million possible pairs. Now before merging our original tables together, we have to make sure that we do not include these 4676 rows. To do this, we will do a little bit of manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes from either of index levels\n",
    "duplicates = full_matches.index.get_level_values('rec_id_2')\n",
    "print(duplicates[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get indexes of some level from multi-level indexes, we use `.get_level_values` on `df.index`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we chose the second level index, we should exclude them from `census_b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the indexes of duplicates from census_b\n",
    "unique_b = census_b[~census_b.index.isin(duplicates)]\n",
    "unique_b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `unique_b` is ready to be appended to the first dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append deduplicated census_b to census_a\n",
    "full_census = census_a.append(unique_b)\n",
    "full_census.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_census.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go. From 10k rows full of duplicates, we got it to 5324 unique rows. Here is the full code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set a time\n",
    "start = time.time()\n",
    "# Create an indexing object\n",
    "indexer = rl.Index()\n",
    "# Block on state\n",
    "indexer.block('state')\n",
    "# Generate candidate pairs\n",
    "pairs = indexer.index(census_a, census_b)\n",
    "\n",
    "# Create a comparing object\n",
    "compare = rl.Compare()\n",
    "\n",
    "# Query the exact matches of state\n",
    "compare.exact('state', 'state', label='state')\n",
    "# Query the exact matches of date of birth\n",
    "compare.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "# Query the exact matches of date of birth\n",
    "compare.exact('soc_sec_id', 'soc_sec_id', label='soc_sec_id')\n",
    "# Query the exact matches of date of birth\n",
    "compare.exact('postcode', 'postcode', label='postcode')\n",
    "# Query the fuzzy matches for given name\n",
    "compare.string('given_name', 'given_name', threshold=0.75, method='levenshtein', label='given_name')\n",
    "# Query the fuzzy matches for surname\n",
    "compare.string('surname', 'surname', threshold=0.75, method='levenshtein', label='surname')\n",
    "# Query the fuzzy matches for address\n",
    "compare.string('address_1', 'address_1', threshold=0.75, method='levenshtein', label='address')\n",
    "\n",
    "# Compute the matches, this will take a while\n",
    "matches = compare.compute(pairs, census_a, census_b)\n",
    "# Query matches with score over 4\n",
    "full_matches = matches[matches.sum(axis='columns') >= 4]\n",
    "\n",
    "# Get the indexes from either of index levels\n",
    "duplicates = full_matches.index.get_level_values('rec_id_2')\n",
    "# Exclude the indexes of duplicates from census_b\n",
    "unique_b = census_b[~census_b.index.isin(duplicates)]\n",
    "\n",
    "# Append deduplicated census_b to census_a\n",
    "full_census = census_a.append(unique_b)\n",
    "\n",
    "# end timer\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage, Case Study 2 <small id='case2'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solidify your knowledge, we will perform record linkage with two other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load restaurants data\n",
    "restaurants_1 = pd.read_csv('data/restaurants1.csv', index_col=0)\n",
    "restaurants_2 = pd.read_csv('data/restaurants2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to merge these two datasets without duplication. Since they have long names and addresses, they will probably be full of typos and inconsistencies, so `.merge` won't work as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = restaurants_1.merge(restaurants_2, on='name')\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None matched! This definitely suggests we use record linkage. I will perform the process without too much details, because the steps will be the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a timer\n",
    "start = time.time()\n",
    "# Create an indexer object\n",
    "indexer = rl.Index()\n",
    "# Block on restaurant type because it has consistent values across datasets\n",
    "indexer.block('type')\n",
    "# Create candidate pairs\n",
    "pairs = indexer.index(restaurants_1, restaurants_2)\n",
    "\n",
    "# Creat a comparion object\n",
    "compare = rl.Compare()\n",
    "# Fuzzy string matches\n",
    "compare.string('name', 'name', label='name', threshold=0.75, method='levenshtein')\n",
    "compare.string('addr', 'addr', label='address', threshold=0.75, method='levenshtein')\n",
    "compare.string('city', 'city', label='city', threshold=0.75, method='levenshtein')\n",
    "# Fuzzy numeric matches\n",
    "compare.numeric('phone', 'phone', label='phone')\n",
    "\n",
    "# Create matches\n",
    "matches = compare.compute(pairs, restaurants_1, restaurants_2)\n",
    "# Query matches of score over 3\n",
    "full_matches = matches[matches.sum(axis='columns') >= 3]\n",
    "\n",
    "# Get the indexes from either of index levels\n",
    "duplicates = full_matches.index.get_level_values(1)\n",
    "# Exclude the indexes of duplicates from census_b\n",
    "res_unique = restaurants_2[~restaurants_2.index.isin(duplicates)]\n",
    "\n",
    "# Append deduplicated census_b to census_a\n",
    "full_restaurants = restaurants_1.append(res_unique)\n",
    "print(\"Number of all restaurants: \" + str(full_restaurants.shape[0]))\n",
    "# end timer\n",
    "end = time.time()\n",
    "end - start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
