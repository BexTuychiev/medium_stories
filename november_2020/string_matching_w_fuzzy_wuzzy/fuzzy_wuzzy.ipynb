{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FuzzyWuzzy: Fuzzy String Matching in Python, Deep Guide\n",
    "## ... and a hands-on practice on a real-world dataset\n",
    "<img src=\"images/repo.jpg\"></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Photo by \n",
    "        <a href='https://pixabay.com/users/stephennorris-7555778/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3052477'>Steve Norris</a>\n",
    "        on \n",
    "        <a href='https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3052477'>Pixabay</a>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <small id='intro'></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup <small id='setup'></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "# fuzzywuzzy to be imported later\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable multiple cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How String Matching Is Performed <small id='comparison'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand string matching, let's get you up to speed with Minimum Edit Distance. As humans, we have no trouble at all if two or more strings are similar or not. To create this ability in computers, many algorithms were created and almost all of them depend on Minimum Edit Distance. \n",
    "\n",
    "Minimum Edit Distance (MED) is the least possible amount of steps needed to transition from one string to another. MED is calculated using only 4 operations:\n",
    "- Insertion\n",
    "- Deletion\n",
    "- Substitution\n",
    "- Replacing consecutive characters\n",
    "\n",
    "Consider these two words: **Program** and **Sonogram**:\n",
    "<img src='images/1.png'></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two strings have five similar characters in the end. We can ignore them and focus on the beginning of the strings. To get from Program to Sonogram, we need 3 steps:\n",
    "1. Add letter 'S' to the beginning of 'Program'.\n",
    "2. Substitute 'P' with 'O'.\n",
    "3. Substitute 'R' with 'N'.\n",
    "<img src='images/2.png'></img>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "        Minimum Edit Distance of 3\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said, there are many algorithms to calculate MED:\n",
    "- Damerau-Levenshtein\n",
    "- Levenshtein\n",
    "- Hamming\n",
    "- Jaro Distance\n",
    "\n",
    "Also, there are packages that use these algorithms: `nltk`, `fuzzywuzzy`, `textdistance`, `difflib`, ...\n",
    "\n",
    "In this article, we will cover `fuzzywuzzy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FuzzWuzzy: Installation <small id='install'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the basic installation can be done easily with `pip`, there are some other options or caveats to `fuzzwuzzy`'s installation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using PIP via PyPI (standard):\n",
    "\n",
    "```pip install fuzzywuzzy```\n",
    "\n",
    "The above method installs the default up-to-date version of the package. At first, I installed it using this method. But whenever I imported it, it started giving a warning saying that the package itself is very slow and I should install `python-Levenshtein` package for more speed. If you hate warnings in your Jupyter Notebook like me, here is how you can install extra dependencies:\n",
    "- Directly install `python-Levenshtein`:\n",
    "\n",
    "```pip install python-Levenshtein```\n",
    "\n",
    "or\n",
    "\n",
    "```pip install fuzzywuzzy[speedup]```\n",
    "\n",
    "**Warning for Windows users**: if you don't have Microsoft Visual Studio build tools installed, installing `python-Levenshtein` fails. You can download it from [here](https://visualstudio.microsoft.com/downloads/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FuzzyWuzzy: The Basics with WRatio <small id='wratio'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with `fuzzywuzzy`, we first import `fuzz` sub-module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sub-module, there are 5 functions for different methods of comparison between 2 strings. The most flexible and best one for everyday use is `WRatio` (Weighted Ratio) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('Python', 'Cython')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are comparing 'Python' to 'Cython'. The output returns a percentage between 0 and 100, 0 being not similar at all and 100 being identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('program', 'sonogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('insert', 'concert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('notebook', 'note')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functions of `fuzzywuzzy` are case-insensitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('Data Science', 'data science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WRatio` is also very good for partial strings with different orderings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('data science', 'science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('United States', 'United States of America')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('Barcelona, Spain', 'ESP, Barcelona')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FuzzyWuzzy: Comparison of Different Methods <small id='methods'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from `WRatio`, there are 4 other functions to compute string similarity:\n",
    "- fuzz.ratio\n",
    "- fuzz.partial_ratio\n",
    "- fuzz.token_sort_ratio\n",
    "- fuzz.token_set_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fuzz.ratio` is perfect for strings with similar lengths and order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('program', 'sonogram')\n",
    "fuzz.ratio('response', 'respond')\n",
    "fuzz.ratio('plant', 'grant')\n",
    "fuzz.ratio('word', 'world')\n",
    "fuzz.ratio('data science', 'data sience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison with WRatio\n",
    "fuzz.WRatio('program', 'sonogram')\n",
    "fuzz.WRatio('response', 'respond')\n",
    "fuzz.WRatio('plant', 'grant')\n",
    "fuzz.WRatio('word', 'world')\n",
    "fuzz.ratio('data science', 'data sience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For strings with differing lengths, it is better to use `fuzz.patial_ratio':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('maths', 'mathematics')\n",
    "fuzz.partial_ratio('maths', 'mathematics')\n",
    "fuzz.WRatio('maths', 'mathematics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('barcelona', 'barca')\n",
    "fuzz.partial_ratio('barcelona', 'barca')\n",
    "fuzz.WRatio('barcelona', 'barca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the strings have the same meaning but their order is different, use `fuzz.token_sort_ratio':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('Barcelona vs. Real Madrid', 'Real Madrid vs. Barcelona')\n",
    "fuzz.partial_ratio('Barcelona vs. Real Madrid', 'Real Madrid vs. Barcelona')\n",
    "fuzz.WRatio('Barcelona vs. Real Madrid', 'Real Madrid vs. Barcelona')\n",
    "fuzz.token_sort_ratio('Barcelona vs. Real Madrid', 'Real Madrid vs. Barcelona')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more edge cases, there is `fuzz.token_set_ratio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('Manchester United vs Manchester City', 'United vs City')\n",
    "fuzz.partial_ratio('Manchester United vs Manchester City', 'United vs City')\n",
    "fuzz.WRatio('Manchester United vs Manchester City', 'United vs City')\n",
    "fuzz.token_set_ratio('Manchester United vs Manchester City', 'City vs United')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, these 5 functions are full with caveats. Their comparison is a whole another topic so I am leaving you a link to the [article](https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/) written by the package creators which explains their difference beautifully. \n",
    "> I think you already saw that `WRatio` function gives the middle ground for all the functions of `fuzzywuzzy`. For many edge cases and different issues, it is best to use `WRatio` for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `fuzzywuzzy.process` to Extract Best Matches to a String from a List of Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some understanding `fuzzywuzzy`'s different functions, we can move on to more complex problems. With real life data, most of the time you have to find the most similar value to your string from a list of options. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_match = 'Mercedez-Benz'\n",
    "options = ['Ford', 'Mustang', 'mersedez benz', 'MAZDA', 'Mercedez']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to find best matches to `Mercedez-Benz` to replace them with the correct spelling of the cars. We can loop over each value but such process could take too long if there are millions of options to choose from. Since this operation is so commonly used, `fuzzywuzzy` provides us with a helpful sub-module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this sub-module, you can extract best matches to your string from a sequence of strings. Let's solve our initial problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mersedez benz', 92), ('Mercedez', 90), ('Ford', 45)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extract(query=string_to_match, choices=options, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of interest in `process.extract` are `query`, `choices` and `limit`. This function computes the similarity of strings given in `query` from a sequence of options given in `choices` and returns a list of tuples. `limit` controls the number of tuples to return. Each of these tuples contain two elements, first one is the matching string and the second one is the similarity score.\n",
    "\n",
    "Under the hood, `process.extract` uses default `WRatio` function. However, depending on your case and knowing the differences between the 5 functions you can change the scoring function with `scorer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mersedez benz', 92), ('Mercedez', 76), ('Ford', 24)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extract(query=string_to_match, choices=options, limit=3, scorer=fuzz.ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have many options, it is best to stick with `WRatio` because it is the most flexible.\n",
    "\n",
    "In the `process` module, there are other similar functions which perform the same operation. `process.extractOne` returns only one output which contains the string with the highest matching score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mersedez benz', 92)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extractOne(string_to_match, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning With FuzzWuzzy On a Real Dataset <small id='real'></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to tackle a real-world problem. I will load the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/raw.csv', usecols=['zip', 'year', 'vehicle_make', 'vehicle_model'], na_values=0)\n",
    "cars.dropna(inplace=True)\n",
    "cars.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>year</th>\n",
       "      <th>vehicle_make</th>\n",
       "      <th>vehicle_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>94546</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>FIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8694</th>\n",
       "      <td>94610</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>PORSCHE</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>94702</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>94621</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>TACOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>94601</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>FORD</td>\n",
       "      <td>E-SERIES CARGO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        zip    year vehicle_make   vehicle_model\n",
       "3435  94546  2013.0        HONDA             FIT\n",
       "8694  94610  2007.0      PORSCHE             911\n",
       "9658  94702  2015.0         AUDI              A3\n",
       "9433  94621  2008.0       TOYOTA          TACOMA\n",
       "7134  94601  2008.0         FORD  E-SERIES CARGO"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8504, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used this dataset in one of my personal projects and the task was to correct the spelling of each vehicle make and model according to the correct values given in another file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle library to load pickle files\n",
    "import pickle\n",
    "\n",
    "with open('data/make_model.pkl', 'rb') as file:\n",
    "    make_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the pickle file, `make_model` is now a dictionary containing the correct spelling of each car make as keys and the correct spelling of models under each key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's see the spellings of makes and models of `Toyota` cars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4Runner',\n",
       " '86',\n",
       " 'Avalon',\n",
       " 'Avalon Hybrid',\n",
       " 'C-HR',\n",
       " 'Camry',\n",
       " 'Camry Hybrid',\n",
       " 'Celica',\n",
       " 'Corolla',\n",
       " 'Corolla Hatchback',\n",
       " 'Corolla Hybrid',\n",
       " 'Corolla iM',\n",
       " 'Cressida',\n",
       " 'Echo',\n",
       " 'FJ Cruiser',\n",
       " 'GR Supra',\n",
       " 'Highlander',\n",
       " 'Highlander Hybrid',\n",
       " 'Land Cruiser',\n",
       " 'MR2',\n",
       " 'Matrix',\n",
       " 'Mirai',\n",
       " 'Paseo',\n",
       " 'Previa',\n",
       " 'Prius',\n",
       " 'Prius Plug-in Hybrid',\n",
       " 'Prius Prime',\n",
       " 'Prius c',\n",
       " 'Prius v',\n",
       " 'RAV4',\n",
       " 'RAV4 Hybrid',\n",
       " 'Regular Cab',\n",
       " 'Sequoia',\n",
       " 'Sienna',\n",
       " 'Solara',\n",
       " 'Supra',\n",
       " 'T100 Regular Cab',\n",
       " 'T100 Xtracab',\n",
       " 'Tacoma Access Cab',\n",
       " 'Tacoma Double Cab',\n",
       " 'Tacoma Regular Cab',\n",
       " 'Tacoma Xtracab',\n",
       " 'Tercel',\n",
       " 'Tundra Access Cab',\n",
       " 'Tundra CrewMax',\n",
       " 'Tundra Double Cab',\n",
       " 'Tundra Regular Cab',\n",
       " 'Venza',\n",
       " 'Xtra Cab',\n",
       " 'Yaris',\n",
       " 'Yaris Hatchback',\n",
       " 'Yaris iA'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_model['Toyota']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's subset the raw data for `Toyota` cars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>year</th>\n",
       "      <th>vehicle_make</th>\n",
       "      <th>vehicle_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94612</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS PLUG-IN HYBRID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>94612</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>TUNDRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>94706</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>94706</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>94706</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>94707</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>94707</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS PLUG-IN HYBRID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>94707</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS PRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>94707</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>COROLLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>94707</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>PRIUS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1722 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        zip    year vehicle_make         vehicle_model\n",
       "4     94612  2014.0       TOYOTA  PRIUS PLUG-IN HYBRID\n",
       "7     94612  2003.0       TOYOTA                TUNDRA\n",
       "10    94706  2017.0       TOYOTA                 PRIUS\n",
       "11    94706  2008.0       TOYOTA                 PRIUS\n",
       "12    94706  2012.0       TOYOTA               PRIUS V\n",
       "...     ...     ...          ...                   ...\n",
       "9992  94707  2016.0       TOYOTA                 PRIUS\n",
       "9995  94707  2012.0       TOYOTA  PRIUS PLUG-IN HYBRID\n",
       "9997  94707  2018.0       TOYOTA           PRIUS PRIME\n",
       "9998  94707  2004.0       TOYOTA               COROLLA\n",
       "9999  94707  2007.0       TOYOTA                 PRIUS\n",
       "\n",
       "[1722 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars[cars['vehicle_make'] == 'TOYOTA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains up to a hundred unique car makes like Audi, Bentley, BMW and each one contains several models which are full of edge cases. We cannot just convert each one to title case or lower case. We also don't know if these contain any spelling errors or inconsistencies and visual search is not an option for such big datasets. There are also some cases where make labels with more than one word divide the name with a `space` while others with a `dash`. If you have this many inconsistencies and there is not a clear pattern, use string matching.\n",
    "\n",
    "Let's start by cleaning up car make labels. For comparison, here are the make labels in both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FORD', 'DODGE', 'CHEVROLET', 'SUBARU', 'TOYOTA', 'MERCEDES-BENZ',\n",
       "       'BMW', 'HONDA', 'HYUNDAI', 'LEXUS', 'SCION', 'NISSAN', 'SAAB',\n",
       "       'PORSCHE', 'KIA', 'JEEP', 'MITSUBISHI', 'VOLKSWAGEN', 'ACURA',\n",
       "       'GMC', 'MINI', 'MAZDA', 'CHRYSLER', 'MERCURY', 'CADILLAC',\n",
       "       'INFINITI', 'VOLVO', 'LINCOLN', 'AUDI', 'TESLA', 'BUICK', 'FIAT',\n",
       "       'SATURN', 'LAND ROVER', 'FREIGHTLINER', 'PONTIAC', 'JAGUAR',\n",
       "       'PETERBILT', 'GEO', 'RAM', 'ISUZU', 'PLYMOUTH', 'MASERATI',\n",
       "       'ASTON MARTIN', 'INTERNATIONAL', 'HINO', 'OLDSMOBILE', 'SUZUKI',\n",
       "       'UD TRUCKS', 'HUMMER', 'WORKHORSE', 'COUNTRY COACH', 'LAMBORGHINI',\n",
       "       'MG', 'SMART', 'GENESIS', 'KENWORTH', 'BENTLEY', 'OSHKOSH'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars['vehicle_make'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Acura', 'Alfa Romeo', 'Aston Martin', 'Audi', 'Bentley', 'BMW', 'Buick', 'Cadillac', 'Chevrolet', 'Chrysler', 'Dodge', 'Ferrari', 'FIAT', 'Ford', 'Freightliner', 'Genesis', 'GMC', 'Honda', 'Hyundai', 'INFINITI', 'Jaguar', 'Jeep', 'Kia', 'Lamborghini', 'Land Rover', 'Lexus', 'Lincoln', 'Lotus', 'Maserati', 'MAZDA', 'McLaren', 'Mercedes-Benz', 'MINI', 'Mitsubishi', 'Nissan', 'Porsche', 'Ram', 'Rolls-Royce', 'smart', 'Subaru', 'Tesla', 'Toyota', 'Volkswagen', 'Volvo', 'HUMMER', 'Maybach', 'Mercury', 'Pontiac', 'Saab', 'Saturn', 'Scion', 'Suzuki'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `process.extract` to match each make with the correct spelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each correct make:\n",
    "for make in make_model.keys():\n",
    "    # find potential matches\n",
    "    matches = process.extract(make, cars['vehicle_make'], limit=cars.shape[0])\n",
    "    # for each match\n",
    "    for match in matches:\n",
    "        # if high similarity score\n",
    "        if match[1] >= 90:\n",
    "            # replace the incorrect spelling with the make\n",
    "            cars.loc[cars['vehicle_make'] == match[0], 'vehicle_make'] = make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ford', 'Dodge', 'Chevrolet', 'Subaru', 'Toyota', 'Mercedes-Benz',\n",
       "       'BMW', 'Honda', 'Hyundai', 'Lexus', 'Scion', 'Nissan', 'Saab',\n",
       "       'Porsche', 'Kia', 'Jeep', 'Mitsubishi', 'Volkswagen', 'Acura',\n",
       "       'GMC', 'MINI', 'MAZDA', 'Chrysler', 'Mercury', 'Cadillac',\n",
       "       'INFINITI', 'Volvo', 'Lincoln', 'Audi', 'Tesla', 'Buick', 'FIAT',\n",
       "       'Saturn', 'Land Rover', 'Freightliner', 'Pontiac', 'Jaguar',\n",
       "       'PETERBILT', 'GEO', 'Ram', 'ISUZU', 'PLYMOUTH', 'Maserati',\n",
       "       'Aston Martin', 'INTERNATIONAL', 'HINO', 'OLDSMOBILE', 'Suzuki',\n",
       "       'UD TRUCKS', 'HUMMER', 'WORKHORSE', 'COUNTRY COACH', 'Lamborghini',\n",
       "       'MG', 'smart', 'Genesis', 'KENWORTH', 'Bentley', 'OSHKOSH'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars['vehicle_make'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the make labels which exist in the `make_model` got converted into their correct spelling. Now, it is time for model labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each make\n",
    "for make in make_model:\n",
    "    # if make exists in the main data\n",
    "    if make in cars['vehicle_make'].unique():\n",
    "        # for each model\n",
    "        for model in make_model[make]:\n",
    "            # subset main data for current make and get its models\n",
    "            options = cars[cars['vehicle_make'] == make]['vehicle_model']\n",
    "            # find motential matches\n",
    "            matches = process.extract(model, options, limit=options.shape[0])\n",
    "            # for each match\n",
    "            for match in matches:\n",
    "                # if high similarity score\n",
    "                if match[1] >= 90:\n",
    "                    # replace incorrect spelling with the correct one\n",
    "                    cars.loc[\n",
    "                        ((cars['vehicle_make'] == make) &\n",
    "                        (cars['vehicle_model'] == match[0])),\n",
    "                        'vehicle_model'\n",
    "                    ] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>year</th>\n",
       "      <th>vehicle_make</th>\n",
       "      <th>vehicle_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9833</th>\n",
       "      <td>94704</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Prius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8561</th>\n",
       "      <td>94610</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Prius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6657</th>\n",
       "      <td>94587</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>KENWORTH</td>\n",
       "      <td>CONSTRUCTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6521</th>\n",
       "      <td>94587</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7817</th>\n",
       "      <td>94605</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>GL-Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403</th>\n",
       "      <td>94544</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Silverado 3500 HD Crew Cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>94606</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>Sentra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>94546</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Corolla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>94610</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>94580</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Crosstour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8611</th>\n",
       "      <td>94610</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Volt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8795</th>\n",
       "      <td>94611</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Highlander Hybrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>94551</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Jetta (New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9781</th>\n",
       "      <td>94703</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Explorer Sport Trac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>94544</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>FIAT</td>\n",
       "      <td>500c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        zip    year   vehicle_make               vehicle_model\n",
       "9833  94704  2005.0         Toyota                       Prius\n",
       "8561  94610  2014.0         Toyota                       Prius\n",
       "6657  94587  1997.0       KENWORTH                CONSTRUCTION\n",
       "6521  94587  2015.0          Honda                         Fit\n",
       "7817  94605  1987.0  Mercedes-Benz                    GL-Class\n",
       "2403  94544  2003.0      Chevrolet  Silverado 3500 HD Crew Cab\n",
       "8121  94606  2002.0         Nissan                      Sentra\n",
       "3476  94546  2001.0         Toyota                     Corolla\n",
       "8712  94610  2014.0          Honda                       Civic\n",
       "6074  94580  2007.0          Honda                   Crosstour\n",
       "8611  94610  2013.0      Chevrolet                        Volt\n",
       "8795  94611  2013.0         Toyota           Highlander Hybrid\n",
       "4178  94551  2019.0     Volkswagen                 Jetta (New)\n",
       "9781  94703  2012.0           Ford         Explorer Sport Trac\n",
       "2726  94544  2012.0           FIAT                        500c"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The last two code snippets were a little hairy. To fully understand how they are working, you should get some practice on `process.extract`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! If you did not know string matching, the task would have been impossible and even Regular Expressions would not have been able to help you. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium_articles",
   "language": "python",
   "name": "medium_articles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
